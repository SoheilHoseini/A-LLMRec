{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpQy1R3MIQKv",
        "outputId": "016888b5-2aae-40af-e1f8-fb7949a76dd2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/drive/MyDrive/Rec_Proj_DL'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 95 (delta 23), reused 21 (delta 21), pack-reused 58 (from 1)\u001b[K\n",
            "Receiving objects: 100% (95/95), 265.35 KiB | 1.43 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SoheilHoseini/A-LLMRec.git /content/drive/MyDrive/Rec_Proj_DL/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "\n",
        "!conda create -n [env name] python=3.10 pip\n",
        "!conda install pytorch==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "!conda install numpy=1.26.3\n",
        "!conda install tqdm\n",
        "!conda install pytz\n",
        "!conda install transformers=4.32.1\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!conda install conda-forge::accelerate=0.25.0\n",
        "!conda install conda-forge::bitsandbytes=0.42.0\n",
        "\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import torch.serialization\n",
        "import argparse\n",
        "torch.serialization.add_safe_globals([argparse.Namespace])\n",
        "\n",
        "import os\n",
        "!pip install accelerate --upgrade\n",
        "import accelerate\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
        "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "t8Joi6DOJpP_",
        "outputId": "cd9927a4-e6f1-42fb-f73b-45b68bd54283",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       3_kmp_llvm           7 KB  conda-forge\n",
            "    blas-2.116                 |              mkl          13 KB  conda-forge\n",
            "    blas-devel-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    ca-certificates-2025.8.3   |       hbd8a1cb_0         151 KB  conda-forge\n",
            "    certifi-2025.8.3           |     pyhd8ed1ab_0         155 KB  conda-forge\n",
            "    conda-24.11.3              |  py311h38be061_0         1.1 MB  conda-forge\n",
            "    cpython-3.11.13            |  py311hd8ed1ab_0          46 KB  conda-forge\n",
            "    cuda-cudart-11.8.89        |                0         197 KB  nvidia\n",
            "    cuda-cupti-11.8.87         |                0        25.3 MB  nvidia\n",
            "    cuda-libraries-11.8.0      |                0           1 KB  nvidia\n",
            "    cuda-nvrtc-11.8.89         |                0        19.1 MB  nvidia\n",
            "    cuda-nvtx-11.8.86          |                0          57 KB  nvidia\n",
            "    cuda-runtime-11.8.0        |                0           1 KB  nvidia\n",
            "    cuda-version-12.9          |                3          17 KB  nvidia\n",
            "    filelock-3.19.1            |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    gmpy2-2.2.1                |  py311h0f6cedb_0         198 KB  conda-forge\n",
            "    jinja2-3.1.6               |     pyhd8ed1ab_0         110 KB  conda-forge\n",
            "    libblas-3.9.0              |   16_linux64_mkl          13 KB  conda-forge\n",
            "    libcblas-3.9.0             |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libcublas-11.11.3.6        |                0       364.0 MB  nvidia\n",
            "    libcufft-10.9.0.58         |                0       142.8 MB  nvidia\n",
            "    libcufile-1.14.1.1         |                4         946 KB  nvidia\n",
            "    libcurand-10.3.10.19       |                0        44.0 MB  nvidia\n",
            "    libcusolver-11.4.1.48      |                0        96.5 MB  nvidia\n",
            "    libcusparse-11.7.5.86      |                0       176.3 MB  nvidia\n",
            "    libgfortran-14.2.0         |       h69a702a_2          52 KB  conda-forge\n",
            "    libgfortran-ng-14.2.0      |       h69a702a_2          53 KB  conda-forge\n",
            "    libgfortran5-14.2.0        |       hf1ad2bd_2         1.4 MB  conda-forge\n",
            "    libhwloc-2.11.2            |default_h0d58e46_1001         2.3 MB  conda-forge\n",
            "    liblapack-3.9.0            |   16_linux64_mkl          12 KB  conda-forge\n",
            "    liblapacke-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libnpp-11.8.0.86           |                0       147.8 MB  nvidia\n",
            "    libnvjpeg-11.9.0.86        |                0         2.4 MB  nvidia\n",
            "    llvm-openmp-15.0.7         |       h0cdce71_0         3.1 MB  conda-forge\n",
            "    markupsafe-3.0.2           |  py311h2dc5d0c_1          25 KB  conda-forge\n",
            "    mkl-2022.1.0               |     h84fe81f_915       199.6 MB  conda-forge\n",
            "    mkl-devel-2022.1.0         |     ha770c72_916          25 KB  conda-forge\n",
            "    mkl-include-2022.1.0       |     h84fe81f_915         745 KB  conda-forge\n",
            "    mpc-1.3.1                  |       h24ddda3_1         114 KB  conda-forge\n",
            "    mpfr-4.2.1                 |       h90cbb55_3         620 KB  conda-forge\n",
            "    mpmath-1.3.0               |     pyhd8ed1ab_1         429 KB  conda-forge\n",
            "    networkx-3.5               |     pyhe01879c_0         1.5 MB  conda-forge\n",
            "    openssl-3.5.2              |       h26f9b46_0         3.0 MB  conda-forge\n",
            "    pytorch-2.1.2              |py3.11_cuda11.8_cudnn8.7.0_0        1.46 GB  pytorch\n",
            "    pytorch-cuda-11.8          |       h7e8668a_6           7 KB  pytorch\n",
            "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
            "    pyyaml-6.0.2               |  py311h2dc5d0c_2         208 KB  conda-forge\n",
            "    sympy-1.14.0               |   pyh2585a3b_105         4.4 MB  conda-forge\n",
            "    tbb-2021.13.0              |       hceb3a55_1         172 KB  conda-forge\n",
            "    torchtriton-2.1.0          |            py311        91.0 MB  pytorch\n",
            "    typing_extensions-4.14.1   |     pyhe01879c_0          50 KB  conda-forge\n",
            "    yaml-0.2.5                 |       h280c20c_3          83 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        2.76 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               conda-forge/linux-64::blas-2.116-mkl \n",
            "  blas-devel         conda-forge/linux-64::blas-devel-3.9.0-16_linux64_mkl \n",
            "  cpython            conda-forge/noarch::cpython-3.11.13-py311hd8ed1ab_0 \n",
            "  cuda-cudart        nvidia/linux-64::cuda-cudart-11.8.89-0 \n",
            "  cuda-cupti         nvidia/linux-64::cuda-cupti-11.8.87-0 \n",
            "  cuda-libraries     nvidia/linux-64::cuda-libraries-11.8.0-0 \n",
            "  cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-11.8.89-0 \n",
            "  cuda-nvtx          nvidia/linux-64::cuda-nvtx-11.8.86-0 \n",
            "  cuda-runtime       nvidia/linux-64::cuda-runtime-11.8.0-0 \n",
            "  cuda-version       nvidia/noarch::cuda-version-12.9-3 \n",
            "  filelock           conda-forge/noarch::filelock-3.19.1-pyhd8ed1ab_0 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  gmpy2              conda-forge/linux-64::gmpy2-2.2.1-py311h0f6cedb_0 \n",
            "  jinja2             conda-forge/noarch::jinja2-3.1.6-pyhd8ed1ab_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-16_linux64_mkl \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-16_linux64_mkl \n",
            "  libcublas          nvidia/linux-64::libcublas-11.11.3.6-0 \n",
            "  libcufft           nvidia/linux-64::libcufft-10.9.0.58-0 \n",
            "  libcufile          nvidia/linux-64::libcufile-1.14.1.1-4 \n",
            "  libcurand          nvidia/linux-64::libcurand-10.3.10.19-0 \n",
            "  libcusolver        nvidia/linux-64::libcusolver-11.4.1.48-0 \n",
            "  libcusparse        nvidia/linux-64::libcusparse-11.7.5.86-0 \n",
            "  libgfortran        conda-forge/linux-64::libgfortran-14.2.0-h69a702a_2 \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-14.2.0-h69a702a_2 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-14.2.0-hf1ad2bd_2 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.11.2-default_h0d58e46_1001 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-16_linux64_mkl \n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-16_linux64_mkl \n",
            "  libnpp             nvidia/linux-64::libnpp-11.8.0.86-0 \n",
            "  libnvjpeg          nvidia/linux-64::libnvjpeg-11.9.0.86-0 \n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-15.0.7-h0cdce71_0 \n",
            "  markupsafe         conda-forge/linux-64::markupsafe-3.0.2-py311h2dc5d0c_1 \n",
            "  mkl                conda-forge/linux-64::mkl-2022.1.0-h84fe81f_915 \n",
            "  mkl-devel          conda-forge/linux-64::mkl-devel-2022.1.0-ha770c72_916 \n",
            "  mkl-include        conda-forge/linux-64::mkl-include-2022.1.0-h84fe81f_915 \n",
            "  mpc                conda-forge/linux-64::mpc-1.3.1-h24ddda3_1 \n",
            "  mpfr               conda-forge/linux-64::mpfr-4.2.1-h90cbb55_3 \n",
            "  mpmath             conda-forge/noarch::mpmath-1.3.0-pyhd8ed1ab_1 \n",
            "  networkx           conda-forge/noarch::networkx-3.5-pyhe01879c_0 \n",
            "  pytorch            pytorch/linux-64::pytorch-2.1.2-py3.11_cuda11.8_cudnn8.7.0_0 \n",
            "  pytorch-cuda       pytorch/linux-64::pytorch-cuda-11.8-h7e8668a_6 \n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda \n",
            "  pyyaml             conda-forge/linux-64::pyyaml-6.0.2-py311h2dc5d0c_2 \n",
            "  sympy              conda-forge/noarch::sympy-1.14.0-pyh2585a3b_105 \n",
            "  tbb                conda-forge/linux-64::tbb-2021.13.0-hceb3a55_1 \n",
            "  torchtriton        pytorch/linux-64::torchtriton-2.1.0-py311 \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.14.1-pyhe01879c_0 \n",
            "  yaml               conda-forge/linux-64::yaml-0.2.5-h280c20c_3 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.8.3-hbd8a1cb_0 \n",
            "  certifi                           2024.12.14-pyhd8ed1ab_0 --> 2025.8.3-pyhd8ed1ab_0 \n",
            "  conda                             24.11.2-py311h38be061_1 --> 24.11.3-py311h38be061_0 \n",
            "  openssl                                  3.4.0-h7b32b05_1 --> 3.5.2-h26f9b46_0 \n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-3_kmp_llvm \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "pytorch-2.1.2        | 1.46 GB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.2        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjpeg-11.9.0.86  | 2.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.11.2      | 2.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.5         | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   0% 8.861394863273745e-05/1 [00:00<19:23, 1163.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   0% 4.293074789542188e-05/1 [00:00<43:31, 2611.92s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   0% 7.826994939611777e-05/1 [00:00<24:15, 1456.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   0% 1.0435991773783623e-05/1 [00:00<3:51:43, 13903.77s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   1% 0.01391238993533978/1 [00:00<00:12, 12.43s/it]    \u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   0% 0.003906698058483391/1 [00:00<00:45, 45.94s/it]   \u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   1% 0.008687964382969073/1 [00:00<00:20, 20.76s/it]   \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   0% 0.0013253709552705202/1 [00:00<02:32, 152.68s/it]     \n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   4% 0.03996489083336459/1 [00:00<00:05,  6.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   1% 0.013866631570221269/1 [00:00<00:17, 17.78s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   3% 0.02989912066931699/1 [00:00<00:08,  8.28s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   0% 0.004101344767096964/1 [00:00<01:04, 65.22s/it]  \n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   7% 0.06548570803959297/1 [00:00<00:04,  5.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   2% 0.023440188350900348/1 [00:00<00:13, 13.98s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   5% 0.04805774892921631/1 [00:00<00:06,  6.99s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   1% 0.007106910397946647/1 [00:00<00:48, 48.91s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   9% 0.08887979047863566/1 [00:00<00:04,  4.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   3% 0.03490269803897799/1 [00:00<00:11, 11.51s/it] \u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   7% 0.06848620572160305/1 [00:00<00:05,  6.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   1% 0.010039424086379846/1 [00:00<00:42, 42.65s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   5% 0.04769606091181371/1 [00:00<00:09,  9.95s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :   9% 0.08515770494297613/1 [00:00<00:05,  6.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  11% 0.11005852420185991/1 [00:00<00:04,  4.95s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   1% 0.012690165996920886/1 [00:00<00:41, 42.33s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  11% 0.10652540112811629/1 [00:00<00:04,  5.59s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   6% 0.059888393314113525/1 [00:00<00:09,  9.81s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  18% 0.1807574840444002/1 [00:00<00:02,  3.20s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   2% 0.015111316088438687/1 [00:00<00:42, 42.91s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  13% 0.12922368645299043/1 [00:00<00:04,  5.16s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   7% 0.07178021048114538/1 [00:00<00:08,  9.34s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  21% 0.21225791108839506/1 [00:00<00:02,  3.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   2% 0.017949905850907832/1 [00:00<00:39, 40.45s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  15% 0.14871290385262376/1 [00:00<00:04,  5.40s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   8% 0.08255582820289628/1 [00:00<00:09, 10.06s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  24% 0.24322980747728937/1 [00:00<00:02,  3.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   2% 0.0204545438766159/1 [00:00<00:41, 42.10s/it]  \n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  17% 0.17422890735575816/1 [00:01<00:04,  4.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  20% 0.19884970073186284/1 [00:01<00:03,  4.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   9% 0.09264455395832043/1 [00:01<00:09, 10.14s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   2% 0.02289656595168127/1 [00:01<00:41, 42.41s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  20% 0.1950487138951255/1 [00:01<00:03,  4.89s/it] \u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  10% 0.10414999439429348/1 [00:01<00:08,  9.73s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   3% 0.025568179845769878/1 [00:01<00:39, 40.84s/it]\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  30% 0.30253094697957505/1 [00:01<00:02,  3.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  11% 0.11471095837656727/1 [00:01<00:08,  9.67s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  22% 0.21563371058630446/1 [00:01<00:04,  5.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  24% 0.24076409843514765/1 [00:01<00:03,  5.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   3% 0.028041509896156596/1 [00:01<00:41, 42.61s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  24% 0.23512292798593779/1 [00:01<00:03,  5.18s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  13% 0.1251431301151548/1 [00:01<00:08,  9.76s/it] \u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  26% 0.262917585593332/1 [00:01<00:03,  4.94s/it]  \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   3% 0.030754867757340338/1 [00:01<00:40, 41.36s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  26% 0.25516003503134393/1 [00:01<00:03,  5.21s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  14% 0.14021182262644788/1 [00:01<00:07,  8.99s/it]\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  29% 0.2877294912104985/1 [00:01<00:03,  4.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   3% 0.033280377766595974/1 [00:01<00:39, 40.97s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  28% 0.2788758296983676/1 [00:01<00:03,  4.92s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  31% 0.31130080154680667/1 [00:01<00:03,  4.53s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  42% 0.4204989891980257/1 [00:01<00:02,  3.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   4% 0.03659902315065917/1 [00:01<00:37, 38.64s/it] \n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  30% 0.2999304460859233/1 [00:01<00:03,  4.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  33% 0.33354290265362374/1 [00:01<00:03,  4.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  16% 0.16365201097734822/1 [00:01<00:07,  9.37s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   4% 0.039187149110557505/1 [00:01<00:38, 40.24s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  32% 0.3204371728277062/1 [00:01<00:03,  4.89s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  36% 0.35950678960301585/1 [00:01<00:02,  4.37s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  48% 0.4769460631627682/1 [00:01<00:01,  3.75s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   4% 0.04222402271672854/1 [00:01<00:36, 37.91s/it] \n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  34% 0.3442312374441259/1 [00:01<00:03,  4.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   4% 0.044947816569686064/1 [00:01<00:35, 37.63s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  18% 0.18455928520241868/1 [00:01<00:08, 10.10s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  50% 0.5040068327039182/1 [00:01<00:01,  3.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  37% 0.3657554735280583/1 [00:01<00:02,  4.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   5% 0.047942946208761966/1 [00:02<00:34, 36.30s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  19% 0.19451921871415656/1 [00:02<00:08, 10.27s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  53% 0.5346616106997522/1 [00:02<00:01,  3.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  39% 0.38704489976380235/1 [00:02<00:03,  4.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  43% 0.43243606932775874/1 [00:02<00:02,  4.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   5% 0.05071892002058841/1 [00:02<00:35, 37.69s/it] \n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  56% 0.5619337925029423/1 [00:02<00:01,  3.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  41% 0.41021280478505323/1 [00:02<00:02,  4.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   5% 0.05339053391467702/1 [00:02<00:35, 37.84s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  22% 0.2193331909977104/1 [00:02<00:07,  9.31s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  59% 0.5893116804371527/1 [00:02<00:01,  3.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  43% 0.4342416792496614/1 [00:02<00:02,  4.66s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   6% 0.05646915148794318/1 [00:02<00:34, 36.72s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  23% 0.2301517394673567/1 [00:02<00:07,  9.46s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  62% 0.6190151032538056/1 [00:02<00:01,  3.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  46% 0.4567051547263472/1 [00:02<00:02,  4.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  50% 0.5030613863880505/1 [00:02<00:02,  4.48s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   6% 0.05920338133267449/1 [00:02<00:35, 38.17s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  48% 0.4783076607596757/1 [00:02<00:02,  4.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  53% 0.5309747802073628/1 [00:02<00:01,  4.18s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   6% 0.061833251259667966/1 [00:02<00:35, 38.31s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  50% 0.4999101667930042/1 [00:02<00:02,  4.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  56% 0.5643822388419049/1 [00:02<00:01,  3.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   6% 0.06475532895632738/1 [00:02<00:34, 37.04s/it] \n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  24% 0.24079856494542135/1 [00:02<00:12, 16.18s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  53% 0.5258957899925153/1 [00:02<00:02,  4.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  60% 0.5974352416819159/1 [00:02<00:01,  3.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   7% 0.06746868681751113/1 [00:02<00:35, 38.05s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  55% 0.5484375354185972/1 [00:02<00:02,  4.50s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  25% 0.2491700607850286/1 [00:02<00:11, 15.32s/it] \u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   7% 0.07010899273627838/1 [00:03<01:58, 126.92s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  26% 0.2574127643809496/1 [00:04<00:43, 59.25s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  78% 0.7778914181770415/1 [00:04<00:04, 18.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  57% 0.5707444709964907/1 [00:04<00:12, 28.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :   7% 0.07205008720620214/1 [00:05<04:35, 296.72s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  26% 0.2633372075905178/1 [00:05<01:07, 91.01s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  80% 0.8005125302153465/1 [00:06<00:06, 34.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   7% 0.07343807411211535/1 [00:06<06:19, 409.94s/it]\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  59% 0.5867115406732988/1 [00:06<00:20, 49.57s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   7% 0.07445036531417237/1 [00:06<06:08, 398.13s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  67% 0.6736432375060701/1 [00:06<00:13, 40.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  60% 0.598138953285132/1 [00:06<00:17, 44.73s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  82% 0.8166855682614245/1 [00:06<00:06, 33.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   8% 0.07725764710132016/1 [00:07<03:54, 254.00s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  69% 0.693935831742967/1 [00:07<00:09, 31.80s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  62% 0.6218547479521557/1 [00:07<00:11, 30.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  85% 0.8472346401262383/1 [00:07<00:03, 23.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   8% 0.07895871376044689/1 [00:07<03:07, 204.01s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  73% 0.7281408159152036/1 [00:07<00:05, 20.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  64% 0.6411874254529968/1 [00:07<00:08, 22.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  87% 0.8650989762686381/1 [00:07<00:02, 19.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   8% 0.08140073583551226/1 [00:07<02:15, 147.93s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  76% 0.7550794562995559/1 [00:07<00:03, 15.83s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  66% 0.6592677837634999/1 [00:07<00:06, 17.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  89% 0.8893056802722449/1 [00:07<00:01, 14.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   8% 0.08440630146636194/1 [00:07<01:35, 104.69s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  78% 0.7792710642762931/1 [00:07<00:02, 12.60s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  68% 0.6824356887847508/1 [00:07<00:04, 13.30s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  92% 0.9181634540407368/1 [00:07<00:00, 10.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   9% 0.0871718392864146/1 [00:07<01:14, 82.02s/it]  \n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  80% 0.8031082164584995/1 [00:07<00:02, 10.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  70% 0.7003595071964618/1 [00:07<00:03, 11.25s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  95% 0.952517946622275/1 [00:07<00:00,  8.11s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   9% 0.08987476115582456/1 [00:07<01:02, 68.71s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  83% 0.8281859639215642/1 [00:07<00:01,  8.50s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  72% 0.7198487245960952/1 [00:07<00:02,  9.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  98% 0.9835955491421893/1 [00:07<00:00,  6.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :   9% 0.09278640286071019/1 [00:07<00:52, 57.46s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  86% 0.8557449019463456/1 [00:07<00:01,  6.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  74% 0.7420773902245926/1 [00:07<00:02,  7.89s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  10% 0.09607374026945203/1 [00:07<00:43, 48.25s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  88% 0.8830379981252287/1 [00:07<00:00,  5.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  76% 0.7644625957518822/1 [00:07<00:01,  6.80s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  10% 0.0988288420977309/1 [00:07<00:40, 44.94s/it] \n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  91% 0.9082929734855588/1 [00:07<00:00,  5.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  79% 0.7894307096092438/1 [00:07<00:01,  5.94s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  10% 0.10162568789310492/1 [00:07<00:37, 42.21s/it]\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  93% 0.9346999301781146/1 [00:07<00:00,  4.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  11% 0.10520523307151271/1 [00:08<00:33, 37.18s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  39% 0.3872353460167054/1 [00:08<00:06, 10.14s/it] \u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  96% 0.9603093613329757/1 [00:08<00:00,  4.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  11% 0.10821079870236239/1 [00:08<00:32, 36.08s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  40% 0.4007156008558678/1 [00:08<00:05,  9.23s/it]\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  99% 0.9874252296145934/1 [00:08<00:00,  4.47s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  11% 0.1117694718972226/1 [00:08<00:29, 33.44s/it] \n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  41% 0.4145822324260891/1 [00:08<00:05,  8.62s/it]\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | :  89% 0.8915729935711775/1 [00:08<00:00,  4.41s/it]\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  12% 0.11517160521547606/1 [00:08<00:28, 32.75s/it]\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  12% 0.11889725427871682/1 [00:08<00:27, 30.82s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  44% 0.44270187229759045/1 [00:08<00:04,  8.11s/it]\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  12% 0.12226807962164893/1 [00:08<00:26, 30.60s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  46% 0.45665436536360254/1 [00:08<00:04,  7.82s/it]\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  13% 0.12564934095635483/1 [00:08<00:26, 30.32s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  47% 0.46970531272381083/1 [00:08<00:04,  7.93s/it]\u001b[A\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  13% 0.12951065791265476/1 [00:08<00:25, 29.12s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  13% 0.13307976709928876/1 [00:08<00:25, 28.89s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  14% 0.1374420116607303/1 [00:08<00:23, 26.86s/it] \n",
            "pytorch-2.1.2        | 1.46 GB   | :  14% 0.14156422841137484/1 [00:09<00:22, 26.73s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  15% 0.14544641735122235/1 [00:09<00:22, 26.80s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  15% 0.14959994207718824/1 [00:09<00:22, 25.96s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  15% 0.15406654655636762/1 [00:09<00:21, 25.03s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  16% 0.15807396739750054/1 [00:09<00:22, 26.42s/it]\n",
            "pytorch-2.1.2        | 1.46 GB   | :  16% 0.1629580115476313/1 [00:09<00:20, 24.45s/it] \n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | : 100% 1.0/1 [00:09<00:00,  6.54s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   0% 0.00010941492379625203/1 [00:09<24:54:09, 89659.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   0% 0.0019694686283325364/1 [00:10<1:01:59, 3726.47s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  62% 0.6157127863161407/1 [00:10<00:06, 15.77s/it]\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | : 100% 1.0/1 [00:10<00:00,  4.47s/it]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   0% 0.0001619872369599508/1 [00:10<17:31:55, 63125.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   3% 0.0328244771388756/1 [00:10<02:34, 160.07s/it]      \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  17% 0.16708022829827582/1 [00:10<00:54, 65.55s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   4% 0.04211668160958721/1 [00:10<02:45, 172.55s/it]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   6% 0.058536984230994835/1 [00:10<01:12, 76.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  17% 0.1701484098797682/1 [00:10<00:49, 59.07s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   8% 0.08374740150829456/1 [00:10<01:07, 73.27s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   9% 0.09234619568403671/1 [00:10<00:36, 39.98s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  17% 0.17309135955997518/1 [00:10<00:44, 53.67s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  13% 0.13023773851580045/1 [00:10<00:33, 38.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  12% 0.11576098937643464/1 [00:10<00:25, 28.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  18% 0.17596125729776566/1 [00:10<00:41, 50.42s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  17% 0.1747842286797869/1 [00:10<00:20, 24.34s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  14% 0.14453711433484892/1 [00:10<00:16, 19.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  18% 0.17870592313427075/1 [00:10<00:39, 48.60s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  22% 0.21706289752633406/1 [00:10<00:13, 16.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  18% 0.17703334670233578/1 [00:10<00:11, 13.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  18% 0.18132535706949046/1 [00:10<00:41, 50.22s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  26% 0.25853163018808145/1 [00:10<00:09, 12.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  21% 0.2054812268893613/1 [00:10<00:08, 10.26s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  18% 0.18373607116923446/1 [00:10<00:39, 48.00s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  30% 0.2975705542954296/1 [00:10<00:06,  9.25s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  23% 0.23195963844805428/1 [00:11<00:06,  8.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  19% 0.18672076481653657/1 [00:11<00:35, 43.50s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  34% 0.3421170444594161/1 [00:11<00:04,  6.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  26% 0.2569062410735998/1 [00:11<00:05,  7.36s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  19% 0.18942368668594653/1 [00:11<00:34, 42.14s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  38% 0.38245186646244383/1 [00:11<00:03,  5.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  28% 0.2814151840039602/1 [00:11<00:04,  6.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  19% 0.1920013766540711/1 [00:11<00:34, 42.42s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  42% 0.42068085438499225/1 [00:11<00:02,  4.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  31% 0.30526763739154317/1 [00:11<00:04,  5.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  19% 0.19450601467977915/1 [00:11<00:33, 41.93s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  46% 0.4605297146771401/1 [00:11<00:02,  4.18s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  34% 0.33612264590208624/1 [00:11<00:03,  5.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  20% 0.19740722039289102/1 [00:11<00:31, 39.57s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  50% 0.4987587025996885/1 [00:11<00:01,  3.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  76% 0.7560534011862747/1 [00:11<00:02,  9.83s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  20% 0.20024581015536017/1 [00:11<00:30, 38.26s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  54% 0.539741473550556/1 [00:11<00:01,  3.34s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [00:11<00:00,  4.22s/it]               \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  39% 0.390283033181231/1 [00:11<00:02,  4.43s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  20% 0.20292786004122254/1 [00:11<00:30, 38.33s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :   0% 0.00017172886155179142/1 [00:11<18:57:38, 68270.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  59% 0.5907674531929406/1 [00:11<00:01,  2.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  42% 0.4175273492064977/1 [00:11<00:02,  4.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  21% 0.20559947393531117/1 [00:11<00:30, 38.16s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :   3% 0.0340023145872547/1 [00:11<03:56, 244.67s/it]         \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  44% 0.4437869309175982/1 [00:11<00:02,  4.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  79% 0.786190786208861/1 [00:11<00:02, 10.66s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  63% 0.6332081092764477/1 [00:11<00:01,  3.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  21% 0.208260651837626/1 [00:11<00:32, 40.47s/it]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  47% 0.4698276827811062/1 [00:11<00:02,  4.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  80% 0.7956784814937492/1 [00:12<00:02, 11.06s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  67% 0.6701411993033165/1 [00:12<00:01,  3.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  21% 0.21077572585510784/1 [00:12<00:34, 43.65s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  49% 0.49499311525424416/1 [00:12<00:02,  4.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  80% 0.8048227307954741/1 [00:12<00:02, 11.24s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  71% 0.7051304424866658/1 [00:12<00:00,  3.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  21% 0.21311338801243537/1 [00:12<00:34, 43.75s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  52% 0.5198303029559933/1 [00:12<00:02,  4.47s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  81% 0.8137952571056172/1 [00:12<00:02, 11.63s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  75% 0.7512968050202519/1 [00:12<00:00,  2.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  22% 0.21544061417798913/1 [00:12<00:34, 44.31s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  55% 0.5480593532954264/1 [00:12<00:01,  4.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  83% 0.8272755119447797/1 [00:12<00:01, 10.18s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  23% 0.22651036838681288/1 [00:12<00:11, 14.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  22% 0.21810179208030395/1 [00:12<00:33, 42.63s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  84% 0.8382228526581122/1 [00:12<00:01,  9.87s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  28% 0.27717038254459136/1 [00:12<00:07,  9.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  22% 0.22078384196616632/1 [00:12<00:32, 41.07s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  85% 0.8484403706572227/1 [00:12<00:01, 10.26s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  57% 0.5727871260733793/1 [00:12<00:02,  5.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  22% 0.22323630003300549/1 [00:12<00:32, 41.60s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  87% 0.8737591561619746/1 [00:12<00:00,  2.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  60% 0.5980619734703135/1 [00:12<00:02,  5.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  86% 0.8582715119252743/1 [00:12<00:01, 11.24s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  23% 0.2256574501245233/1 [00:12<00:32, 42.21s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  92% 0.9192775697477208/1 [00:12<00:00,  2.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  62% 0.6219144268578964/1 [00:12<00:01,  4.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  87% 0.8688754066554435/1 [00:12<00:01, 10.92s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  23% 0.22803685624894596/1 [00:12<00:33, 43.70s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  40% 0.4015020783080884/1 [00:12<00:03,  5.08s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  64% 0.6442350713123319/1 [00:12<00:01,  4.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | : 100% 0.9978413796732969/1 [00:12<00:00,  2.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  88% 0.8808101545703708/1 [00:12<00:01, 10.35s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  23% 0.23041626237336862/1 [00:12<00:33, 43.98s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  67% 0.6695099187092661/1 [00:12<00:01,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  89% 0.8905983650905269/1 [00:13<00:01, 10.45s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  23% 0.23390188362581235/1 [00:13<00:30, 39.34s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  69% 0.6927058825540715/1 [00:13<00:01,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  90% 0.9002577833669969/1 [00:13<00:01, 10.46s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  24% 0.23644826561861554/1 [00:13<00:30, 39.45s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  72% 0.7156830165512845/1 [00:13<00:01,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  91% 0.9107328858534798/1 [00:13<00:00, 10.18s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  24% 0.23956862715897684/1 [00:13<00:28, 37.00s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  75% 0.7452250459762725/1 [00:13<00:01,  4.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  92% 0.9206069578694268/1 [00:13<00:00, 10.28s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  24% 0.2422819850201606/1 [00:13<00:28, 38.27s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  77% 0.769733988906633/1 [00:13<00:00,  4.32s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  93% 0.9305668913811648/1 [00:13<00:00, 10.21s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  24% 0.24491185494715406/1 [00:13<00:29, 39.22s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  80% 0.8006984123409723/1 [00:13<00:00,  3.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  94% 0.9403980326492163/1 [00:13<00:00, 10.61s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  25% 0.24832442425718132/1 [00:13<00:27, 35.96s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  83% 0.8301310268421641/1 [00:13<00:00,  3.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  74% 0.7404948510113246/1 [00:13<00:00,  2.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  25% 0.2511212700525553/1 [00:13<00:28, 37.52s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  86% 0.8570470980960421/1 [00:13<00:00,  4.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  78% 0.7799924891682366/1 [00:13<00:00,  2.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  25% 0.2538033199384177/1 [00:13<00:29, 39.47s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  88% 0.8829784350357538/1 [00:13<00:00,  4.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  82% 0.8193183984635969/1 [00:13<00:00,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  97% 0.9699343872012666/1 [00:13<00:00, 10.79s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  26% 0.2563601379229947/1 [00:13<00:33, 45.54s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  87% 0.8656851910825806/1 [00:13<00:00,  2.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  98% 0.9795938054777366/1 [00:13<00:00, 10.69s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  26% 0.259240471652559/1 [00:14<00:32, 43.22s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  91% 0.9077587621627695/1 [00:14<00:00,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  99% 0.9898542542247424/1 [00:14<00:00, 10.44s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  26% 0.2616407497605292/1 [00:14<00:31, 42.95s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | :  96% 0.9585905051820998/1 [00:14<00:00,  2.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  28% 0.277472149281359/1 [00:14<00:24, 34.40s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | : 100% 1.0/1 [00:15<00:00,  2.62s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | :   0% 0.00035504780175590466/1 [00:15<11:49:54, 42609.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | :   4% 0.035149732373834564/1 [00:15<04:54, 305.15s/it]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | : 100% 1.0/1 [00:15<00:00,  2.35s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   0% 0.0006180718489662367/1 [00:15<6:59:48, 25203.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   1% 0.012979508828290971/1 [00:15<14:09, 860.50s/it]     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | : 100% 1.0/1 [00:15<00:00,  3.58s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   0% 0.0008163460135194354/1 [00:15<5:23:03, 19399.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   3% 0.0277557644596608/1 [00:15<06:36, 407.39s/it]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   3% 0.025340945807615707/1 [00:15<06:05, 375.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   3% 0.032757807995210546/1 [00:16<04:07, 256.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   4% 0.03955659833383915/1 [00:16<02:55, 183.10s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   5% 0.050613452838204995/1 [00:16<03:02, 192.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   5% 0.046973460521433995/1 [00:16<02:02, 128.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | :   7% 0.0688792735406455/1 [00:16<02:17, 147.97s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   7% 0.06530768108155482/1 [00:16<02:03, 132.01s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   5% 0.054390322709028834/1 [00:16<01:28, 93.22s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   6% 0.06118911304765744/1 [00:16<01:06, 70.46s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   8% 0.07755287128434636/1 [00:16<01:30, 98.06s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   7% 0.06798790338628605/1 [00:16<00:50, 54.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   9% 0.08816536946009902/1 [00:16<01:08, 75.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   8% 0.07540476557388089/1 [00:16<00:38, 41.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  10% 0.09877786763585168/1 [00:16<00:52, 57.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  28% 0.28040466296979216/1 [00:16<02:45, 229.68s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   8% 0.08282162776147572/1 [00:16<00:30, 33.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  11% 0.10939036581160434/1 [00:16<00:40, 45.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   9% 0.09023848994907056/1 [00:16<00:24, 27.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  12% 0.120002863987357/1 [00:17<00:31, 35.27s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | :   9% 0.09479776306882655/1 [00:17<01:26, 95.72s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  10% 0.09827342398563164/1 [00:17<00:20, 22.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  13% 0.13061536216310965/1 [00:17<00:24, 27.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  11% 0.10630835802219271/1 [00:17<00:17, 19.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  28% 0.28250229731632265/1 [00:17<02:39, 222.12s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  29% 0.2881701427723607/1 [00:17<00:03,  4.56s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  27% 0.2738058290920429/1 [00:17<00:01,  2.45s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  28% 0.2847355995559124/1 [00:17<02:07, 177.67s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  52% 0.5159306805442831/1 [00:17<00:00,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  43% 0.42523343208877085/1 [00:17<00:00,  1.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  29% 0.28739677745822717/1 [00:17<01:36, 135.18s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  63% 0.6294027764234846/1 [00:17<00:00,  1.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  51% 0.5123815627930103/1 [00:17<00:00,  1.41s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  29% 0.289671823664912/1 [00:17<01:18, 110.53s/it]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  75% 0.7502219864243611/1 [00:17<00:00,  1.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  63% 0.6254887111538315/1 [00:17<00:00,  1.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  29% 0.2918633819374066/1 [00:17<01:06, 93.23s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :  90% 0.9004296529119372/1 [00:17<00:00,  1.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  76% 0.7614645179264037/1 [00:17<00:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  29% 0.2942845320289244/1 [00:17<00:55, 78.48s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  92% 0.921545126808659/1 [00:17<00:00,  1.09it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  30% 0.2969039659641441/1 [00:17<00:47, 67.17s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  30% 0.29936686002275703/1 [00:17<00:42, 61.07s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  30% 0.3015584182952516/1 [00:18<00:45, 65.10s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  30% 0.3044074440494945/1 [00:18<00:38, 54.96s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | :  92% 0.9234793323671081/1 [00:18<00:00,  1.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  31% 0.3066198743055366/1 [00:18<00:44, 64.24s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | : 100% 0.9966191795288244/1 [00:18<00:00,  1.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  31% 0.30850878881659144/1 [00:18<00:43, 63.60s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  31% 0.31031421539345605/1 [00:18<00:42, 61.66s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | : 100% 1.0/1 [00:18<00:00, 146.37s/it]              \u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | :  31% 0.3124431577153079/1 [00:18<00:40, 58.88s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | :   1% 0.005012289041185573/1 [00:18<1:02:06, 3744.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | : 100% 1.0/1 [00:18<00:00, 19.75s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  31% 0.3144051241687792/1 [00:18<00:38, 56.65s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.2        | 3.0 MB    | :   1% 0.005236433740607962/1 [00:18<59:44, 3603.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  32% 0.3163775266140243/1 [00:18<00:39, 57.13s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.2        | 3.0 MB    | :  91% 0.9059030371251774/1 [00:18<00:01, 14.72s/it]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:18<00:00, 13.36s/it]                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:18<00:00, 13.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjpeg-11.9.0.86  | 2.4 MB    | : 100% 1.0/1 [00:19<00:00, 13.38s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  32% 0.3200509957183961/1 [00:19<00:30, 44.60s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.2        | 3.0 MB    | : 100% 1.0/1 [00:19<00:00, 14.72s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.11.2      | 2.3 MB    | :   1% 0.006761307362165732/1 [00:19<46:39, 2818.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.5         | 1.5 MB    | :   1% 0.01047260975338487/1 [00:19<30:04, 1823.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  32% 0.3243714963127426/1 [00:19<00:24, 35.56s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.5         | 1.5 MB    | : 100% 1.0/1 [00:19<00:00, 1823.13s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.11.2      | 2.3 MB    | : 100% 1.0/1 [00:19<00:00, 13.48s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.11.2      | 2.3 MB    | : 100% 1.0/1 [00:19<00:00, 13.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [00:19<00:00, 1706.02s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  33% 0.3286398169482201/1 [00:19<00:20, 31.14s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | :   1% 0.01364371451460054/1 [00:19<23:11, 1411.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | :  34% 0.33655029871274805/1 [00:19<00:18, 28.09s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.1.2        | 1.46 GB   | : 100% 1.0/1 [00:44<00:00, 21.80s/it]               \n",
            "\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | : 100% 1.0/1 [00:51<00:00,  6.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | : 100% 1.0/1 [00:54<00:00,  4.47s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | : 100% 1.0/1 [01:20<00:00,  2.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [01:40<00:00,  4.22s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.1.0    | 91.0 MB   | : 100% 1.0/1 [01:40<00:00,  2.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | : 100% 1.0/1 [01:44<00:00,  1.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | : 100% 1.0/1 [02:06<00:00,  3.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.14.0         | 4.4 MB    | : 100% 1.0/1 [02:07<00:00, 19.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | : 100% 1.0/1 [02:12<00:00,  1.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [02:12<00:00, 13.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnvjpeg-11.9.0.86  | 2.4 MB    | : 100% 1.0/1 [02:13<00:00, 13.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.2        | 3.0 MB    | : 100% 1.0/1 [02:13<00:00, 14.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.5         | 1.5 MB    | : 100% 1.0/1 [02:14<00:00, 129.04s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.5         | 1.5 MB    | : 100% 1.0/1 [02:14<00:00, 129.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libhwloc-2.11.2      | 2.3 MB    | : 100% 1.0/1 [02:14<00:00, 13.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [02:14<00:00, 129.47s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-14.2.0  | 1.4 MB    | : 100% 1.0/1 [02:14<00:00, 129.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [02:15<00:00, 130.68s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [02:15<00:00, 130.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.10.19 | 44.0 MB   | : 100% 1.0/1 [02:17<00:00,  1.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.1.2        | 1.46 GB   | : 100% 1.0/1 [05:36<00:00, 21.80s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                         \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - numpy=1.26.3\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    numpy-1.26.3               |  py311h64a7726_0         7.8 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         7.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  numpy              conda-forge/linux-64::numpy-1.26.3-py311h64a7726_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "                                                                        \n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Channels:\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytz\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    pytz-2025.2                |     pyhd8ed1ab_0         185 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         185 KB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  pytz               conda-forge/noarch::pytz-2025.2-pyhd8ed1ab_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "                                                                        \n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - transformers=4.32.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _python_abi3_support-1.0   |       hd8ed1ab_2           8 KB  conda-forge\n",
            "    aiohappyeyeballs-2.6.1     |     pyhd8ed1ab_0          19 KB  conda-forge\n",
            "    aiohttp-3.12.15            |  py311h3778330_0         988 KB  conda-forge\n",
            "    aiosignal-1.4.0            |     pyhd8ed1ab_0          13 KB  conda-forge\n",
            "    attrs-25.3.0               |     pyh71513ae_0          56 KB  conda-forge\n",
            "    aws-c-auth-0.8.1           |       h205f482_0         106 KB  conda-forge\n",
            "    aws-c-cal-0.8.1            |       h1a47875_3          46 KB  conda-forge\n",
            "    aws-c-common-0.10.6        |       hb9d3cd8_0         231 KB  conda-forge\n",
            "    aws-c-compression-0.3.0    |       h4e1184b_5          19 KB  conda-forge\n",
            "    aws-c-event-stream-0.5.0   |      h7959bf6_11          53 KB  conda-forge\n",
            "    aws-c-http-0.9.2           |       hefd7a92_4         193 KB  conda-forge\n",
            "    aws-c-io-0.15.3            |       h173a860_6         154 KB  conda-forge\n",
            "    aws-c-mqtt-0.11.0          |      h11f4f37_12         190 KB  conda-forge\n",
            "    aws-c-s3-0.7.9             |       he1b24dc_1         113 KB  conda-forge\n",
            "    aws-c-sdkutils-0.2.2       |       h4e1184b_0          55 KB  conda-forge\n",
            "    aws-checksums-0.2.2        |       h4e1184b_4          71 KB  conda-forge\n",
            "    aws-crt-cpp-0.29.9         |       he0e7f3f_2         345 KB  conda-forge\n",
            "    aws-sdk-cpp-1.11.489       |       h4d475cb_0         3.0 MB  conda-forge\n",
            "    azure-core-cpp-1.14.0      |       h5cfcd09_0         337 KB  conda-forge\n",
            "    azure-identity-cpp-1.10.0  |       h113e628_0         227 KB  conda-forge\n",
            "    azure-storage-blobs-cpp-12.13.0|       h3cf044e_1         536 KB  conda-forge\n",
            "    azure-storage-common-cpp-12.8.0|       h736e048_1         146 KB  conda-forge\n",
            "    azure-storage-files-datalake-cpp-12.12.0|       ha633028_1         281 KB  conda-forge\n",
            "    click-8.2.1                |     pyh707e725_0          86 KB  conda-forge\n",
            "    dataclasses-0.8            |     pyhc8e2a94_3          10 KB  conda-forge\n",
            "    datasets-3.6.0             |     pyhd8ed1ab_0         331 KB  conda-forge\n",
            "    dill-0.3.8                 |     pyhd8ed1ab_0          86 KB  conda-forge\n",
            "    frozenlist-1.7.0           |  py311h52bc045_0          54 KB  conda-forge\n",
            "    fsspec-2025.3.0            |     pyhd8ed1ab_0         138 KB  conda-forge\n",
            "    gflags-2.2.2               |    h5888daf_1005         117 KB  conda-forge\n",
            "    glog-0.7.1                 |       hbabe93e_0         140 KB  conda-forge\n",
            "    hf-xet-1.1.8               |   py39h598437d_0         2.5 MB  conda-forge\n",
            "    huggingface_hub-0.34.4     |     pyhd8ed1ab_0         326 KB  conda-forge\n",
            "    importlib-metadata-8.7.0   |     pyhe01879c_1          34 KB  conda-forge\n",
            "    importlib_metadata-8.7.0   |       h40b2b14_1          22 KB  conda-forge\n",
            "    joblib-1.5.1               |     pyhd8ed1ab_0         219 KB  conda-forge\n",
            "    libabseil-20240722.0       | cxx17_hbbce691_4         1.3 MB  conda-forge\n",
            "    libarrow-19.0.1            |   hfa2a6e7_0_cpu         8.6 MB  conda-forge\n",
            "    libarrow-acero-19.0.1      |   hcb10f89_0_cpu         623 KB  conda-forge\n",
            "    libarrow-dataset-19.0.1    |   hcb10f89_0_cpu         590 KB  conda-forge\n",
            "    libarrow-substrait-19.0.1  |   h08228c5_0_cpu         511 KB  conda-forge\n",
            "    libbrotlicommon-1.1.0      |       hb9d3cd8_2          67 KB  conda-forge\n",
            "    libbrotlidec-1.1.0         |       hb9d3cd8_2          32 KB  conda-forge\n",
            "    libbrotlienc-1.1.0         |       hb9d3cd8_2         275 KB  conda-forge\n",
            "    libcrc32c-1.1.2            |       h9c3ff4c_0          20 KB  conda-forge\n",
            "    libevent-2.1.12            |       hf998b51_1         417 KB  conda-forge\n",
            "    libgoogle-cloud-2.35.0     |       h2b5623c_0         1.2 MB  conda-forge\n",
            "    libgoogle-cloud-storage-2.35.0|       h0121fbd_0         767 KB  conda-forge\n",
            "    libgrpc-1.67.1             |       h25350d4_2         7.8 MB  conda-forge\n",
            "    libopentelemetry-cpp-1.18.0|       hfcad708_1         783 KB  conda-forge\n",
            "    libopentelemetry-cpp-headers-1.18.0|       ha770c72_1         313 KB  conda-forge\n",
            "    libparquet-19.0.1          |   h081d1f1_0_cpu         1.2 MB  conda-forge\n",
            "    libprotobuf-5.28.3         |       h6128344_1         2.8 MB  conda-forge\n",
            "    libre2-11-2024.07.02       |       hbbce691_2         205 KB  conda-forge\n",
            "    libthrift-0.21.0           |       h0e7cc3e_0         416 KB  conda-forge\n",
            "    libutf8proc-2.10.0         |       h202a827_0          81 KB  conda-forge\n",
            "    multidict-6.6.3            |  py311h2dc5d0c_0          95 KB  conda-forge\n",
            "    multiprocess-0.70.16       |  py311h9ecbd09_1         340 KB  conda-forge\n",
            "    nlohmann_json-3.12.0       |       h3f2d84a_0         133 KB  conda-forge\n",
            "    orc-2.0.3                  |       h12ee42a_2         1.1 MB  conda-forge\n",
            "    pandas-2.3.1               |  py311hed34c8f_0        14.7 MB  conda-forge\n",
            "    prometheus-cpp-1.3.0       |       ha5d0236_0         195 KB  conda-forge\n",
            "    propcache-0.3.1            |  py311h2dc5d0c_0          53 KB  conda-forge\n",
            "    pyarrow-19.0.1             |  py311h38be061_0          25 KB  conda-forge\n",
            "    pyarrow-core-19.0.1        |py311h4854187_0_cpu         4.5 MB  conda-forge\n",
            "    python-dateutil-2.9.0.post0|     pyhe01879c_2         228 KB  conda-forge\n",
            "    python-gil-3.11.13         |       hd8ed1ab_0          46 KB  conda-forge\n",
            "    python-tzdata-2025.2       |     pyhd8ed1ab_0         141 KB  conda-forge\n",
            "    python-xxhash-3.5.0        |  py311h9ecbd09_2          23 KB  conda-forge\n",
            "    re2-2024.07.02             |       h9925aae_2          26 KB  conda-forge\n",
            "    regex-2025.7.34            |  py311h49ec1c0_0         406 KB  conda-forge\n",
            "    s2n-1.5.11                 |       h072c03f_0         348 KB  conda-forge\n",
            "    sacremoses-0.0.53          |     pyhd8ed1ab_0         427 KB  conda-forge\n",
            "    safetensors-0.6.2          |  py311hc8fb587_0         433 KB  conda-forge\n",
            "    six-1.17.0                 |     pyhe01879c_1          18 KB  conda-forge\n",
            "    snappy-1.2.2               |       h03e3b7b_0          45 KB  conda-forge\n",
            "    tokenizers-0.13.3          |  py311h1b04a43_0         3.9 MB  conda-forge\n",
            "    transformers-4.32.1        |     pyhd8ed1ab_0         2.5 MB  conda-forge\n",
            "    typing-extensions-4.14.1   |       h4440ef1_0          88 KB  conda-forge\n",
            "    xxhash-0.8.3               |       hb47aa4a_0         106 KB  conda-forge\n",
            "    yarl-1.20.1                |  py311h2dc5d0c_0         148 KB  conda-forge\n",
            "    zipp-3.23.0                |     pyhd8ed1ab_0          22 KB  conda-forge\n",
            "    zlib-1.3.1                 |       hb9d3cd8_2          90 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        68.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _python_abi3_supp~ conda-forge/noarch::_python_abi3_support-1.0-hd8ed1ab_2 \n",
            "  aiohappyeyeballs   conda-forge/noarch::aiohappyeyeballs-2.6.1-pyhd8ed1ab_0 \n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.12.15-py311h3778330_0 \n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.4.0-pyhd8ed1ab_0 \n",
            "  attrs              conda-forge/noarch::attrs-25.3.0-pyh71513ae_0 \n",
            "  aws-c-auth         conda-forge/linux-64::aws-c-auth-0.8.1-h205f482_0 \n",
            "  aws-c-cal          conda-forge/linux-64::aws-c-cal-0.8.1-h1a47875_3 \n",
            "  aws-c-common       conda-forge/linux-64::aws-c-common-0.10.6-hb9d3cd8_0 \n",
            "  aws-c-compression  conda-forge/linux-64::aws-c-compression-0.3.0-h4e1184b_5 \n",
            "  aws-c-event-stream conda-forge/linux-64::aws-c-event-stream-0.5.0-h7959bf6_11 \n",
            "  aws-c-http         conda-forge/linux-64::aws-c-http-0.9.2-hefd7a92_4 \n",
            "  aws-c-io           conda-forge/linux-64::aws-c-io-0.15.3-h173a860_6 \n",
            "  aws-c-mqtt         conda-forge/linux-64::aws-c-mqtt-0.11.0-h11f4f37_12 \n",
            "  aws-c-s3           conda-forge/linux-64::aws-c-s3-0.7.9-he1b24dc_1 \n",
            "  aws-c-sdkutils     conda-forge/linux-64::aws-c-sdkutils-0.2.2-h4e1184b_0 \n",
            "  aws-checksums      conda-forge/linux-64::aws-checksums-0.2.2-h4e1184b_4 \n",
            "  aws-crt-cpp        conda-forge/linux-64::aws-crt-cpp-0.29.9-he0e7f3f_2 \n",
            "  aws-sdk-cpp        conda-forge/linux-64::aws-sdk-cpp-1.11.489-h4d475cb_0 \n",
            "  azure-core-cpp     conda-forge/linux-64::azure-core-cpp-1.14.0-h5cfcd09_0 \n",
            "  azure-identity-cpp conda-forge/linux-64::azure-identity-cpp-1.10.0-h113e628_0 \n",
            "  azure-storage-blo~ conda-forge/linux-64::azure-storage-blobs-cpp-12.13.0-h3cf044e_1 \n",
            "  azure-storage-com~ conda-forge/linux-64::azure-storage-common-cpp-12.8.0-h736e048_1 \n",
            "  azure-storage-fil~ conda-forge/linux-64::azure-storage-files-datalake-cpp-12.12.0-ha633028_1 \n",
            "  click              conda-forge/noarch::click-8.2.1-pyh707e725_0 \n",
            "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyhc8e2a94_3 \n",
            "  datasets           conda-forge/noarch::datasets-3.6.0-pyhd8ed1ab_0 \n",
            "  dill               conda-forge/noarch::dill-0.3.8-pyhd8ed1ab_0 \n",
            "  frozenlist         conda-forge/linux-64::frozenlist-1.7.0-py311h52bc045_0 \n",
            "  fsspec             conda-forge/noarch::fsspec-2025.3.0-pyhd8ed1ab_0 \n",
            "  gflags             conda-forge/linux-64::gflags-2.2.2-h5888daf_1005 \n",
            "  glog               conda-forge/linux-64::glog-0.7.1-hbabe93e_0 \n",
            "  hf-xet             conda-forge/linux-64::hf-xet-1.1.8-py39h598437d_0 \n",
            "  huggingface_hub    conda-forge/noarch::huggingface_hub-0.34.4-pyhd8ed1ab_0 \n",
            "  importlib-metadata conda-forge/noarch::importlib-metadata-8.7.0-pyhe01879c_1 \n",
            "  importlib_metadata conda-forge/noarch::importlib_metadata-8.7.0-h40b2b14_1 \n",
            "  joblib             conda-forge/noarch::joblib-1.5.1-pyhd8ed1ab_0 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20240722.0-cxx17_hbbce691_4 \n",
            "  libarrow           conda-forge/linux-64::libarrow-19.0.1-hfa2a6e7_0_cpu \n",
            "  libarrow-acero     conda-forge/linux-64::libarrow-acero-19.0.1-hcb10f89_0_cpu \n",
            "  libarrow-dataset   conda-forge/linux-64::libarrow-dataset-19.0.1-hcb10f89_0_cpu \n",
            "  libarrow-substrait conda-forge/linux-64::libarrow-substrait-19.0.1-h08228c5_0_cpu \n",
            "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.1.0-hb9d3cd8_2 \n",
            "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.1.0-hb9d3cd8_2 \n",
            "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.1.0-hb9d3cd8_2 \n",
            "  libcrc32c          conda-forge/linux-64::libcrc32c-1.1.2-h9c3ff4c_0 \n",
            "  libevent           conda-forge/linux-64::libevent-2.1.12-hf998b51_1 \n",
            "  libgoogle-cloud    conda-forge/linux-64::libgoogle-cloud-2.35.0-h2b5623c_0 \n",
            "  libgoogle-cloud-s~ conda-forge/linux-64::libgoogle-cloud-storage-2.35.0-h0121fbd_0 \n",
            "  libgrpc            conda-forge/linux-64::libgrpc-1.67.1-h25350d4_2 \n",
            "  libopentelemetry-~ conda-forge/linux-64::libopentelemetry-cpp-1.18.0-hfcad708_1 \n",
            "  libopentelemetry-~ conda-forge/linux-64::libopentelemetry-cpp-headers-1.18.0-ha770c72_1 \n",
            "  libparquet         conda-forge/linux-64::libparquet-19.0.1-h081d1f1_0_cpu \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-5.28.3-h6128344_1 \n",
            "  libre2-11          conda-forge/linux-64::libre2-11-2024.07.02-hbbce691_2 \n",
            "  libthrift          conda-forge/linux-64::libthrift-0.21.0-h0e7cc3e_0 \n",
            "  libutf8proc        conda-forge/linux-64::libutf8proc-2.10.0-h202a827_0 \n",
            "  multidict          conda-forge/linux-64::multidict-6.6.3-py311h2dc5d0c_0 \n",
            "  multiprocess       conda-forge/linux-64::multiprocess-0.70.16-py311h9ecbd09_1 \n",
            "  nlohmann_json      conda-forge/linux-64::nlohmann_json-3.12.0-h3f2d84a_0 \n",
            "  orc                conda-forge/linux-64::orc-2.0.3-h12ee42a_2 \n",
            "  pandas             conda-forge/linux-64::pandas-2.3.1-py311hed34c8f_0 \n",
            "  prometheus-cpp     conda-forge/linux-64::prometheus-cpp-1.3.0-ha5d0236_0 \n",
            "  propcache          conda-forge/linux-64::propcache-0.3.1-py311h2dc5d0c_0 \n",
            "  pyarrow            conda-forge/linux-64::pyarrow-19.0.1-py311h38be061_0 \n",
            "  pyarrow-core       conda-forge/linux-64::pyarrow-core-19.0.1-py311h4854187_0_cpu \n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0.post0-pyhe01879c_2 \n",
            "  python-gil         conda-forge/noarch::python-gil-3.11.13-hd8ed1ab_0 \n",
            "  python-tzdata      conda-forge/noarch::python-tzdata-2025.2-pyhd8ed1ab_0 \n",
            "  python-xxhash      conda-forge/linux-64::python-xxhash-3.5.0-py311h9ecbd09_2 \n",
            "  re2                conda-forge/linux-64::re2-2024.07.02-h9925aae_2 \n",
            "  regex              conda-forge/linux-64::regex-2025.7.34-py311h49ec1c0_0 \n",
            "  s2n                conda-forge/linux-64::s2n-1.5.11-h072c03f_0 \n",
            "  sacremoses         conda-forge/noarch::sacremoses-0.0.53-pyhd8ed1ab_0 \n",
            "  safetensors        conda-forge/linux-64::safetensors-0.6.2-py311hc8fb587_0 \n",
            "  six                conda-forge/noarch::six-1.17.0-pyhe01879c_1 \n",
            "  snappy             conda-forge/linux-64::snappy-1.2.2-h03e3b7b_0 \n",
            "  tokenizers         conda-forge/linux-64::tokenizers-0.13.3-py311h1b04a43_0 \n",
            "  transformers       conda-forge/noarch::transformers-4.32.1-pyhd8ed1ab_0 \n",
            "  typing-extensions  conda-forge/noarch::typing-extensions-4.14.1-h4440ef1_0 \n",
            "  xxhash             conda-forge/linux-64::xxhash-0.8.3-hb47aa4a_0 \n",
            "  yarl               conda-forge/linux-64::yarl-1.20.1-py311h2dc5d0c_0 \n",
            "  zipp               conda-forge/noarch::zipp-3.23.0-pyhd8ed1ab_0 \n",
            "  zlib               conda-forge/linux-64::zlib-1.3.1-hb9d3cd8_2 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "pandas-2.3.1         | 14.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "libarrow-19.0.1      | 8.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "libgrpc-1.67.1       | 7.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pyarrow-core-19.0.1  | 4.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizers-0.13.3    | 3.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aws-sdk-cpp-1.11.489 | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-5.28.3   | 2.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "transformers-4.32.1  | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hf-xet-1.1.8         | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libabseil-20240722.0 | 1.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-2.35 | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libparquet-19.0.1    | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "orc-2.0.3            | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aiohttp-3.12.15      | 988 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopentelemetry-cpp | 783 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-stor | 767 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-acero-19.0. | 623 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-dataset-19. | 590 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "azure-storage-blobs- | 536 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pandas-2.3.1         | 14.7 MB   | :   0% 0.001065997434032788/1 [00:00<01:39, 99.21s/it]\n",
            "\n",
            "libgrpc-1.67.1       | 7.8 MB    | :   0% 0.0019999599617390473/1 [00:00<00:52, 52.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pyarrow-core-19.0.1  | 4.5 MB    | :   0% 0.003465760176334087/1 [00:00<00:30, 30.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pandas-2.3.1         | 14.7 MB   | :  14% 0.14497565102845916/1 [00:00<00:01,  1.23s/it] \n",
            "libarrow-19.0.1      | 8.6 MB    | :  19% 0.19365976754636863/1 [00:00<00:00,  1.08it/s]  \u001b[A\n",
            "\n",
            "libgrpc-1.67.1       | 7.8 MB    | :  26% 0.26199475498781516/1 [00:00<00:00,  1.44it/s]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pyarrow-core-19.0.1  | 4.5 MB    | :  39% 0.39163089992575184/1 [00:00<00:00,  2.17it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizers-0.13.3    | 3.9 MB    | :  74% 0.738330315441315/1 [00:00<00:00,  4.10it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pandas-2.3.1         | 14.7 MB   | :  52% 0.5212727452420333/1 [00:00<00:00,  2.13it/s] \n",
            "libarrow-19.0.1      | 8.6 MB    | :  65% 0.6504045023255399/1 [00:00<00:00,  2.63it/s] \u001b[A\n",
            "\n",
            "libgrpc-1.67.1       | 7.8 MB    | :  86% 0.8639827034712684/1 [00:00<00:00,  3.45it/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aws-sdk-cpp-1.11.489 | 3.0 MB    | :   1% 0.005210592666752323/1 [00:00<01:04, 65.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pyarrow-core-19.0.1  | 4.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.43it/s]                \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pyarrow-core-19.0.1  | 4.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pandas-2.3.1         | 14.7 MB   | :  74% 0.738736221784722/1 [00:00<00:00,  2.08it/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aws-sdk-cpp-1.11.489 | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.66it/s]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aws-sdk-cpp-1.11.489 | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pandas-2.3.1         | 14.7 MB   | :  95% 0.949803713723214/1 [00:00<00:00,  2.00it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-5.28.3   | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.37it/s]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-5.28.3   | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libarrow-19.0.1      | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.86it/s]               \u001b[A\n",
            "libarrow-19.0.1      | 8.6 MB    | : 100% 1.0/1 [00:00<00:00,  1.86it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hf-xet-1.1.8         | 2.5 MB    | :   1% 0.006302634150244581/1 [00:00<01:29, 89.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "transformers-4.32.1  | 2.5 MB    | :   1% 0.00614833611404443/1 [00:00<01:33, 93.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libabseil-20240722.0 | 1.3 MB    | :   1% 0.012491622820694435/1 [00:00<00:47, 48.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libabseil-20240722.0 | 1.3 MB    | : 100% 1.0/1 [00:00<00:00, 48.54s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-2.35 | 1.2 MB    | :   1% 0.013023485038174614/1 [00:00<00:49, 49.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hf-xet-1.1.8         | 2.5 MB    | : 100% 1.0/1 [00:00<00:00, 89.84s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "transformers-4.32.1  | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.93it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "transformers-4.32.1  | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libparquet-19.0.1    | 1.2 MB    | :   1% 0.013162493000596907/1 [00:00<00:52, 53.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-2.35 | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 49.76s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "orc-2.0.3            | 1.1 MB    | :   1% 0.013781026023630624/1 [00:00<00:52, 53.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pandas-2.3.1         | 14.7 MB   | : 100% 1.0/1 [00:00<00:00,  2.00it/s]              \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopentelemetry-cpp | 783 KB    | :   2% 0.020430787340992386/1 [00:00<00:36, 37.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aiohttp-3.12.15      | 988 KB    | :   2% 0.016199984377456473/1 [00:00<00:46, 47.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopentelemetry-cpp | 783 KB    | : 100% 1.0/1 [00:00<00:00, 37.32s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "orc-2.0.3            | 1.1 MB    | : 100% 1.0/1 [00:00<00:00, 53.18s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aiohttp-3.12.15      | 988 KB    | : 100% 1.0/1 [00:00<00:00, 47.54s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizers-0.13.3    | 3.9 MB    | : 100% 1.0/1 [00:00<00:00,  4.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-acero-19.0. | 623 KB    | :   3% 0.025678077404106863/1 [00:00<00:30, 31.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-stor | 767 KB    | :   2% 0.02085069937145017/1 [00:00<00:38, 38.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-acero-19.0. | 623 KB    | : 100% 1.0/1 [00:00<00:00, 31.24s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-stor | 767 KB    | : 100% 1.0/1 [00:00<00:00, 38.85s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "azure-storage-blobs- | 536 KB    | :   3% 0.029824772181992275/1 [00:00<00:27, 28.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-dataset-19. | 590 KB    | :   3% 0.02710339123242349/1 [00:00<00:31, 31.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "azure-storage-blobs- | 536 KB    | : 100% 1.0/1 [00:00<00:00, 28.78s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-dataset-19. | 590 KB    | : 100% 1.0/1 [00:00<00:00, 31.96s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pyarrow-core-19.0.1  | 4.5 MB    | : 100% 1.0/1 [00:01<00:00,  3.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libgrpc-1.67.1       | 7.8 MB    | : 100% 1.0/1 [00:02<00:00,  3.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aws-sdk-cpp-1.11.489 | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  2.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libprotobuf-5.28.3   | 2.8 MB    | : 100% 1.0/1 [00:02<00:00,  2.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libabseil-20240722.0 | 1.3 MB    | : 100% 1.0/1 [00:03<00:00,  2.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libabseil-20240722.0 | 1.3 MB    | : 100% 1.0/1 [00:03<00:00,  2.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libarrow-19.0.1      | 8.6 MB    | : 100% 1.0/1 [00:03<00:00,  1.86it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hf-xet-1.1.8         | 2.5 MB    | : 100% 1.0/1 [00:03<00:00,  3.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hf-xet-1.1.8         | 2.5 MB    | : 100% 1.0/1 [00:03<00:00,  3.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-2.35 | 1.2 MB    | : 100% 1.0/1 [00:03<00:00,  3.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-2.35 | 1.2 MB    | : 100% 1.0/1 [00:03<00:00,  3.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libparquet-19.0.1    | 1.2 MB    | : 100% 1.0/1 [00:03<00:00,  3.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libparquet-19.0.1    | 1.2 MB    | : 100% 1.0/1 [00:03<00:00,  3.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "transformers-4.32.1  | 2.5 MB    | : 100% 1.0/1 [00:04<00:00,  1.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "orc-2.0.3            | 1.1 MB    | : 100% 1.0/1 [00:04<00:00,  4.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "orc-2.0.3            | 1.1 MB    | : 100% 1.0/1 [00:04<00:00,  4.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopentelemetry-cpp | 783 KB    | : 100% 1.0/1 [00:04<00:00,  4.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopentelemetry-cpp | 783 KB    | : 100% 1.0/1 [00:04<00:00,  4.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aiohttp-3.12.15      | 988 KB    | : 100% 1.0/1 [00:04<00:00,  4.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "aiohttp-3.12.15      | 988 KB    | : 100% 1.0/1 [00:04<00:00,  4.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-acero-19.0. | 623 KB    | : 100% 1.0/1 [00:04<00:00,  4.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-acero-19.0. | 623 KB    | : 100% 1.0/1 [00:04<00:00,  4.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-stor | 767 KB    | : 100% 1.0/1 [00:04<00:00,  4.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgoogle-cloud-stor | 767 KB    | : 100% 1.0/1 [00:04<00:00,  4.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "azure-storage-blobs- | 536 KB    | : 100% 1.0/1 [00:04<00:00,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "azure-storage-blobs- | 536 KB    | : 100% 1.0/1 [00:04<00:00,  4.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarrow-dataset-19. | 590 KB    | : 100% 1.0/1 [00:05<00:00,  4.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pandas-2.3.1         | 14.7 MB   | : 100% 1.0/1 [00:06<00:00,  2.00it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (4.32.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (2.1.2)\n",
            "Collecting torchvision (from sentence-transformers==2.2.2)\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (1.26.3)\n",
            "Collecting scikit-learn (from sentence-transformers==2.2.2)\n",
            "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy (from sentence-transformers==2.2.2)\n",
            "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "Collecting nltk (from sentence-transformers==2.2.2)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting sentencepiece (from sentence-transformers==2.2.2)\n",
            "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers==2.2.2) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.1.8)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk->sentence-transformers==2.2.2) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/site-packages (from nltk->sentence-transformers==2.2.2) (1.5.1)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers==2.2.2)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers==2.2.2)\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->sentence-transformers==2.2.2)\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.6.0->sentence-transformers==2.2.2) (65.6.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.8.3)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m177.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=60902d6cdf0b782a3e5243aded692dcb9979df3362994574397f68db4a7640b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: nvidia-cusparselt-cu12, triton, threadpoolctl, sentencepiece, scipy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nltk, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, sentence-transformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2\n",
            "    Uninstalling torch-2.1.2:\n",
            "      Successfully uninstalled torch-2.1.2\n",
            "Successfully installed nltk-3.9.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-2.2.2 sentencepiece-0.2.1 threadpoolctl-3.6.0 torch-2.8.0 torchvision-0.23.0 triton-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "a6d2fab01f864bdfb658f46f613cd77b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda-forge::accelerate=0.25.0\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    accelerate-0.25.0          |     pyhd8ed1ab_0         175 KB  conda-forge\n",
            "    psutil-7.0.0               |  py311h9ecbd09_0         473 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         648 KB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  accelerate         conda-forge/noarch::accelerate-0.25.0-pyhd8ed1ab_0 \n",
            "  psutil             conda-forge/linux-64::psutil-7.0.0-py311h9ecbd09_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "psutil-7.0.0         | 473 KB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "psutil-7.0.0         | 473 KB    | : 100% 1.0/1 [00:00<00:00,  3.07s/it]                 \n",
            "psutil-7.0.0         | 473 KB    | : 100% 1.0/1 [00:00<00:00,  5.42it/s]\n",
            "accelerate-0.25.0    | 175 KB    | : 100% 1.0/1 [00:00<00:00,  1.99s/it]                \u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda-forge::bitsandbytes=0.42.0\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bitsandbytes-0.42.0        |  py311hccbc8e4_0         272 KB  conda-forge\n",
            "    scipy-1.16.0               |  py311h2d3ef60_0        16.1 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        16.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  bitsandbytes       conda-forge/linux-64::bitsandbytes-0.42.0-py311hccbc8e4_0 \n",
            "  scipy              conda-forge/linux-64::scipy-1.16.0-py311h2d3ef60_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "scipy-1.16.0         | 16.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "scipy-1.16.0         | 16.1 MB   | :   0% 0.0009680595876600291/1 [00:00<02:39, 159.42s/it]\n",
            "bitsandbytes-0.42.0  | 272 KB    | :   6% 0.058877010151828224/1 [00:00<00:02,  2.96s/it]\u001b[A\n",
            "scipy-1.16.0         | 16.1 MB   | :  16% 0.1636020703145449/1 [00:00<00:01,  1.27s/it]    \n",
            "bitsandbytes-0.42.0  | 272 KB    | : 100% 1.0/1 [00:00<00:00,  4.21it/s]\u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\bdone\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.8)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.11.0->sentence-transformers) (65.6.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
            "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, sentence-transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.32.1\n",
            "    Uninstalling transformers-4.32.1:\n",
            "      Successfully uninstalled transformers-4.32.1\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 2.2.2\n",
            "    Uninstalling sentence-transformers-2.2.2:\n",
            "      Successfully uninstalled sentence-transformers-2.2.2\n",
            "Successfully installed sentence-transformers-5.1.0 tokenizers-0.21.4 transformers-4.55.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Software Datase\n",
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "target_dir = \"/content/drive/MyDrive/Rec_Proj_DL/data/amazon\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Download the files\n",
        "!wget https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFiles/Software.json.gz\n",
        "!wget https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/metaFiles2/meta_Software.json.gz\n",
        "\n",
        "# Move the compressed files to the target directory\n",
        "shutil.move('/content/Software.json.gz', f'{target_dir}/Software.json.gz')\n",
        "shutil.move('/content/meta_Software.json.gz', f'{target_dir}/meta_Software.json.gz')\n",
        "\n",
        "# Extract the JSON files to the target directory\n",
        "with gzip.open(f'{target_dir}/Software.json.gz', 'rb') as f_in:\n",
        "    with open(f'{target_dir}/Software.json', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "with gzip.open(f'{target_dir}/meta_Software.json.gz', 'rb') as f_in:\n",
        "    with open(f'{target_dir}/meta_Software.json', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OjMIf1en9Nx7",
        "outputId": "a91c495e-6957-4bff-bc0f-4deb21d5864e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-19 16:21:54--  https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFiles/Software.json.gz\n",
            "Resolving mcauleylab.ucsd.edu (mcauleylab.ucsd.edu)... 169.228.63.88\n",
            "Connecting to mcauleylab.ucsd.edu (mcauleylab.ucsd.edu)|169.228.63.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97337106 (93M) [application/gzip]\n",
            "Saving to: ‘Software.json.gz’\n",
            "\n",
            "Software.json.gz    100%[===================>]  92.83M   104MB/s    in 0.9s    \n",
            "\n",
            "2025-08-19 16:21:55 (104 MB/s) - ‘Software.json.gz’ saved [97337106/97337106]\n",
            "\n",
            "--2025-08-19 16:21:55--  https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/metaFiles2/meta_Software.json.gz\n",
            "Resolving mcauleylab.ucsd.edu (mcauleylab.ucsd.edu)... 169.228.63.88\n",
            "Connecting to mcauleylab.ucsd.edu (mcauleylab.ucsd.edu)|169.228.63.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15236627 (15M) [application/gzip]\n",
            "Saving to: ‘meta_Software.json.gz’\n",
            "\n",
            "meta_Software.json. 100%[===================>]  14.53M  76.4MB/s    in 0.2s    \n",
            "\n",
            "2025-08-19 16:21:55 (76.4 MB/s) - ‘meta_Software.json.gz’ saved [15236627/15236627]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Rec_Proj_DL/pre_train/sasrec/main.py --device=cuda --dataset Software\n",
        "shutil.move('/content/Software', '/content/drive/MyDrive/Rec_Proj_DL/pre_train/sasrec/Software')"
      ],
      "metadata": {
        "id": "Du-3ugMSSQUb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train stage1\n",
        "!python /content/drive/MyDrive/Rec_Proj_DL/main.py --pretrain_stage1 --rec_pre_trained_data Software --num_epochs 10"
      ],
      "metadata": {
        "id": "adWqS5QoSzWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e3db76-65fd-442a-a868-a9da1b1e36b1",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A-LLMRec start train phase-1\n",
            "\n",
            "user num: 3577 item num: 4557\n",
            "average sequence length: 4.91\n",
            "Initializing with num_user: 3577\n",
            "  0% 0/10 [00:00<?, ?it/s]loss in epoch 1/11 iteration 0/111: 1.5891025066375732 / BPR loss: 1.386664867401123 / Matching loss: 0.008168688975274563 / Item reconstruction: 0.1626603603363037 / Text reconstruction: 0.5646936893463135\n",
            "loss in epoch 1/11 iteration 1/111: 1.5688492059707642 / BPR loss: 1.3867764472961426 / Matching loss: 0.0070985713973641396 / Item reconstruction: 0.155629500746727 / Text reconstruction: 0.4857977628707886\n",
            "loss in epoch 1/11 iteration 2/111: 1.5537662506103516 / BPR loss: 1.3865816593170166 / Matching loss: 0.005577250849455595 / Item reconstruction: 0.15000197291374207 / Text reconstruction: 0.4330321252346039\n",
            "loss in epoch 1/11 iteration 3/111: 1.5415055751800537 / BPR loss: 1.3866277933120728 / Matching loss: 0.004639504011720419 / Item reconstruction: 0.14458389580249786 / Text reconstruction: 0.38973182439804077\n",
            "loss in epoch 1/11 iteration 4/111: 1.529069423675537 / BPR loss: 1.3865175247192383 / Matching loss: 0.0034790062345564365 / Item reconstruction: 0.13847488164901733 / Text reconstruction: 0.34917712211608887\n",
            "loss in epoch 1/11 iteration 5/111: 1.5242242813110352 / BPR loss: 1.3865723609924316 / Matching loss: 0.0031870207749307156 / Item reconstruction: 0.13390542566776276 / Text reconstruction: 0.3375609517097473\n",
            "loss in epoch 1/11 iteration 6/111: 1.516426920890808 / BPR loss: 1.3865296840667725 / Matching loss: 0.002568344585597515 / Item reconstruction: 0.12872005999088287 / Text reconstruction: 0.3148442506790161\n",
            "loss in epoch 1/11 iteration 7/111: 1.5114779472351074 / BPR loss: 1.3865118026733398 / Matching loss: 0.0023478218354284763 / Item reconstruction: 0.12340099364519119 / Text reconstruction: 0.3045892119407654\n",
            "loss in epoch 1/11 iteration 8/111: 1.507333517074585 / BPR loss: 1.3865203857421875 / Matching loss: 0.0022385718766599894 / Item reconstruction: 0.11871479451656342 / Text reconstruction: 0.2960853576660156\n",
            "loss in epoch 1/11 iteration 9/111: 1.5021591186523438 / BPR loss: 1.3866026401519775 / Matching loss: 0.002041079569607973 / Item reconstruction: 0.1137261837720871 / Text reconstruction: 0.28326141834259033\n",
            "loss in epoch 1/11 iteration 10/111: 1.4975920915603638 / BPR loss: 1.386362075805664 / Matching loss: 0.0018827201565727592 / Item reconstruction: 0.11080829799175262 / Text reconstruction: 0.2697156071662903\n",
            "loss in epoch 1/11 iteration 11/111: 1.4937254190444946 / BPR loss: 1.3864092826843262 / Matching loss: 0.001741392770782113 / Item reconstruction: 0.10646060854196548 / Text reconstruction: 0.2617221474647522\n",
            "loss in epoch 1/11 iteration 12/111: 1.4904334545135498 / BPR loss: 1.3863165378570557 / Matching loss: 0.0015979179879650474 / Item reconstruction: 0.10226941108703613 / Text reconstruction: 0.25692135095596313\n",
            "loss in epoch 1/11 iteration 13/111: 1.4865901470184326 / BPR loss: 1.3862820863723755 / Matching loss: 0.0014691875549033284 / Item reconstruction: 0.0976882129907608 / Text reconstruction: 0.249973863363266\n",
            "loss in epoch 1/11 iteration 14/111: 1.483095645904541 / BPR loss: 1.3862605094909668 / Matching loss: 0.001351729966700077 / Item reconstruction: 0.09375870227813721 / Text reconstruction: 0.24301978945732117\n",
            "loss in epoch 1/11 iteration 15/111: 1.4802594184875488 / BPR loss: 1.3863036632537842 / Matching loss: 0.0012814507354050875 / Item reconstruction: 0.09054315090179443 / Text reconstruction: 0.2370133101940155\n",
            "loss in epoch 1/11 iteration 16/111: 1.4765244722366333 / BPR loss: 1.3862731456756592 / Matching loss: 0.0012487240601330996 / Item reconstruction: 0.08623987436294556 / Text reconstruction: 0.22941358387470245\n",
            "loss in epoch 1/11 iteration 17/111: 1.4737803936004639 / BPR loss: 1.3865412473678589 / Matching loss: 0.001213325303979218 / Item reconstruction: 0.08178980648517609 / Text reconstruction: 0.22565478086471558\n",
            "loss in epoch 1/11 iteration 18/111: 1.4707478284835815 / BPR loss: 1.3864684104919434 / Matching loss: 0.001169174793176353 / Item reconstruction: 0.07891996204853058 / Text reconstruction: 0.2182513028383255\n",
            "loss in epoch 1/11 iteration 19/111: 1.467565894126892 / BPR loss: 1.386512041091919 / Matching loss: 0.0011278267484158278 / Item reconstruction: 0.07522691041231155 / Text reconstruction: 0.2115626335144043\n",
            "loss in epoch 1/11 iteration 20/111: 1.464949131011963 / BPR loss: 1.3864219188690186 / Matching loss: 0.0010838157031685114 / Item reconstruction: 0.07243185490369797 / Text reconstruction: 0.20613734424114227\n",
            "loss in epoch 1/11 iteration 21/111: 1.4621546268463135 / BPR loss: 1.3863900899887085 / Matching loss: 0.0010228335158899426 / Item reconstruction: 0.06907208263874054 / Text reconstruction: 0.20102833211421967\n",
            "loss in epoch 1/11 iteration 22/111: 1.4597740173339844 / BPR loss: 1.3864691257476807 / Matching loss: 0.0009521075407974422 / Item reconstruction: 0.06670604646205902 / Text reconstruction: 0.19499874114990234\n",
            "loss in epoch 1/11 iteration 23/111: 1.4571624994277954 / BPR loss: 1.3863835334777832 / Matching loss: 0.0008822871604934335 / Item reconstruction: 0.06355597078800201 / Text reconstruction: 0.19059371948242188\n",
            "loss in epoch 1/11 iteration 24/111: 1.4547441005706787 / BPR loss: 1.3864377737045288 / Matching loss: 0.000804693263489753 / Item reconstruction: 0.06074110418558121 / Text reconstruction: 0.1856553852558136\n",
            "loss in epoch 1/11 iteration 25/111: 1.4525527954101562 / BPR loss: 1.386408805847168 / Matching loss: 0.0007523667882196605 / Item reconstruction: 0.0582699291408062 / Text reconstruction: 0.1812836229801178\n",
            "loss in epoch 1/11 iteration 26/111: 1.4504839181900024 / BPR loss: 1.3864107131958008 / Matching loss: 0.0007091402658261359 / Item reconstruction: 0.05598767101764679 / Text reconstruction: 0.17685100436210632\n",
            "loss in epoch 1/11 iteration 27/111: 1.448256492614746 / BPR loss: 1.3863835334777832 / Matching loss: 0.0006657125777564943 / Item reconstruction: 0.05328012630343437 / Text reconstruction: 0.17283599078655243\n",
            "loss in epoch 1/11 iteration 28/111: 1.446062684059143 / BPR loss: 1.386404275894165 / Matching loss: 0.0006347845774143934 / Item reconstruction: 0.05063619464635849 / Text reconstruction: 0.16852737963199615\n",
            "loss in epoch 1/11 iteration 29/111: 1.4442787170410156 / BPR loss: 1.3863654136657715 / Matching loss: 0.0005982804577797651 / Item reconstruction: 0.04877086356282234 / Text reconstruction: 0.16464780271053314\n",
            "loss in epoch 1/11 iteration 30/111: 1.4424394369125366 / BPR loss: 1.386418104171753 / Matching loss: 0.0005760022904723883 / Item reconstruction: 0.04661540314555168 / Text reconstruction: 0.1606884002685547\n",
            "loss in epoch 1/11 iteration 31/111: 1.4405708312988281 / BPR loss: 1.38640558719635 / Matching loss: 0.0005533825606107712 / Item reconstruction: 0.04476465284824371 / Text reconstruction: 0.15614771842956543\n",
            "loss in epoch 1/11 iteration 32/111: 1.4387272596359253 / BPR loss: 1.3864160776138306 / Matching loss: 0.0005275896983221173 / Item reconstruction: 0.04267416149377823 / Text reconstruction: 0.15223249793052673\n",
            "loss in epoch 1/11 iteration 33/111: 1.4371733665466309 / BPR loss: 1.386407732963562 / Matching loss: 0.000509939796756953 / Item reconstruction: 0.04111681133508682 / Text reconstruction: 0.14848637580871582\n",
            "loss in epoch 1/11 iteration 34/111: 1.435286521911621 / BPR loss: 1.3863264322280884 / Matching loss: 0.0004756795824505389 / Item reconstruction: 0.03895946964621544 / Text reconstruction: 0.14502370357513428\n",
            "loss in epoch 1/11 iteration 35/111: 1.433637022972107 / BPR loss: 1.386428713798523 / Matching loss: 0.00045140215661376715 / Item reconstruction: 0.03677342087030411 / Text reconstruction: 0.14185073971748352\n",
            "loss in epoch 1/11 iteration 36/111: 1.4323093891143799 / BPR loss: 1.3864351511001587 / Matching loss: 0.00042300435598008335 / Item reconstruction: 0.03557376563549042 / Text reconstruction: 0.1383219063282013\n",
            "loss in epoch 1/11 iteration 37/111: 1.4307997226715088 / BPR loss: 1.3863301277160645 / Matching loss: 0.00040246263961307704 / Item reconstruction: 0.0340079627931118 / Text reconstruction: 0.1353156566619873\n",
            "loss in epoch 1/11 iteration 38/111: 1.4293915033340454 / BPR loss: 1.3863871097564697 / Matching loss: 0.00038296356797218323 / Item reconstruction: 0.03231639787554741 / Text reconstruction: 0.1323154866695404\n",
            "loss in epoch 1/11 iteration 39/111: 1.428063154220581 / BPR loss: 1.3863811492919922 / Matching loss: 0.00036366822314448655 / Item reconstruction: 0.031007282435894012 / Text reconstruction: 0.12907356023788452\n",
            "loss in epoch 1/11 iteration 40/111: 1.4266786575317383 / BPR loss: 1.3863195180892944 / Matching loss: 0.00034766586031764746 / Item reconstruction: 0.029474198818206787 / Text reconstruction: 0.12637218832969666\n",
            "loss in epoch 1/11 iteration 41/111: 1.4253462553024292 / BPR loss: 1.3862956762313843 / Matching loss: 0.00034125911770388484 / Item reconstruction: 0.02813626453280449 / Text reconstruction: 0.12320588529109955\n",
            "loss in epoch 1/11 iteration 42/111: 1.4240849018096924 / BPR loss: 1.3863370418548584 / Matching loss: 0.00033229723339900374 / Item reconstruction: 0.026602070778608322 / Text reconstruction: 0.12057223916053772\n",
            "loss in epoch 1/11 iteration 43/111: 1.4231135845184326 / BPR loss: 1.3863112926483154 / Matching loss: 0.000322461302857846 / Item reconstruction: 0.025820530951023102 / Text reconstruction: 0.11784821003675461\n",
            "loss in epoch 1/11 iteration 44/111: 1.4216959476470947 / BPR loss: 1.3863104581832886 / Matching loss: 0.0003060100134462118 / Item reconstruction: 0.024105872958898544 / Text reconstruction: 0.11513282358646393\n",
            "loss in epoch 1/11 iteration 45/111: 1.4206749200820923 / BPR loss: 1.3863694667816162 / Matching loss: 0.00030012201750651 / Item reconstruction: 0.02298858016729355 / Text reconstruction: 0.11255475133657455\n",
            "loss in epoch 1/11 iteration 46/111: 1.419769525527954 / BPR loss: 1.3863520622253418 / Matching loss: 0.00029658660059794784 / Item reconstruction: 0.022272594273090363 / Text reconstruction: 0.10992282629013062\n",
            "loss in epoch 1/11 iteration 47/111: 1.418549656867981 / BPR loss: 1.3863080739974976 / Matching loss: 0.00028442865004763007 / Item reconstruction: 0.020922759547829628 / Text reconstruction: 0.1074792742729187\n",
            "loss in epoch 1/11 iteration 48/111: 1.417708158493042 / BPR loss: 1.3863425254821777 / Matching loss: 0.00027816067449748516 / Item reconstruction: 0.020134761929512024 / Text reconstruction: 0.10510116070508957\n",
            "loss in epoch 1/11 iteration 49/111: 1.4166738986968994 / BPR loss: 1.386335849761963 / Matching loss: 0.0002697753952816129 / Item reconstruction: 0.01893063820898533 / Text reconstruction: 0.10301443189382553\n",
            "loss in epoch 1/11 iteration 50/111: 1.415890097618103 / BPR loss: 1.3863918781280518 / Matching loss: 0.00025467705563642085 / Item reconstruction: 0.018116354942321777 / Text reconstruction: 0.10092654079198837\n",
            "loss in epoch 1/11 iteration 51/111: 1.4150022268295288 / BPR loss: 1.3863381147384644 / Matching loss: 0.00024199231120292097 / Item reconstruction: 0.017285894602537155 / Text reconstruction: 0.0988960936665535\n",
            "loss in epoch 1/11 iteration 52/111: 1.4139482975006104 / BPR loss: 1.3862775564193726 / Matching loss: 0.00023372357827611268 / Item reconstruction: 0.01616688445210457 / Text reconstruction: 0.09676764905452728\n",
            "loss in epoch 1/11 iteration 53/111: 1.4131983518600464 / BPR loss: 1.386256217956543 / Matching loss: 0.00022516498574987054 / Item reconstruction: 0.01540948636829853 / Text reconstruction: 0.09506095945835114\n",
            "loss in epoch 1/11 iteration 54/111: 1.4125419855117798 / BPR loss: 1.3863117694854736 / Matching loss: 0.00021786733123008162 / Item reconstruction: 0.01481065433472395 / Text reconstruction: 0.09303505718708038\n",
            "loss in epoch 1/11 iteration 55/111: 1.4117783308029175 / BPR loss: 1.3863072395324707 / Matching loss: 0.00021613360149785876 / Item reconstruction: 0.01404937356710434 / Text reconstruction: 0.09115162491798401\n",
            "loss in epoch 1/11 iteration 56/111: 1.4110337495803833 / BPR loss: 1.3862833976745605 / Matching loss: 0.00021336841746233404 / Item reconstruction: 0.013301793485879898 / Text reconstruction: 0.0894303247332573\n",
            "loss in epoch 1/11 iteration 57/111: 1.4103765487670898 / BPR loss: 1.386335015296936 / Matching loss: 0.00020548095926642418 / Item reconstruction: 0.012661846354603767 / Text reconstruction: 0.08752529323101044\n",
            "loss in epoch 1/11 iteration 58/111: 1.4096027612686157 / BPR loss: 1.3862769603729248 / Matching loss: 0.0002022896078415215 / Item reconstruction: 0.01187120284885168 / Text reconstruction: 0.0859399363398552\n",
            "loss in epoch 1/11 iteration 59/111: 1.4090207815170288 / BPR loss: 1.3862547874450684 / Matching loss: 0.0002000275708269328 / Item reconstruction: 0.011415020562708378 / Text reconstruction: 0.08429227769374847\n",
            "loss in epoch 1/11 iteration 60/111: 1.4084727764129639 / BPR loss: 1.3863109350204468 / Matching loss: 0.00019486495875753462 / Item reconstruction: 0.010916810482740402 / Text reconstruction: 0.08254318684339523\n",
            "loss in epoch 1/11 iteration 61/111: 1.4078360795974731 / BPR loss: 1.3863332271575928 / Matching loss: 0.00019815529230982065 / Item reconstruction: 0.010241677053272724 / Text reconstruction: 0.08091927319765091\n",
            "loss in epoch 1/11 iteration 62/111: 1.407357096672058 / BPR loss: 1.3863232135772705 / Matching loss: 0.00019797278218902647 / Item reconstruction: 0.009865171276032925 / Text reconstruction: 0.07951641082763672\n",
            "loss in epoch 1/11 iteration 63/111: 1.4066439867019653 / BPR loss: 1.3863093852996826 / Matching loss: 0.0001920493523357436 / Item reconstruction: 0.009134141728281975 / Text reconstruction: 0.077877476811409\n",
            "loss in epoch 1/11 iteration 64/111: 1.406113624572754 / BPR loss: 1.3862581253051758 / Matching loss: 0.0001852001587394625 / Item reconstruction: 0.008740037679672241 / Text reconstruction: 0.07650114595890045\n",
            "loss in epoch 1/11 iteration 65/111: 1.4056777954101562 / BPR loss: 1.3862971067428589 / Matching loss: 0.00018014035595115274 / Item reconstruction: 0.00838758796453476 / Text reconstruction: 0.07503381371498108\n",
            "loss in epoch 1/11 iteration 66/111: 1.4053797721862793 / BPR loss: 1.3863801956176758 / Matching loss: 0.00017671276873443276 / Item reconstruction: 0.00811871699988842 / Text reconstruction: 0.0738181471824646\n",
            "loss in epoch 1/11 iteration 67/111: 1.4047255516052246 / BPR loss: 1.3863239288330078 / Matching loss: 0.00017184216994792223 / Item reconstruction: 0.007466706447303295 / Text reconstruction: 0.07248144596815109\n",
            "loss in epoch 1/11 iteration 68/111: 1.4043269157409668 / BPR loss: 1.3863188028335571 / Matching loss: 0.0001695909013506025 / Item reconstruction: 0.007191759534180164 / Text reconstruction: 0.07121296226978302\n",
            "loss in epoch 1/11 iteration 69/111: 1.4038684368133545 / BPR loss: 1.3863180875778198 / Matching loss: 0.0001658937253523618 / Item reconstruction: 0.006795598194003105 / Text reconstruction: 0.06993275880813599\n",
            "loss in epoch 1/11 iteration 70/111: 1.4034138917922974 / BPR loss: 1.3862954378128052 / Matching loss: 0.000164671684615314 / Item reconstruction: 0.006395810283720493 / Text reconstruction: 0.0687793493270874\n",
            "loss in epoch 1/11 iteration 71/111: 1.4030381441116333 / BPR loss: 1.3862521648406982 / Matching loss: 0.00016183042316697538 / Item reconstruction: 0.006206981372088194 / Text reconstruction: 0.06760276854038239\n",
            "loss in epoch 1/11 iteration 72/111: 1.4026373624801636 / BPR loss: 1.3862557411193848 / Matching loss: 0.00015926352352835238 / Item reconstruction: 0.005876464769244194 / Text reconstruction: 0.06642059981822968\n",
            "loss in epoch 1/11 iteration 73/111: 1.402296781539917 / BPR loss: 1.3862687349319458 / Matching loss: 0.00015920816804282367 / Item reconstruction: 0.005618748255074024 / Text reconstruction: 0.06529673933982849\n",
            "loss in epoch 1/11 iteration 74/111: 1.401944637298584 / BPR loss: 1.386275291442871 / Matching loss: 0.0001595871290192008 / Item reconstruction: 0.005341797601431608 / Text reconstruction: 0.06419406831264496\n",
            "loss in epoch 1/11 iteration 75/111: 1.401569128036499 / BPR loss: 1.3862559795379639 / Matching loss: 0.0001546854618936777 / Item reconstruction: 0.0050778938457369804 / Text reconstruction: 0.06309744715690613\n",
            "loss in epoch 1/11 iteration 76/111: 1.4012967348098755 / BPR loss: 1.3862357139587402 / Matching loss: 0.00015657866606488824 / Item reconstruction: 0.004950419533997774 / Text reconstruction: 0.06214608997106552\n",
            "loss in epoch 1/11 iteration 77/111: 1.4009122848510742 / BPR loss: 1.3862876892089844 / Matching loss: 0.00015463904128409922 / Item reconstruction: 0.004524017684161663 / Text reconstruction: 0.06103980541229248\n",
            "loss in epoch 1/11 iteration 78/111: 1.400742530822754 / BPR loss: 1.386338233947754 / Matching loss: 0.00015183583309408277 / Item reconstruction: 0.00444955937564373 / Text reconstruction: 0.060137856751680374\n",
            "loss in epoch 1/11 iteration 79/111: 1.400378704071045 / BPR loss: 1.3862724304199219 / Matching loss: 0.0001493759627919644 / Item reconstruction: 0.004217637237161398 / Text reconstruction: 0.05924047902226448\n",
            "loss in epoch 1/11 iteration 80/111: 1.4000599384307861 / BPR loss: 1.3862967491149902 / Matching loss: 0.00014441768871620297 / Item reconstruction: 0.003939139656722546 / Text reconstruction: 0.05824636295437813\n",
            "loss in epoch 1/11 iteration 81/111: 1.3997477293014526 / BPR loss: 1.386206865310669 / Matching loss: 0.00014312262646853924 / Item reconstruction: 0.003834841074422002 / Text reconstruction: 0.05740165337920189\n",
            "loss in epoch 1/11 iteration 82/111: 1.399474859237671 / BPR loss: 1.3862628936767578 / Matching loss: 0.00014368742995429784 / Item reconstruction: 0.003542832564562559 / Text reconstruction: 0.05648435652256012\n",
            "loss in epoch 1/11 iteration 83/111: 1.3994086980819702 / BPR loss: 1.3863599300384521 / Matching loss: 0.0001461518695577979 / Item reconstruction: 0.003578656818717718 / Text reconstruction: 0.05556660145521164\n",
            "loss in epoch 1/11 iteration 84/111: 1.3989949226379395 / BPR loss: 1.3862857818603516 / Matching loss: 0.00014013370673637837 / Item reconstruction: 0.0032217162661254406 / Text reconstruction: 0.05479062348604202\n",
            "loss in epoch 1/11 iteration 85/111: 1.3988583087921143 / BPR loss: 1.3863167762756348 / Matching loss: 0.0001369940728181973 / Item reconstruction: 0.00323568657040596 / Text reconstruction: 0.05393397435545921\n",
            "loss in epoch 1/11 iteration 86/111: 1.3985992670059204 / BPR loss: 1.3862757682800293 / Matching loss: 0.00013433564163278788 / Item reconstruction: 0.003102845512330532 / Text reconstruction: 0.05318862199783325\n",
            "loss in epoch 1/11 iteration 87/111: 1.3984103202819824 / BPR loss: 1.3863227367401123 / Matching loss: 0.00013362042955122888 / Item reconstruction: 0.00291926646605134 / Text reconstruction: 0.0524718351662159\n",
            "loss in epoch 1/11 iteration 88/111: 1.3982123136520386 / BPR loss: 1.386293649673462 / Matching loss: 0.00013610292808152735 / Item reconstruction: 0.002829663921147585 / Text reconstruction: 0.05183901637792587\n",
            "loss in epoch 1/11 iteration 89/111: 1.3980721235275269 / BPR loss: 1.386344313621521 / Matching loss: 0.0001369465608149767 / Item reconstruction: 0.0027806994039565325 / Text reconstruction: 0.051002614200115204\n",
            "loss in epoch 1/11 iteration 90/111: 1.3977717161178589 / BPR loss: 1.3863141536712646 / Matching loss: 0.0001340247836196795 / Item reconstruction: 0.0025212951004505157 / Text reconstruction: 0.050314467400312424\n",
            "loss in epoch 1/11 iteration 91/111: 1.397540807723999 / BPR loss: 1.3862794637680054 / Matching loss: 0.00013460063200909644 / Item reconstruction: 0.002424219623208046 / Text reconstruction: 0.0495733842253685\n",
            "loss in epoch 1/11 iteration 92/111: 1.397343635559082 / BPR loss: 1.3862683773040771 / Matching loss: 0.00013226873124949634 / Item reconstruction: 0.0023734630085527897 / Text reconstruction: 0.04878108948469162\n",
            "loss in epoch 1/11 iteration 93/111: 1.3972004652023315 / BPR loss: 1.3862743377685547 / Matching loss: 0.00012795694055967033 / Item reconstruction: 0.002299051731824875 / Text reconstruction: 0.04824341833591461\n",
            "loss in epoch 1/11 iteration 94/111: 1.3970144987106323 / BPR loss: 1.3862553834915161 / Matching loss: 0.00012728451110888273 / Item reconstruction: 0.0021982965990900993 / Text reconstruction: 0.047663453966379166\n",
            "loss in epoch 1/11 iteration 95/111: 1.3968247175216675 / BPR loss: 1.3862364292144775 / Matching loss: 0.00012509588850662112 / Item reconstruction: 0.002109925262629986 / Text reconstruction: 0.047041356563568115\n",
            "loss in epoch 1/11 iteration 96/111: 1.3966947793960571 / BPR loss: 1.3863004446029663 / Matching loss: 0.00012421610881574452 / Item reconstruction: 0.0019769633654505014 / Text reconstruction: 0.046408288180828094\n",
            "loss in epoch 1/11 iteration 97/111: 1.3966141939163208 / BPR loss: 1.3863334655761719 / Matching loss: 0.0001254687231266871 / Item reconstruction: 0.0019550214055925608 / Text reconstruction: 0.04588858410716057\n",
            "loss in epoch 1/11 iteration 98/111: 1.3964663743972778 / BPR loss: 1.3863531351089478 / Matching loss: 0.00012537039583548903 / Item reconstruction: 0.001886834972538054 / Text reconstruction: 0.04522212594747543\n",
            "loss in epoch 1/11 iteration 99/111: 1.3962047100067139 / BPR loss: 1.3862249851226807 / Matching loss: 0.00012689261347986758 / Item reconstruction: 0.001833037706092 / Text reconstruction: 0.044682297855615616\n",
            "loss in epoch 1/11 iteration 100/111: 1.396135926246643 / BPR loss: 1.3862829208374023 / Matching loss: 0.00012304249685257673 / Item reconstruction: 0.001826226245611906 / Text reconstruction: 0.04408424720168114\n",
            "loss in epoch 1/11 iteration 101/111: 1.3960131406784058 / BPR loss: 1.3863227367401123 / Matching loss: 0.00011990184430032969 / Item reconstruction: 0.0016928932163864374 / Text reconstruction: 0.04361965134739876\n",
            "loss in epoch 1/11 iteration 102/111: 1.3958665132522583 / BPR loss: 1.3862380981445312 / Matching loss: 0.00011721665214281529 / Item reconstruction: 0.0017905703280121088 / Text reconstruction: 0.043079957365989685\n",
            "loss in epoch 1/11 iteration 103/111: 1.3958431482315063 / BPR loss: 1.3863588571548462 / Matching loss: 0.00011564225860638544 / Item reconstruction: 0.0016864156350493431 / Text reconstruction: 0.042627301067113876\n",
            "loss in epoch 1/11 iteration 104/111: 1.3956470489501953 / BPR loss: 1.3863275051116943 / Matching loss: 0.00011749043187592179 / Item reconstruction: 0.001559457741677761 / Text reconstruction: 0.04211121425032616\n",
            "loss in epoch 1/11 iteration 105/111: 1.3954657316207886 / BPR loss: 1.3862489461898804 / Matching loss: 0.00012018992856610566 / Item reconstruction: 0.001559139578603208 / Text reconstruction: 0.041585251688957214\n",
            "loss in epoch 1/11 iteration 106/111: 1.395313024520874 / BPR loss: 1.386225700378418 / Matching loss: 0.00012221978977322578 / Item reconstruction: 0.0014815421309322119 / Text reconstruction: 0.04112182557582855\n",
            "loss in epoch 1/11 iteration 107/111: 1.3952308893203735 / BPR loss: 1.3862347602844238 / Matching loss: 0.00011965230805799365 / Item reconstruction: 0.0015005709137767553 / Text reconstruction: 0.04063092917203903\n",
            "loss in epoch 1/11 iteration 108/111: 1.395164966583252 / BPR loss: 1.386287808418274 / Matching loss: 0.00011439912486821413 / Item reconstruction: 0.0014523363206535578 / Text reconstruction: 0.04018250107765198\n",
            "loss in epoch 1/11 iteration 109/111: 1.3950644731521606 / BPR loss: 1.3863024711608887 / Matching loss: 0.00011242464097449556 / Item reconstruction: 0.0013704325538128614 / Text reconstruction: 0.039821870625019073\n",
            "loss in epoch 1/11 iteration 110/111: 1.3949329853057861 / BPR loss: 1.3862576484680176 / Matching loss: 0.00011005044507328421 / Item reconstruction: 0.0012397854588925838 / Text reconstruction: 0.03972688317298889\n",
            "loss in epoch 1/11 iteration 111/111: 1.394826889038086 / BPR loss: 1.3862674236297607 / Matching loss: 0.00011013707262463868 / Item reconstruction: 0.0010930123971775174 / Text reconstruction: 0.039514441043138504\n",
            " 10% 1/10 [05:24<48:44, 325.00s/it]loss in epoch 2/11 iteration 0/111: 1.3947434425354004 / BPR loss: 1.3862667083740234 / Matching loss: 0.00011343858204782009 / Item reconstruction: 0.0013118695933371782 / Text reconstruction: 0.03853681683540344\n",
            "loss in epoch 2/11 iteration 1/111: 1.3946752548217773 / BPR loss: 1.3862745761871338 / Matching loss: 0.00011286522203590721 / Item reconstruction: 0.0012981407344341278 / Text reconstruction: 0.03819366544485092\n",
            "loss in epoch 2/11 iteration 2/111: 1.3944991827011108 / BPR loss: 1.386213779449463 / Matching loss: 0.00011656236893031746 / Item reconstruction: 0.0012542399344965816 / Text reconstruction: 0.037708207964897156\n",
            "loss in epoch 2/11 iteration 3/111: 1.394497275352478 / BPR loss: 1.38629150390625 / Matching loss: 0.00011304793588351458 / Item reconstruction: 0.001302843214944005 / Text reconstruction: 0.03720669820904732\n",
            "loss in epoch 2/11 iteration 4/111: 1.3943696022033691 / BPR loss: 1.3862587213516235 / Matching loss: 0.00011176714906468987 / Item reconstruction: 0.0012642692308872938 / Text reconstruction: 0.036834388971328735\n",
            "loss in epoch 2/11 iteration 5/111: 1.39430832862854 / BPR loss: 1.3862828016281128 / Matching loss: 0.00010711484355852008 / Item reconstruction: 0.0011860430240631104 / Text reconstruction: 0.036626435816287994\n",
            "loss in epoch 2/11 iteration 6/111: 1.394274115562439 / BPR loss: 1.3862900733947754 / Matching loss: 0.00010871454287553206 / Item reconstruction: 0.0012678352650254965 / Text reconstruction: 0.036207038909196854\n",
            "loss in epoch 2/11 iteration 7/111: 1.3941521644592285 / BPR loss: 1.3862874507904053 / Matching loss: 0.00010557702626101673 / Item reconstruction: 0.0011720963520929217 / Text reconstruction: 0.035865381360054016\n",
            "loss in epoch 2/11 iteration 8/111: 1.3940486907958984 / BPR loss: 1.386263132095337 / Matching loss: 0.00010525692778173834 / Item reconstruction: 0.0010984543478116393 / Text reconstruction: 0.035655710846185684\n",
            "loss in epoch 2/11 iteration 9/111: 1.394036889076233 / BPR loss: 1.386275053024292 / Matching loss: 0.00010870709957089275 / Item reconstruction: 0.001228415290825069 / Text reconstruction: 0.035194821655750275\n",
            "loss in epoch 2/11 iteration 10/111: 1.393990159034729 / BPR loss: 1.3860657215118408 / Matching loss: 0.00011553682270459831 / Item reconstruction: 0.001653806772083044 / Text reconstruction: 0.034910041838884354\n",
            "loss in epoch 2/11 iteration 11/111: 1.3938723802566528 / BPR loss: 1.3860509395599365 / Matching loss: 0.00010994228796334937 / Item reconstruction: 0.0015666158869862556 / Text reconstruction: 0.03464086726307869\n",
            "loss in epoch 2/11 iteration 12/111: 1.3937784433364868 / BPR loss: 1.3860108852386475 / Matching loss: 0.00010474944429006428 / Item reconstruction: 0.0015820376574993134 / Text reconstruction: 0.034358493983745575\n",
            "loss in epoch 2/11 iteration 13/111: 1.3938010931015015 / BPR loss: 1.3861114978790283 / Matching loss: 0.00010831472172867507 / Item reconstruction: 0.0015081902965903282 / Text reconstruction: 0.03413545340299606\n",
            "loss in epoch 2/11 iteration 14/111: 1.3936585187911987 / BPR loss: 1.3861069679260254 / Matching loss: 0.00011072318739024922 / Item reconstruction: 0.0013665567385032773 / Text reconstruction: 0.03378757834434509\n",
            "loss in epoch 2/11 iteration 15/111: 1.3935878276824951 / BPR loss: 1.3861119747161865 / Matching loss: 0.0001072572631528601 / Item reconstruction: 0.001346504781395197 / Text reconstruction: 0.033476267009973526\n",
            "loss in epoch 2/11 iteration 16/111: 1.3934887647628784 / BPR loss: 1.3861199617385864 / Matching loss: 0.00010950842988677323 / Item reconstruction: 0.0012683140812441707 / Text reconstruction: 0.033125124871730804\n",
            "loss in epoch 2/11 iteration 17/111: 1.3935315608978271 / BPR loss: 1.3862675428390503 / Matching loss: 0.0001065301476046443 / Item reconstruction: 0.0012065933551639318 / Text reconstruction: 0.032770514488220215\n",
            "loss in epoch 2/11 iteration 18/111: 1.3934719562530518 / BPR loss: 1.3862569332122803 / Matching loss: 0.00010272934014210477 / Item reconstruction: 0.0012448537163436413 / Text reconstruction: 0.03244940564036369\n",
            "loss in epoch 2/11 iteration 19/111: 1.3934193849563599 / BPR loss: 1.3862724304199219 / Matching loss: 9.979441529139876e-05 / Item reconstruction: 0.001211707480251789 / Text reconstruction: 0.032206952571868896\n",
            "loss in epoch 2/11 iteration 20/111: 1.3933511972427368 / BPR loss: 1.3862619400024414 / Matching loss: 0.0001018252078210935 / Item reconstruction: 0.0012121829204261303 / Text reconstruction: 0.03190690279006958\n",
            "loss in epoch 2/11 iteration 21/111: 1.393314003944397 / BPR loss: 1.386303186416626 / Matching loss: 0.00010127655696123838 / Item reconstruction: 0.0011071396293118596 / Text reconstruction: 0.031779222190380096\n",
            "loss in epoch 2/11 iteration 22/111: 1.3932548761367798 / BPR loss: 1.386281967163086 / Matching loss: 0.00010463017679285258 / Item reconstruction: 0.0011436149943619967 / Text reconstruction: 0.03148169815540314\n",
            "loss in epoch 2/11 iteration 23/111: 1.3931385278701782 / BPR loss: 1.3862735033035278 / Matching loss: 9.938984294421971e-05 / Item reconstruction: 0.001075565698556602 / Text reconstruction: 0.03113909810781479\n",
            "loss in epoch 2/11 iteration 24/111: 1.3931032419204712 / BPR loss: 1.3862910270690918 / Matching loss: 9.952099935617298e-05 / Item reconstruction: 0.0010619231034070253 / Text reconstruction: 0.03090861067175865\n",
            "loss in epoch 2/11 iteration 25/111: 1.393051028251648 / BPR loss: 1.3862998485565186 / Matching loss: 9.792193304747343e-05 / Item reconstruction: 0.0010342064779251814 / Text reconstruction: 0.030680915340781212\n",
            "loss in epoch 2/11 iteration 26/111: 1.3929858207702637 / BPR loss: 1.3862743377685547 / Matching loss: 0.00010426492372062057 / Item reconstruction: 0.0010127769783139229 / Text reconstruction: 0.030504075810313225\n",
            "loss in epoch 2/11 iteration 27/111: 1.3929553031921387 / BPR loss: 1.3862987756729126 / Matching loss: 0.00010142762039322406 / Item reconstruction: 0.001012683380395174 / Text reconstruction: 0.0302436426281929\n",
            "loss in epoch 2/11 iteration 28/111: 1.392938256263733 / BPR loss: 1.3863162994384766 / Matching loss: 9.894843969959766e-05 / Item reconstruction: 0.0010261089773848653 / Text reconstruction: 0.030049696564674377\n",
            "loss in epoch 2/11 iteration 29/111: 1.3928635120391846 / BPR loss: 1.386292815208435 / Matching loss: 9.495245467405766e-05 / Item reconstruction: 0.0010397712467238307 / Text reconstruction: 0.0297793447971344\n",
            "loss in epoch 2/11 iteration 30/111: 1.3927266597747803 / BPR loss: 1.386193037033081 / Matching loss: 9.438709821552038e-05 / Item reconstruction: 0.0010514151072129607 / Text reconstruction: 0.029567237943410873\n",
            "loss in epoch 2/11 iteration 31/111: 1.3927478790283203 / BPR loss: 1.386275053024292 / Matching loss: 9.094671258935705e-05 / Item reconstruction: 0.0010218871757388115 / Text reconstruction: 0.02935468591749668\n",
            "loss in epoch 2/11 iteration 32/111: 1.3927135467529297 / BPR loss: 1.3862991333007812 / Matching loss: 9.422234143130481e-05 / Item reconstruction: 0.0009848198387771845 / Text reconstruction: 0.02913890779018402\n",
            "loss in epoch 2/11 iteration 33/111: 1.3927100896835327 / BPR loss: 1.3863155841827393 / Matching loss: 9.58175805862993e-05 / Item reconstruction: 0.00103880581445992 / Text reconstruction: 0.028896555304527283\n",
            "loss in epoch 2/11 iteration 34/111: 1.3925656080245972 / BPR loss: 1.386218547821045 / Matching loss: 9.692241292214021e-05 / Item reconstruction: 0.0010285477619618177 / Text reconstruction: 0.028679434210062027\n",
            "loss in epoch 2/11 iteration 35/111: 1.3926153182983398 / BPR loss: 1.3863372802734375 / Matching loss: 9.563843195792288e-05 / Item reconstruction: 0.000973770278505981 / Text reconstruction: 0.028478069230914116\n",
            "loss in epoch 2/11 iteration 36/111: 1.3925833702087402 / BPR loss: 1.386305809020996 / Matching loss: 9.724286792334169e-05 / Item reconstruction: 0.0009986007353290915 / Text reconstruction: 0.02840501442551613\n",
            "loss in epoch 2/11 iteration 37/111: 1.3924405574798584 / BPR loss: 1.3862390518188477 / Matching loss: 9.426059841644019e-05 / Item reconstruction: 0.000962924852501601 / Text reconstruction: 0.02812838926911354\n",
            "loss in epoch 2/11 iteration 38/111: 1.3924864530563354 / BPR loss: 1.3862898349761963 / Matching loss: 9.537959704175591e-05 / Item reconstruction: 0.0010342502500861883 / Text reconstruction: 0.027920369058847427\n",
            "loss in epoch 2/11 iteration 39/111: 1.392297387123108 / BPR loss: 1.3861873149871826 / Matching loss: 9.051896631717682e-05 / Item reconstruction: 0.0009558856836520135 / Text reconstruction: 0.02770811691880226\n",
            "loss in epoch 2/11 iteration 40/111: 1.3923437595367432 / BPR loss: 1.38627028465271 / Matching loss: 8.960736886365339e-05 / Item reconstruction: 0.000938906567171216 / Text reconstruction: 0.02757180854678154\n",
            "loss in epoch 2/11 iteration 41/111: 1.3923723697662354 / BPR loss: 1.3863303661346436 / Matching loss: 9.073915862245485e-05 / Item reconstruction: 0.0009377567330375314 / Text reconstruction: 0.027412068098783493\n",
            "loss in epoch 2/11 iteration 42/111: 1.3923156261444092 / BPR loss: 1.386305809020996 / Matching loss: 9.104047785513103e-05 / Item reconstruction: 0.0009443791350349784 / Text reconstruction: 0.027232563123106956\n",
            "loss in epoch 2/11 iteration 43/111: 1.3922271728515625 / BPR loss: 1.3862507343292236 / Matching loss: 8.981088467407972e-05 / Item reconstruction: 0.0009627994149923325 / Text reconstruction: 0.02702656015753746\n",
            "loss in epoch 2/11 iteration 44/111: 1.3921958208084106 / BPR loss: 1.386252522468567 / Matching loss: 8.818830247037113e-05 / Item reconstruction: 0.0009594131261110306 / Text reconstruction: 0.02687707543373108\n",
            "loss in epoch 2/11 iteration 45/111: 1.3921409845352173 / BPR loss: 1.3862426280975342 / Matching loss: 8.83720931597054e-05 / Item reconstruction: 0.0009273345349356532 / Text reconstruction: 0.026731640100479126\n",
            "loss in epoch 2/11 iteration 46/111: 1.392215371131897 / BPR loss: 1.3863277435302734 / Matching loss: 9.18747900868766e-05 / Item reconstruction: 0.0009814389050006866 / Text reconstruction: 0.026525290682911873\n",
            "loss in epoch 2/11 iteration 47/111: 1.391993761062622 / BPR loss: 1.386164903640747 / Matching loss: 9.221600339515135e-05 / Item reconstruction: 0.0009364186553284526 / Text reconstruction: 0.026341507211327553\n",
            "loss in epoch 2/11 iteration 48/111: 1.3920406103134155 / BPR loss: 1.3862168788909912 / Matching loss: 9.187283285427839e-05 / Item reconstruction: 0.0009973817504942417 / Text reconstruction: 0.02616584114730358\n",
            "loss in epoch 2/11 iteration 49/111: 1.3920313119888306 / BPR loss: 1.386261224746704 / Matching loss: 8.892417827155441e-05 / Item reconstruction: 0.0009493925608694553 / Text reconstruction: 0.02603204734623432\n",
            "loss in epoch 2/11 iteration 50/111: 1.39192795753479 / BPR loss: 1.3861961364746094 / Matching loss: 8.501486445311457e-05 / Item reconstruction: 0.0009320465032942593 / Text reconstruction: 0.02590416558086872\n",
            "loss in epoch 2/11 iteration 51/111: 1.3920032978057861 / BPR loss: 1.386272668838501 / Matching loss: 8.357684419024736e-05 / Item reconstruction: 0.0009903078898787498 / Text reconstruction: 0.025759603828191757\n",
            "loss in epoch 2/11 iteration 52/111: 1.3919695615768433 / BPR loss: 1.3862841129302979 / Matching loss: 8.445663843303919e-05 / Item reconstruction: 0.0009506014757789671 / Text reconstruction: 0.025628812611103058\n",
            "loss in epoch 2/11 iteration 53/111: 1.3918492794036865 / BPR loss: 1.386210560798645 / Matching loss: 8.783634257270023e-05 / Item reconstruction: 0.0009140806505456567 / Text reconstruction: 0.02546888031065464\n",
            "loss in epoch 2/11 iteration 54/111: 1.3918797969818115 / BPR loss: 1.3862322568893433 / Matching loss: 8.982668805401772e-05 / Item reconstruction: 0.0009996606968343258 / Text reconstruction: 0.02528935670852661\n",
            "loss in epoch 2/11 iteration 55/111: 1.391815185546875 / BPR loss: 1.386235237121582 / Matching loss: 8.863062976161018e-05 / Item reconstruction: 0.0009317214135080576 / Text reconstruction: 0.025127651169896126\n",
            "loss in epoch 2/11 iteration 56/111: 1.391848087310791 / BPR loss: 1.386263132095337 / Matching loss: 8.970574708655477e-05 / Item reconstruction: 0.000985004473477602 / Text reconstruction: 0.02501372992992401\n",
            "loss in epoch 2/11 iteration 57/111: 1.3918745517730713 / BPR loss: 1.3863219022750854 / Matching loss: 8.544333832105622e-05 / Item reconstruction: 0.0009717279463075101 / Text reconstruction: 0.024906596168875694\n",
            "loss in epoch 2/11 iteration 58/111: 1.3917391300201416 / BPR loss: 1.3862206935882568 / Matching loss: 8.487359446007758e-05 / Item reconstruction: 0.0009395144297741354 / Text reconstruction: 0.02481902949512005\n",
            "loss in epoch 2/11 iteration 59/111: 1.391760230064392 / BPR loss: 1.386278510093689 / Matching loss: 8.422844985034317e-05 / Item reconstruction: 0.0009328315500169992 / Text reconstruction: 0.024654649198055267\n",
            "loss in epoch 2/11 iteration 60/111: 1.391662836074829 / BPR loss: 1.3862318992614746 / Matching loss: 8.436360076302662e-05 / Item reconstruction: 0.0009035376715473831 / Text reconstruction: 0.024473432451486588\n",
            "loss in epoch 2/11 iteration 61/111: 1.3917230367660522 / BPR loss: 1.386277437210083 / Matching loss: 8.52770172059536e-05 / Item reconstruction: 0.0009652295848354697 / Text reconstruction: 0.024388795718550682\n",
            "loss in epoch 2/11 iteration 62/111: 1.3916518688201904 / BPR loss: 1.3862504959106445 / Matching loss: 8.210929809138179e-05 / Item reconstruction: 0.0009389373008161783 / Text reconstruction: 0.02424883469939232\n",
            "loss in epoch 2/11 iteration 63/111: 1.391618013381958 / BPR loss: 1.386251449584961 / Matching loss: 8.387403795495629e-05 / Item reconstruction: 0.0009327360894531012 / Text reconstruction: 0.02408142015337944\n",
            "loss in epoch 2/11 iteration 64/111: 1.3915908336639404 / BPR loss: 1.3862476348876953 / Matching loss: 8.363636879948899e-05 / Item reconstruction: 0.0009015675750561059 / Text reconstruction: 0.024043714627623558\n",
            "loss in epoch 2/11 iteration 65/111: 1.3915797472000122 / BPR loss: 1.386234164237976 / Matching loss: 8.893059566617012e-05 / Item reconstruction: 0.0009810758056119084 / Text reconstruction: 0.023830637335777283\n",
            "loss in epoch 2/11 iteration 66/111: 1.3915495872497559 / BPR loss: 1.3862595558166504 / Matching loss: 8.536879613529891e-05 / Item reconstruction: 0.0009218148188665509 / Text reconstruction: 0.023718837648630142\n",
            "loss in epoch 2/11 iteration 67/111: 1.3915420770645142 / BPR loss: 1.3862533569335938 / Matching loss: 8.046574657782912e-05 / Item reconstruction: 0.000982713303528726 / Text reconstruction: 0.023584162816405296\n",
            "loss in epoch 2/11 iteration 68/111: 1.3915374279022217 / BPR loss: 1.3863048553466797 / Matching loss: 7.716662366874516e-05 / Item reconstruction: 0.0009252083254978061 / Text reconstruction: 0.023463966324925423\n",
            "loss in epoch 2/11 iteration 69/111: 1.391493797302246 / BPR loss: 1.386277198791504 / Matching loss: 8.003978291526437e-05 / Item reconstruction: 0.0009256562334485352 / Text reconstruction: 0.023369118571281433\n",
            "loss in epoch 2/11 iteration 70/111: 1.3914891481399536 / BPR loss: 1.3862673044204712 / Matching loss: 8.58543353388086e-05 / Item reconstruction: 0.0009539790917187929 / Text reconstruction: 0.023295430466532707\n",
            "loss in epoch 2/11 iteration 71/111: 1.3914700746536255 / BPR loss: 1.3863046169281006 / Matching loss: 8.276122389361262e-05 / Item reconstruction: 0.0008978893747553229 / Text reconstruction: 0.02316890098154545\n",
            "loss in epoch 2/11 iteration 72/111: 1.3913980722427368 / BPR loss: 1.3862495422363281 / Matching loss: 8.303775393869728e-05 / Item reconstruction: 0.0009377406677231193 / Text reconstruction: 0.022982902824878693\n",
            "loss in epoch 2/11 iteration 73/111: 1.3913931846618652 / BPR loss: 1.386272668838501 / Matching loss: 8.240906026912853e-05 / Item reconstruction: 0.0009212617878802121 / Text reconstruction: 0.022887788712978363\n",
            "loss in epoch 2/11 iteration 74/111: 1.391339898109436 / BPR loss: 1.3862277269363403 / Matching loss: 7.894761802162975e-05 / Item reconstruction: 0.0009331362089142203 / Text reconstruction: 0.022833330556750298\n",
            "loss in epoch 2/11 iteration 75/111: 1.391319751739502 / BPR loss: 1.3862253427505493 / Matching loss: 7.853060378693044e-05 / Item reconstruction: 0.0009606026578694582 / Text reconstruction: 0.022677980363368988\n",
            "loss in epoch 2/11 iteration 76/111: 1.3912993669509888 / BPR loss: 1.3862285614013672 / Matching loss: 8.023978443816304e-05 / Item reconstruction: 0.0009507822105661035 / Text reconstruction: 0.022575831040740013\n",
            "loss in epoch 2/11 iteration 77/111: 1.3913815021514893 / BPR loss: 1.3863351345062256 / Matching loss: 8.149269706336781e-05 / Item reconstruction: 0.0009380368865095079 / Text reconstruction: 0.022479277104139328\n",
            "loss in epoch 2/11 iteration 78/111: 1.391276478767395 / BPR loss: 1.386228084564209 / Matching loss: 8.173012611223385e-05 / Item reconstruction: 0.0010008314857259393 / Text reconstruction: 0.02233116887509823\n",
            "loss in epoch 2/11 iteration 79/111: 1.391294240951538 / BPR loss: 1.3862864971160889 / Matching loss: 8.117173274513334e-05 / Item reconstruction: 0.0009471256053075194 / Text reconstruction: 0.02226477488875389\n",
            "loss in epoch 2/11 iteration 80/111: 1.3912030458450317 / BPR loss: 1.3862215280532837 / Matching loss: 7.932260632514954e-05 / Item reconstruction: 0.0009470770019106567 / Text reconstruction: 0.022143620997667313\n",
            "loss in epoch 2/11 iteration 81/111: 1.3911951780319214 / BPR loss: 1.3862488269805908 / Matching loss: 7.496481703128666e-05 / Item reconstruction: 0.0009191769640892744 / Text reconstruction: 0.022058870643377304\n",
            "loss in epoch 2/11 iteration 82/111: 1.3912103176116943 / BPR loss: 1.3862674236297607 / Matching loss: 7.593743066536263e-05 / Item reconstruction: 0.0009428831981495023 / Text reconstruction: 0.021977579221129417\n",
            "loss in epoch 2/11 iteration 83/111: 1.3911833763122559 / BPR loss: 1.3862578868865967 / Matching loss: 8.146282198140398e-05 / Item reconstruction: 0.0009605536470189691 / Text reconstruction: 0.021818699315190315\n",
            "loss in epoch 2/11 iteration 84/111: 1.391166090965271 / BPR loss: 1.3862528800964355 / Matching loss: 8.054571662796661e-05 / Item reconstruction: 0.0009475229890085757 / Text reconstruction: 0.021794576197862625\n",
            "loss in epoch 2/11 iteration 85/111: 1.391121745109558 / BPR loss: 1.3862473964691162 / Matching loss: 7.941444346215576e-05 / Item reconstruction: 0.0009556799195706844 / Text reconstruction: 0.02158574014902115\n",
            "loss in epoch 2/11 iteration 86/111: 1.3911159038543701 / BPR loss: 1.3862684965133667 / Matching loss: 7.762538734823465e-05 / Item reconstruction: 0.000943043502047658 / Text reconstruction: 0.021491654217243195\n",
            "loss in epoch 2/11 iteration 87/111: 1.3910505771636963 / BPR loss: 1.3862155675888062 / Matching loss: 7.446201925631613e-05 / Item reconstruction: 0.0009496238781139255 / Text reconstruction: 0.02142849937081337\n",
            "loss in epoch 2/11 iteration 88/111: 1.3910664319992065 / BPR loss: 1.386236548423767 / Matching loss: 7.495428872061893e-05 / Item reconstruction: 0.0009395756642334163 / Text reconstruction: 0.02142559550702572\n",
            "loss in epoch 2/11 iteration 89/111: 1.3910373449325562 / BPR loss: 1.3862569332122803 / Matching loss: 7.765382179059088e-05 / Item reconstruction: 0.0009119573514908552 / Text reconstruction: 0.021234363317489624\n",
            "loss in epoch 2/11 iteration 90/111: 1.3910799026489258 / BPR loss: 1.3863060474395752 / Matching loss: 8.109354530461133e-05 / Item reconstruction: 0.0009283148101530969 / Text reconstruction: 0.021142909303307533\n",
            "loss in epoch 2/11 iteration 91/111: 1.3910261392593384 / BPR loss: 1.3862736225128174 / Matching loss: 7.98641995061189e-05 / Item reconstruction: 0.0009245355613529682 / Text reconstruction: 0.021051906049251556\n",
            "loss in epoch 2/11 iteration 92/111: 1.3909603357315063 / BPR loss: 1.3862332105636597 / Matching loss: 7.528695277869701e-05 / Item reconstruction: 0.0009114975691772997 / Text reconstruction: 0.020980019122362137\n",
            "loss in epoch 2/11 iteration 93/111: 1.3910255432128906 / BPR loss: 1.3863027095794678 / Matching loss: 7.314298272831365e-05 / Item reconstruction: 0.0009554623393341899 / Text reconstruction: 0.02085954137146473\n",
            "loss in epoch 2/11 iteration 94/111: 1.3910218477249146 / BPR loss: 1.3863186836242676 / Matching loss: 7.398501475108787e-05 / Item reconstruction: 0.0009322582045570016 / Text reconstruction: 0.020815208554267883\n",
            "loss in epoch 2/11 iteration 95/111: 1.3909398317337036 / BPR loss: 1.3862568140029907 / Matching loss: 7.664702570764348e-05 / Item reconstruction: 0.0009298165678046644 / Text reconstruction: 0.020707501098513603\n",
            "loss in epoch 2/11 iteration 96/111: 1.390897274017334 / BPR loss: 1.3862537145614624 / Matching loss: 7.456861203536391e-05 / Item reconstruction: 0.0008925857255235314 / Text reconstruction: 0.020612947642803192\n",
            "loss in epoch 2/11 iteration 97/111: 1.39093017578125 / BPR loss: 1.386285662651062 / Matching loss: 7.871056732255965e-05 / Item reconstruction: 0.0009033116512000561 / Text reconstruction: 0.020570676773786545\n",
            "loss in epoch 2/11 iteration 98/111: 1.3909497261047363 / BPR loss: 1.3863502740859985 / Matching loss: 7.474263838957995e-05 / Item reconstruction: 0.0008852444589138031 / Text reconstruction: 0.02041051909327507\n",
            "loss in epoch 2/11 iteration 99/111: 1.390836477279663 / BPR loss: 1.3862295150756836 / Matching loss: 7.292632653843611e-05 / Item reconstruction: 0.000932730152271688 / Text reconstruction: 0.0203383918851614\n",
            "loss in epoch 2/11 iteration 100/111: 1.3908836841583252 / BPR loss: 1.3862920999526978 / Matching loss: 7.229064067360014e-05 / Item reconstruction: 0.0009495042031630874 / Text reconstruction: 0.020222561433911324\n",
            "loss in epoch 2/11 iteration 101/111: 1.3907837867736816 / BPR loss: 1.3862342834472656 / Matching loss: 7.448310498148203e-05 / Item reconstruction: 0.0008891314500942826 / Text reconstruction: 0.020152563229203224\n",
            "loss in epoch 2/11 iteration 102/111: 1.3907994031906128 / BPR loss: 1.386251449584961 / Matching loss: 7.72390776546672e-05 / Item reconstruction: 0.0009393712971359491 / Text reconstruction: 0.020005276426672935\n",
            "loss in epoch 2/11 iteration 103/111: 1.390830159187317 / BPR loss: 1.3862862586975098 / Matching loss: 7.46597652323544e-05 / Item reconstruction: 0.0009360348340123892 / Text reconstruction: 0.02000652253627777\n",
            "loss in epoch 2/11 iteration 104/111: 1.3907901048660278 / BPR loss: 1.3862788677215576 / Matching loss: 7.182346598710865e-05 / Item reconstruction: 0.000892899464815855 / Text reconstruction: 0.019965190440416336\n",
            "loss in epoch 2/11 iteration 105/111: 1.3907581567764282 / BPR loss: 1.3862545490264893 / Matching loss: 7.207771704997867e-05 / Item reconstruction: 0.0009278529323637486 / Text reconstruction: 0.019837697967886925\n",
            "loss in epoch 2/11 iteration 106/111: 1.3907359838485718 / BPR loss: 1.3862379789352417 / Matching loss: 7.41042458685115e-05 / Item reconstruction: 0.0009547651279717684 / Text reconstruction: 0.01973237842321396\n",
            "loss in epoch 2/11 iteration 107/111: 1.390720009803772 / BPR loss: 1.3862769603729248 / Matching loss: 7.361923780990764e-05 / Item reconstruction: 0.0008897606167010963 / Text reconstruction: 0.01962241902947426\n",
            "loss in epoch 2/11 iteration 108/111: 1.3907548189163208 / BPR loss: 1.3863134384155273 / Matching loss: 7.399900641757995e-05 / Item reconstruction: 0.0009077247814275324 / Text reconstruction: 0.019567688927054405\n",
            "loss in epoch 2/11 iteration 109/111: 1.3907074928283691 / BPR loss: 1.3862664699554443 / Matching loss: 7.201566768344492e-05 / Item reconstruction: 0.0009327547159045935 / Text reconstruction: 0.01951361633837223\n",
            "loss in epoch 2/11 iteration 110/111: 1.3906822204589844 / BPR loss: 1.3862719535827637 / Matching loss: 6.792149360990152e-05 / Item reconstruction: 0.00083047931548208 / Text reconstruction: 0.01963547244668007\n",
            "loss in epoch 2/11 iteration 111/111: 1.3905973434448242 / BPR loss: 1.3862297534942627 / Matching loss: 7.022422505542636e-05 / Item reconstruction: 0.000723693985491991 / Text reconstruction: 0.019677836447954178\n",
            " 20% 2/10 [10:48<43:11, 323.96s/it]loss in epoch 3/11 iteration 0/111: 1.3906171321868896 / BPR loss: 1.386240005493164 / Matching loss: 7.98685141489841e-05 / Item reconstruction: 0.0009326916770078242 / Text reconstruction: 0.019154325127601624\n",
            "loss in epoch 3/11 iteration 1/111: 1.3906235694885254 / BPR loss: 1.3862581253051758 / Matching loss: 7.767821080051363e-05 / Item reconstruction: 0.0009269738220609725 / Text reconstruction: 0.019121354445815086\n",
            "loss in epoch 3/11 iteration 2/111: 1.3905802965164185 / BPR loss: 1.3862208127975464 / Matching loss: 7.174660277087241e-05 / Item reconstruction: 0.0009371056803502142 / Text reconstruction: 0.01909533515572548\n",
            "loss in epoch 3/11 iteration 3/111: 1.390624761581421 / BPR loss: 1.3862619400024414 / Matching loss: 6.668554851785302e-05 / Item reconstruction: 0.000971574685536325 / Text reconstruction: 0.019052287563681602\n",
            "loss in epoch 3/11 iteration 4/111: 1.3905999660491943 / BPR loss: 1.3862498998641968 / Matching loss: 6.967382796574384e-05 / Item reconstruction: 0.000982449040748179 / Text reconstruction: 0.018945802003145218\n",
            "loss in epoch 3/11 iteration 5/111: 1.3906080722808838 / BPR loss: 1.3862894773483276 / Matching loss: 7.432882557623088e-05 / Item reconstruction: 0.0009341172408312559 / Text reconstruction: 0.018885953351855278\n",
            "loss in epoch 3/11 iteration 6/111: 1.3905904293060303 / BPR loss: 1.3863084316253662 / Matching loss: 7.889010885264724e-05 / Item reconstruction: 0.0009338465752080083 / Text reconstruction: 0.018680453300476074\n",
            "loss in epoch 3/11 iteration 7/111: 1.3905693292617798 / BPR loss: 1.3862922191619873 / Matching loss: 7.887401443440467e-05 / Item reconstruction: 0.0009497248684056103 / Text reconstruction: 0.018617097288370132\n",
            "loss in epoch 3/11 iteration 8/111: 1.3905277252197266 / BPR loss: 1.3862528800964355 / Matching loss: 7.242341234814376e-05 / Item reconstruction: 0.0009161533671431243 / Text reconstruction: 0.01872144266963005\n",
            "loss in epoch 3/11 iteration 9/111: 1.3905071020126343 / BPR loss: 1.3862268924713135 / Matching loss: 6.4282794483006e-05 / Item reconstruction: 0.000981874531134963 / Text reconstruction: 0.018625015392899513\n",
            "loss in epoch 3/11 iteration 10/111: 1.3904355764389038 / BPR loss: 1.3859790563583374 / Matching loss: 6.954846321605146e-05 / Item reconstruction: 0.0013439881149679422 / Text reconstruction: 0.018575359135866165\n",
            "loss in epoch 3/11 iteration 11/111: 1.3903344869613647 / BPR loss: 1.3859319686889648 / Matching loss: 7.466175884474069e-05 / Item reconstruction: 0.0012632196303457022 / Text reconstruction: 0.018481722101569176\n",
            "loss in epoch 3/11 iteration 12/111: 1.3903272151947021 / BPR loss: 1.3859434127807617 / Matching loss: 7.785305206198245e-05 / Item reconstruction: 0.0012360191904008389 / Text reconstruction: 0.018439881503582\n",
            "loss in epoch 3/11 iteration 13/111: 1.3901995420455933 / BPR loss: 1.3858668804168701 / Matching loss: 8.07029427960515e-05 / Item reconstruction: 0.001166131580248475 / Text reconstruction: 0.01834442839026451\n",
            "loss in epoch 3/11 iteration 14/111: 1.3902063369750977 / BPR loss: 1.3859410285949707 / Matching loss: 7.820587779860944e-05 / Item reconstruction: 0.0010835258290171623 / Text reconstruction: 0.018226267769932747\n",
            "loss in epoch 3/11 iteration 15/111: 1.3901580572128296 / BPR loss: 1.385921835899353 / Matching loss: 7.118536450434476e-05 / Item reconstruction: 0.0010659302351996303 / Text reconstruction: 0.018160507082939148\n",
            "loss in epoch 3/11 iteration 16/111: 1.3903582096099854 / BPR loss: 1.3861000537872314 / Matching loss: 6.968582601984963e-05 / Item reconstruction: 0.0011418892536312342 / Text reconstruction: 0.018087688833475113\n",
            "loss in epoch 3/11 iteration 17/111: 1.3905638456344604 / BPR loss: 1.3862671852111816 / Matching loss: 7.17062212061137e-05 / Item reconstruction: 0.0012572882696986198 / Text reconstruction: 0.01798136532306671\n",
            "loss in epoch 3/11 iteration 18/111: 1.3905417919158936 / BPR loss: 1.386228084564209 / Matching loss: 7.225708395708352e-05 / Item reconstruction: 0.001308877719566226 / Text reconstruction: 0.017935125157237053\n",
            "loss in epoch 3/11 iteration 19/111: 1.3905510902404785 / BPR loss: 1.3862661123275757 / Matching loss: 7.45526995160617e-05 / Item reconstruction: 0.0012481359299272299 / Text reconstruction: 0.017931897193193436\n",
            "loss in epoch 3/11 iteration 20/111: 1.3904751539230347 / BPR loss: 1.3862266540527344 / Matching loss: 7.221889245556667e-05 / Item reconstruction: 0.0012375564547255635 / Text reconstruction: 0.017787311226129532\n",
            "loss in epoch 3/11 iteration 21/111: 1.390461802482605 / BPR loss: 1.3862831592559814 / Matching loss: 6.704499537590891e-05 / Item reconstruction: 0.0011265651555731893 / Text reconstruction: 0.017742175608873367\n",
            "loss in epoch 3/11 iteration 22/111: 1.390437126159668 / BPR loss: 1.386303424835205 / Matching loss: 7.016523159109056e-05 / Item reconstruction: 0.0010875132866203785 / Text reconstruction: 0.01759892888367176\n",
            "loss in epoch 3/11 iteration 23/111: 1.3903642892837524 / BPR loss: 1.3862700462341309 / Matching loss: 7.213430944830179e-05 / Item reconstruction: 0.001027336111292243 / Text reconstruction: 0.017541993409395218\n",
            "loss in epoch 3/11 iteration 24/111: 1.3903343677520752 / BPR loss: 1.3862688541412354 / Matching loss: 7.31065301806666e-05 / Item reconstruction: 0.0010018665343523026 / Text reconstruction: 0.017457662150263786\n",
            "loss in epoch 3/11 iteration 25/111: 1.3903454542160034 / BPR loss: 1.3862833976745605 / Matching loss: 7.258365803863853e-05 / Item reconstruction: 0.0010232710046693683 / Text reconstruction: 0.0173892043530941\n",
            "loss in epoch 3/11 iteration 26/111: 1.3902592658996582 / BPR loss: 1.3862223625183105 / Matching loss: 7.078111229930073e-05 / Item reconstruction: 0.0010074871825054288 / Text reconstruction: 0.017311373725533485\n",
            "loss in epoch 3/11 iteration 27/111: 1.3902840614318848 / BPR loss: 1.3862793445587158 / Matching loss: 6.392192881321535e-05 / Item reconstruction: 0.000990136293694377 / Text reconstruction: 0.017228610813617706\n",
            "loss in epoch 3/11 iteration 28/111: 1.390283226966858 / BPR loss: 1.3862848281860352 / Matching loss: 6.6226719354745e-05 / Item reconstruction: 0.000994489062577486 / Text reconstruction: 0.017174527049064636\n",
            "loss in epoch 3/11 iteration 29/111: 1.3902933597564697 / BPR loss: 1.3862814903259277 / Matching loss: 7.229117909446359e-05 / Item reconstruction: 0.0010659731924533844 / Text reconstruction: 0.017033107578754425\n",
            "loss in epoch 3/11 iteration 30/111: 1.3901900053024292 / BPR loss: 1.3862025737762451 / Matching loss: 7.457242463715374e-05 / Item reconstruction: 0.0010354290716350079 / Text reconstruction: 0.016975242644548416\n",
            "loss in epoch 3/11 iteration 31/111: 1.390242099761963 / BPR loss: 1.3862659931182861 / Matching loss: 6.92853209329769e-05 / Item reconstruction: 0.0010435916483402252 / Text reconstruction: 0.016925092786550522\n",
            "loss in epoch 3/11 iteration 32/111: 1.3902031183242798 / BPR loss: 1.3862888813018799 / Matching loss: 6.555308937095106e-05 / Item reconstruction: 0.0009593843133188784 / Text reconstruction: 0.01684483513236046\n",
            "loss in epoch 3/11 iteration 33/111: 1.39018714427948 / BPR loss: 1.386257529258728 / Matching loss: 6.846817268524319e-05 / Item reconstruction: 0.001020836061798036 / Text reconstruction: 0.01675354316830635\n",
            "loss in epoch 3/11 iteration 34/111: 1.3901008367538452 / BPR loss: 1.3861862421035767 / Matching loss: 7.05839047441259e-05 / Item reconstruction: 0.0010247279424220324 / Text reconstruction: 0.01665804162621498\n",
            "loss in epoch 3/11 iteration 35/111: 1.3901660442352295 / BPR loss: 1.3862850666046143 / Matching loss: 6.840730202384293e-05 / Item reconstruction: 0.0009894241811707616 / Text reconstruction: 0.01658891886472702\n",
            "loss in epoch 3/11 iteration 36/111: 1.390198826789856 / BPR loss: 1.3863232135772705 / Matching loss: 6.598993786610663e-05 / Item reconstruction: 0.0009769536554813385 / Text reconstruction: 0.016605425626039505\n",
            "loss in epoch 3/11 iteration 37/111: 1.3900120258331299 / BPR loss: 1.3861727714538574 / Matching loss: 6.480159936472774e-05 / Item reconstruction: 0.0009572664275765419 / Text reconstruction: 0.01647878997027874\n",
            "loss in epoch 3/11 iteration 38/111: 1.3900936841964722 / BPR loss: 1.3862591981887817 / Matching loss: 6.849896453786641e-05 / Item reconstruction: 0.0009734096238389611 / Text reconstruction: 0.016395781189203262\n",
            "loss in epoch 3/11 iteration 39/111: 1.390040397644043 / BPR loss: 1.3862359523773193 / Matching loss: 6.907962961122394e-05 / Item reconstruction: 0.0009471632656641304 / Text reconstruction: 0.016309253871440887\n",
            "loss in epoch 3/11 iteration 40/111: 1.3900494575500488 / BPR loss: 1.3862487077713013 / Matching loss: 6.750931061105803e-05 / Item reconstruction: 0.0009746159776113927 / Text reconstruction: 0.016230043023824692\n",
            "loss in epoch 3/11 iteration 41/111: 1.3900247812271118 / BPR loss: 1.38625168800354 / Matching loss: 6.591499550268054e-05 / Item reconstruction: 0.0009305622661486268 / Text reconstruction: 0.016209300607442856\n",
            "loss in epoch 3/11 iteration 42/111: 1.3900452852249146 / BPR loss: 1.3862709999084473 / Matching loss: 6.356208177749068e-05 / Item reconstruction: 0.000971197965554893 / Text reconstruction: 0.01612589880824089\n",
            "loss in epoch 3/11 iteration 43/111: 1.3899999856948853 / BPR loss: 1.3862476348876953 / Matching loss: 6.782954005757347e-05 / Item reconstruction: 0.0009586145170032978 / Text reconstruction: 0.01602567732334137\n",
            "loss in epoch 3/11 iteration 44/111: 1.3900227546691895 / BPR loss: 1.386270523071289 / Matching loss: 6.679132638964802e-05 / Item reconstruction: 0.0009895814582705498 / Text reconstruction: 0.015953119844198227\n",
            "loss in epoch 3/11 iteration 45/111: 1.3899574279785156 / BPR loss: 1.3862367868423462 / Matching loss: 6.453940295614302e-05 / Item reconstruction: 0.0009525215718895197 / Text reconstruction: 0.015899579972028732\n",
            "loss in epoch 3/11 iteration 46/111: 1.3899606466293335 / BPR loss: 1.3862793445587158 / Matching loss: 6.3242347096093e-05 / Item reconstruction: 0.0009145766962319613 / Text reconstruction: 0.01580384373664856\n",
            "loss in epoch 3/11 iteration 47/111: 1.3898736238479614 / BPR loss: 1.3862080574035645 / Matching loss: 6.415598909370601e-05 / Item reconstruction: 0.0009103405172936618 / Text reconstruction: 0.015731550753116608\n",
            "loss in epoch 3/11 iteration 48/111: 1.3898820877075195 / BPR loss: 1.3861809968948364 / Matching loss: 6.622566434089094e-05 / Item reconstruction: 0.0010090058203786612 / Text reconstruction: 0.01565176248550415\n",
            "loss in epoch 3/11 iteration 49/111: 1.389883279800415 / BPR loss: 1.3862173557281494 / Matching loss: 6.956935249036178e-05 / Item reconstruction: 0.0009630187414586544 / Text reconstruction: 0.01557430811226368\n",
            "loss in epoch 3/11 iteration 50/111: 1.3898875713348389 / BPR loss: 1.386244773864746 / Matching loss: 6.723783735651523e-05 / Item reconstruction: 0.0009495431440882385 / Text reconstruction: 0.015503701753914356\n",
            "loss in epoch 3/11 iteration 51/111: 1.3898720741271973 / BPR loss: 1.3862361907958984 / Matching loss: 6.186871905811131e-05 / Item reconstruction: 0.0009625228121876717 / Text reconstruction: 0.015463979914784431\n",
            "loss in epoch 3/11 iteration 52/111: 1.3898539543151855 / BPR loss: 1.38625168800354 / Matching loss: 6.20735518168658e-05 / Item reconstruction: 0.0009333486086688936 / Text reconstruction: 0.015367338433861732\n",
            "loss in epoch 3/11 iteration 53/111: 1.3898228406906128 / BPR loss: 1.3862472772598267 / Matching loss: 6.481858144979924e-05 / Item reconstruction: 0.0009093120461329818 / Text reconstruction: 0.015280108898878098\n",
            "loss in epoch 3/11 iteration 54/111: 1.3898168802261353 / BPR loss: 1.3862216472625732 / Matching loss: 6.784474680898711e-05 / Item reconstruction: 0.0009732068283483386 / Text reconstruction: 0.01520378515124321\n",
            "loss in epoch 3/11 iteration 55/111: 1.3897250890731812 / BPR loss: 1.386145830154419 / Matching loss: 6.556927837664261e-05 / Item reconstruction: 0.0009640628704801202 / Text reconstruction: 0.015158174559473991\n",
            "loss in epoch 3/11 iteration 56/111: 1.389877438545227 / BPR loss: 1.386299967765808 / Matching loss: 6.557157030329108e-05 / Item reconstruction: 0.000986569793894887 / Text reconstruction: 0.015092829242348671\n",
            "loss in epoch 3/11 iteration 57/111: 1.389739990234375 / BPR loss: 1.3861842155456543 / Matching loss: 6.324490095721558e-05 / Item reconstruction: 0.0009677605121396482 / Text reconstruction: 0.015043284744024277\n",
            "loss in epoch 3/11 iteration 58/111: 1.3897584676742554 / BPR loss: 1.3862203359603882 / Matching loss: 6.537209264934063e-05 / Item reconstruction: 0.0009460649453103542 / Text reconstruction: 0.01499904878437519\n",
            "loss in epoch 3/11 iteration 59/111: 1.3897526264190674 / BPR loss: 1.3862322568893433 / Matching loss: 6.509273225674406e-05 / Item reconstruction: 0.000946053653024137 / Text reconstruction: 0.014911462552845478\n",
            "loss in epoch 3/11 iteration 60/111: 1.3897095918655396 / BPR loss: 1.3862180709838867 / Matching loss: 6.403525185305625e-05 / Item reconstruction: 0.0009276383789256215 / Text reconstruction: 0.014818360097706318\n",
            "loss in epoch 3/11 iteration 61/111: 1.3897348642349243 / BPR loss: 1.3862279653549194 / Matching loss: 6.571888661710545e-05 / Item reconstruction: 0.000984939280897379 / Text reconstruction: 0.01474407222121954\n",
            "loss in epoch 3/11 iteration 62/111: 1.3897150754928589 / BPR loss: 1.3862504959106445 / Matching loss: 6.349400064209476e-05 / Item reconstruction: 0.0009387307800352573 / Text reconstruction: 0.014658665284514427\n",
            "loss in epoch 3/11 iteration 63/111: 1.3897045850753784 / BPR loss: 1.3862595558166504 / Matching loss: 6.228078564163297e-05 / Item reconstruction: 0.0009376202942803502 / Text reconstruction: 0.014569752849638462\n",
            "loss in epoch 3/11 iteration 64/111: 1.3896355628967285 / BPR loss: 1.3862056732177734 / Matching loss: 6.203271186677739e-05 / Item reconstruction: 0.0009090700186789036 / Text reconstruction: 0.014566744677722454\n",
            "loss in epoch 3/11 iteration 65/111: 1.3897162675857544 / BPR loss: 1.386277437210083 / Matching loss: 6.371545168804005e-05 / Item reconstruction: 0.0009632543660700321 / Text reconstruction: 0.014467975124716759\n",
            "loss in epoch 3/11 iteration 66/111: 1.3895987272262573 / BPR loss: 1.3861854076385498 / Matching loss: 6.101014150772244e-05 / Item reconstruction: 0.000945208128541708 / Text reconstruction: 0.014398686587810516\n",
            "loss in epoch 3/11 iteration 67/111: 1.3896229267120361 / BPR loss: 1.3862102031707764 / Matching loss: 6.240244692889974e-05 / Item reconstruction: 0.0009720021625980735 / Text reconstruction: 0.014322017319500446\n",
            "loss in epoch 3/11 iteration 68/111: 1.389580488204956 / BPR loss: 1.3862056732177734 / Matching loss: 6.392760406015441e-05 / Item reconstruction: 0.0009273291216231883 / Text reconstruction: 0.014235728420317173\n",
            "loss in epoch 3/11 iteration 69/111: 1.3895719051361084 / BPR loss: 1.3861987590789795 / Matching loss: 6.71908346703276e-05 / Item reconstruction: 0.00094511458883062 / Text reconstruction: 0.014166980981826782\n",
            "loss in epoch 3/11 iteration 70/111: 1.3896063566207886 / BPR loss: 1.3862451314926147 / Matching loss: 6.322009721770883e-05 / Item reconstruction: 0.000936514581553638 / Text reconstruction: 0.014148782938718796\n",
            "loss in epoch 3/11 iteration 71/111: 1.389639973640442 / BPR loss: 1.386290192604065 / Matching loss: 5.731148849008605e-05 / Item reconstruction: 0.0009403866715729237 / Text reconstruction: 0.014111345633864403\n",
            "loss in epoch 3/11 iteration 72/111: 1.3895212411880493 / BPR loss: 1.3861894607543945 / Matching loss: 6.0018166550435126e-05 / Item reconstruction: 0.0009531643008813262 / Text reconstruction: 0.013975925743579865\n",
            "loss in epoch 3/11 iteration 73/111: 1.3895233869552612 / BPR loss: 1.3862290382385254 / Matching loss: 6.350033800117671e-05 / Item reconstruction: 0.0009000140707939863 / Text reconstruction: 0.01390412263572216\n",
            "loss in epoch 3/11 iteration 74/111: 1.3895184993743896 / BPR loss: 1.3861958980560303 / Matching loss: 6.584521179320291e-05 / Item reconstruction: 0.000970970606431365 / Text reconstruction: 0.013856105506420135\n",
            "loss in epoch 3/11 iteration 75/111: 1.3896234035491943 / BPR loss: 1.3863426446914673 / Matching loss: 6.0328940890030935e-05 / Item reconstruction: 0.0009380769333802164 / Text reconstruction: 0.013756807893514633\n",
            "loss in epoch 3/11 iteration 76/111: 1.3895207643508911 / BPR loss: 1.386232614517212 / Matching loss: 6.132690759841353e-05 / Item reconstruction: 0.0009657086338847876 / Text reconstruction: 0.013720589689910412\n",
            "loss in epoch 3/11 iteration 77/111: 1.3894681930541992 / BPR loss: 1.386209487915039 / Matching loss: 6.195988680701703e-05 / Item reconstruction: 0.000935161835514009 / Text reconstruction: 0.013645859435200691\n",
            "loss in epoch 3/11 iteration 78/111: 1.3894798755645752 / BPR loss: 1.3862296342849731 / Matching loss: 6.102836778154597e-05 / Item reconstruction: 0.0009477811399847269 / Text reconstruction: 0.013576613739132881\n",
            "loss in epoch 3/11 iteration 79/111: 1.389496088027954 / BPR loss: 1.3862682580947876 / Matching loss: 5.9959762438666075e-05 / Item reconstruction: 0.0009260401129722595 / Text reconstruction: 0.013524100184440613\n",
            "loss in epoch 3/11 iteration 80/111: 1.3894835710525513 / BPR loss: 1.3862658739089966 / Matching loss: 6.0187026974745095e-05 / Item reconstruction: 0.0009445225587114692 / Text reconstruction: 0.013425713405013084\n",
            "loss in epoch 3/11 iteration 81/111: 1.3894059658050537 / BPR loss: 1.386213779449463 / Matching loss: 6.170543929329142e-05 / Item reconstruction: 0.00090320676099509 / Text reconstruction: 0.01339422445744276\n",
            "loss in epoch 3/11 iteration 82/111: 1.3894376754760742 / BPR loss: 1.3862333297729492 / Matching loss: 6.159691838547587e-05 / Item reconstruction: 0.0009424907038919628 / Text reconstruction: 0.013357285410165787\n",
            "loss in epoch 3/11 iteration 83/111: 1.3894507884979248 / BPR loss: 1.3862521648406982 / Matching loss: 5.976052489131689e-05 / Item reconstruction: 0.0009677393827587366 / Text reconstruction: 0.013274947181344032\n",
            "loss in epoch 3/11 iteration 84/111: 1.3894158601760864 / BPR loss: 1.3862638473510742 / Matching loss: 5.758633778896183e-05 / Item reconstruction: 0.0008939380059018731 / Text reconstruction: 0.013237321749329567\n",
            "loss in epoch 3/11 iteration 85/111: 1.3893643617630005 / BPR loss: 1.3861985206604004 / Matching loss: 6.174435839056969e-05 / Item reconstruction: 0.0009665312827564776 / Text reconstruction: 0.013104218989610672\n",
            "loss in epoch 3/11 iteration 86/111: 1.389369249343872 / BPR loss: 1.3862212896347046 / Matching loss: 6.369579205056652e-05 / Item reconstruction: 0.0009654848836362362 / Text reconstruction: 0.013007335364818573\n",
            "loss in epoch 3/11 iteration 87/111: 1.3893563747406006 / BPR loss: 1.3862272500991821 / Matching loss: 6.29376809229143e-05 / Item reconstruction: 0.0009494280675426126 / Text reconstruction: 0.012957420200109482\n",
            "loss in epoch 3/11 iteration 88/111: 1.3893368244171143 / BPR loss: 1.386247158050537 / Matching loss: 5.8573226851876825e-05 / Item reconstruction: 0.0008773905574344099 / Text reconstruction: 0.012962320819497108\n",
            "loss in epoch 3/11 iteration 89/111: 1.3893582820892334 / BPR loss: 1.3862518072128296 / Matching loss: 5.6509154092054814e-05 / Item reconstruction: 0.0009559466270729899 / Text reconstruction: 0.012859771028161049\n",
            "loss in epoch 3/11 iteration 90/111: 1.389308214187622 / BPR loss: 1.3862272500991821 / Matching loss: 5.654394772136584e-05 / Item reconstruction: 0.0009120912291109562 / Text reconstruction: 0.012841539457440376\n",
            "loss in epoch 3/11 iteration 91/111: 1.3892767429351807 / BPR loss: 1.3862247467041016 / Matching loss: 6.0593676607823e-05 / Item reconstruction: 0.0008927917806431651 / Text reconstruction: 0.012724832631647587\n",
            "loss in epoch 3/11 iteration 92/111: 1.3892611265182495 / BPR loss: 1.3862175941467285 / Matching loss: 6.065181514713913e-05 / Item reconstruction: 0.0009091826505027711 / Text reconstruction: 0.012641465291380882\n",
            "loss in epoch 3/11 iteration 93/111: 1.3893139362335205 / BPR loss: 1.3862459659576416 / Matching loss: 6.105896318331361e-05 / Item reconstruction: 0.0009705651900731027 / Text reconstruction: 0.012608448043465614\n",
            "loss in epoch 3/11 iteration 94/111: 1.3893035650253296 / BPR loss: 1.3862807750701904 / Matching loss: 5.817561032017693e-05 / Item reconstruction: 0.0009142592898570001 / Text reconstruction: 0.0125372763723135\n",
            "loss in epoch 3/11 iteration 95/111: 1.389253854751587 / BPR loss: 1.386240005493164 / Matching loss: 5.817030614707619e-05 / Item reconstruction: 0.0009189400589093566 / Text reconstruction: 0.01248108595609665\n",
            "loss in epoch 3/11 iteration 96/111: 1.3892786502838135 / BPR loss: 1.3862817287445068 / Matching loss: 5.7427725550951436e-05 / Item reconstruction: 0.0009085033088922501 / Text reconstruction: 0.012425793334841728\n",
            "loss in epoch 3/11 iteration 97/111: 1.3892582654953003 / BPR loss: 1.3862497806549072 / Matching loss: 5.91218558838591e-05 / Item reconstruction: 0.0009410794591531157 / Text reconstruction: 0.01239440031349659\n",
            "loss in epoch 3/11 iteration 98/111: 1.3893033266067505 / BPR loss: 1.3863259553909302 / Matching loss: 5.676506771123968e-05 / Item reconstruction: 0.0009217346087098122 / Text reconstruction: 0.012298639863729477\n",
            "loss in epoch 3/11 iteration 99/111: 1.3892403841018677 / BPR loss: 1.386246919631958 / Matching loss: 5.838774086441845e-05 / Item reconstruction: 0.000976966810412705 / Text reconstruction: 0.012232532724738121\n",
            "loss in epoch 3/11 iteration 100/111: 1.3892409801483154 / BPR loss: 1.386274814605713 / Matching loss: 5.8220546634402126e-05 / Item reconstruction: 0.0009591308771632612 / Text reconstruction: 0.012142159044742584\n",
            "loss in epoch 3/11 iteration 101/111: 1.389157772064209 / BPR loss: 1.3862154483795166 / Matching loss: 5.8547710068523884e-05 / Item reconstruction: 0.0009226604597643018 / Text reconstruction: 0.01211215928196907\n",
            "loss in epoch 3/11 iteration 102/111: 1.389169454574585 / BPR loss: 1.386240005493164 / Matching loss: 5.967631295789033e-05 / Item reconstruction: 0.0009299229132011533 / Text reconstruction: 0.01202400028705597\n",
            "loss in epoch 3/11 iteration 103/111: 1.3892096281051636 / BPR loss: 1.3862775564193726 / Matching loss: 5.7206343626603484e-05 / Item reconstruction: 0.0009440237190574408 / Text reconstruction: 0.01201411709189415\n",
            "loss in epoch 3/11 iteration 104/111: 1.3891385793685913 / BPR loss: 1.3862463235855103 / Matching loss: 5.554675226449035e-05 / Item reconstruction: 0.0008876144420355558 / Text reconstruction: 0.011964491568505764\n",
            "loss in epoch 3/11 iteration 105/111: 1.3891377449035645 / BPR loss: 1.386237621307373 / Matching loss: 5.6064196542138234e-05 / Item reconstruction: 0.0009352570050396025 / Text reconstruction: 0.011882256716489792\n",
            "loss in epoch 3/11 iteration 106/111: 1.3891801834106445 / BPR loss: 1.3862800598144531 / Matching loss: 5.977765249554068e-05 / Item reconstruction: 0.000957341049797833 / Text reconstruction: 0.011808972805738449\n",
            "loss in epoch 3/11 iteration 107/111: 1.3891152143478394 / BPR loss: 1.3862590789794922 / Matching loss: 6.0118876717751846e-05 / Item reconstruction: 0.0008968802867457271 / Text reconstruction: 0.011737653985619545\n",
            "loss in epoch 3/11 iteration 108/111: 1.3891531229019165 / BPR loss: 1.3862884044647217 / Matching loss: 5.801982479169965e-05 / Item reconstruction: 0.0009322065743617713 / Text reconstruction: 0.01170288771390915\n",
            "loss in epoch 3/11 iteration 109/111: 1.3891552686691284 / BPR loss: 1.3863279819488525 / Matching loss: 5.298877658788115e-05 / Item reconstruction: 0.0008792118169367313 / Text reconstruction: 0.01167318131774664\n",
            "loss in epoch 3/11 iteration 110/111: 1.389062523841858 / BPR loss: 1.386243224143982 / Matching loss: 5.0267146434634924e-05 / Item reconstruction: 0.0008222467731684446 / Text reconstruction: 0.011788916774094105\n",
            "loss in epoch 3/11 iteration 111/111: 1.3890845775604248 / BPR loss: 1.3863024711608887 / Matching loss: 5.3557276260107756e-05 / Item reconstruction: 0.0007178121013566852 / Text reconstruction: 0.011848485097289085\n",
            " 30% 3/10 [16:09<37:39, 322.86s/it]loss in epoch 4/11 iteration 0/111: 1.3890241384506226 / BPR loss: 1.3862062692642212 / Matching loss: 6.198287883307785e-05 / Item reconstruction: 0.0009616725728847086 / Text reconstruction: 0.01137483213096857\n",
            "loss in epoch 4/11 iteration 1/111: 1.3889758586883545 / BPR loss: 1.3861618041992188 / Matching loss: 5.799181235488504e-05 / Item reconstruction: 0.000949123059399426 / Text reconstruction: 0.011407889425754547\n",
            "loss in epoch 4/11 iteration 2/111: 1.3890180587768555 / BPR loss: 1.3862106800079346 / Matching loss: 5.577103729592636e-05 / Item reconstruction: 0.0009520555613562465 / Text reconstruction: 0.011377839371562004\n",
            "loss in epoch 4/11 iteration 3/111: 1.3890399932861328 / BPR loss: 1.386218786239624 / Matching loss: 5.355167013476603e-05 / Item reconstruction: 0.0010024961084127426 / Text reconstruction: 0.011332221329212189\n",
            "loss in epoch 4/11 iteration 4/111: 1.3890479803085327 / BPR loss: 1.3862515687942505 / Matching loss: 5.75790400034748e-05 / Item reconstruction: 0.0009840975981205702 / Text reconstruction: 0.011233886703848839\n",
            "loss in epoch 4/11 iteration 5/111: 1.3890454769134521 / BPR loss: 1.3862929344177246 / Matching loss: 5.661657633027062e-05 / Item reconstruction: 0.0009174107690341771 / Text reconstruction: 0.011186202988028526\n",
            "loss in epoch 4/11 iteration 6/111: 1.3890855312347412 / BPR loss: 1.3863186836242676 / Matching loss: 5.8922440075548366e-05 / Item reconstruction: 0.0009933424880728126 / Text reconstruction: 0.011056799441576004\n",
            "loss in epoch 4/11 iteration 7/111: 1.3890351057052612 / BPR loss: 1.3863065242767334 / Matching loss: 5.8817051467485726e-05 / Item reconstruction: 0.000942420621868223 / Text reconstruction: 0.010992961935698986\n",
            "loss in epoch 4/11 iteration 8/111: 1.3890342712402344 / BPR loss: 1.3863232135772705 / Matching loss: 5.452365439850837e-05 / Item reconstruction: 0.000889918184839189 / Text reconstruction: 0.011057700961828232\n",
            "loss in epoch 4/11 iteration 9/111: 1.3889648914337158 / BPR loss: 1.386216640472412 / Matching loss: 5.170947406440973e-05 / Item reconstruction: 0.0010012940037995577 / Text reconstruction: 0.010979345068335533\n",
            "loss in epoch 4/11 iteration 10/111: 1.3887532949447632 / BPR loss: 1.3858665227890015 / Matching loss: 5.382657400332391e-05 / Item reconstruction: 0.0012881186557933688 / Text reconstruction: 0.010943773202598095\n",
            "loss in epoch 4/11 iteration 11/111: 1.3887752294540405 / BPR loss: 1.3859148025512695 / Matching loss: 5.7086530432570726e-05 / Item reconstruction: 0.0012490892549976707 / Text reconstruction: 0.010894185863435268\n",
            "loss in epoch 4/11 iteration 12/111: 1.3886046409606934 / BPR loss: 1.3857853412628174 / Matching loss: 6.079676313675009e-05 / Item reconstruction: 0.0011688886443153024 / Text reconstruction: 0.010870258323848248\n",
            "loss in epoch 4/11 iteration 13/111: 1.3885260820388794 / BPR loss: 1.3857762813568115 / Matching loss: 6.311795732472092e-05 / Item reconstruction: 0.0010535968467593193 / Text reconstruction: 0.010799811221659184\n",
            "loss in epoch 4/11 iteration 14/111: 1.388579249382019 / BPR loss: 1.385856032371521 / Matching loss: 5.8453228120924905e-05 / Item reconstruction: 0.0010543516837060452 / Text reconstruction: 0.010688570328056812\n",
            "loss in epoch 4/11 iteration 15/111: 1.3884694576263428 / BPR loss: 1.3858166933059692 / Matching loss: 4.953179814037867e-05 / Item reconstruction: 0.0009420346468687057 / Text reconstruction: 0.010660814121365547\n",
            "loss in epoch 4/11 iteration 16/111: 1.388750433921814 / BPR loss: 1.3860193490982056 / Matching loss: 5.153018355485983e-05 / Item reconstruction: 0.0011089186882600188 / Text reconstruction: 0.010625709779560566\n",
            "loss in epoch 4/11 iteration 17/111: 1.3892054557800293 / BPR loss: 1.3862926959991455 / Matching loss: 5.629463339573704e-05 / Item reconstruction: 0.0014716911828145385 / Text reconstruction: 0.010603269562125206\n",
            "loss in epoch 4/11 iteration 18/111: 1.3891738653182983 / BPR loss: 1.3862656354904175 / Matching loss: 5.887165025342256e-05 / Item reconstruction: 0.0014584732707589865 / Text reconstruction: 0.010600480251014233\n",
            "loss in epoch 4/11 iteration 19/111: 1.3891488313674927 / BPR loss: 1.3862783908843994 / Matching loss: 5.9080353821627796e-05 / Item reconstruction: 0.00139445043168962 / Text reconstruction: 0.010570533573627472\n",
            "loss in epoch 4/11 iteration 20/111: 1.3890669345855713 / BPR loss: 1.3862760066986084 / Matching loss: 5.567863263422623e-05 / Item reconstruction: 0.0013099181232973933 / Text reconstruction: 0.010401333682239056\n",
            "loss in epoch 4/11 iteration 21/111: 1.3889853954315186 / BPR loss: 1.386242151260376 / Matching loss: 5.2145562221994624e-05 / Item reconstruction: 0.0012165997177362442 / Text reconstruction: 0.010414229705929756\n",
            "loss in epoch 4/11 iteration 22/111: 1.3889117240905762 / BPR loss: 1.38626229763031 / Matching loss: 5.2444927860051394e-05 / Item reconstruction: 0.0010927824769169092 / Text reconstruction: 0.010253461077809334\n",
            "loss in epoch 4/11 iteration 23/111: 1.3887412548065186 / BPR loss: 1.3861541748046875 / Matching loss: 5.3755946282763034e-05 / Item reconstruction: 0.0009956524008885026 / Text reconstruction: 0.010177250020205975\n",
            "loss in epoch 4/11 iteration 24/111: 1.3888365030288696 / BPR loss: 1.386257529258728 / Matching loss: 5.807921843370423e-05 / Item reconstruction: 0.0010017678141593933 / Text reconstruction: 0.01010010577738285\n",
            "loss in epoch 4/11 iteration 25/111: 1.3888012170791626 / BPR loss: 1.3862462043762207 / Matching loss: 5.749802221544087e-05 / Item reconstruction: 0.0009788295719772577 / Text reconstruction: 0.010040296241641045\n",
            "loss in epoch 4/11 iteration 26/111: 1.3887739181518555 / BPR loss: 1.3862040042877197 / Matching loss: 5.609104846371338e-05 / Item reconstruction: 0.0010139516089111567 / Text reconstruction: 0.010033605620265007\n",
            "loss in epoch 4/11 iteration 27/111: 1.3888013362884521 / BPR loss: 1.3862338066101074 / Matching loss: 5.113636507303454e-05 / Item reconstruction: 0.0010425264481455088 / Text reconstruction: 0.00997551716864109\n",
            "loss in epoch 4/11 iteration 28/111: 1.3888216018676758 / BPR loss: 1.3862686157226562 / Matching loss: 5.2668285206891596e-05 / Item reconstruction: 0.0010191162582486868 / Text reconstruction: 0.00995420478284359\n",
            "loss in epoch 4/11 iteration 29/111: 1.3888437747955322 / BPR loss: 1.3862831592559814 / Matching loss: 5.330752901500091e-05 / Item reconstruction: 0.00108344666659832 / Text reconstruction: 0.009828336536884308\n",
            "loss in epoch 4/11 iteration 30/111: 1.388834834098816 / BPR loss: 1.3862730264663696 / Matching loss: 5.7916055084206164e-05 / Item reconstruction: 0.0011035425122827291 / Text reconstruction: 0.009760446846485138\n",
            "loss in epoch 4/11 iteration 31/111: 1.3887608051300049 / BPR loss: 1.3862624168395996 / Matching loss: 5.380206130212173e-05 / Item reconstruction: 0.0010169798042625189 / Text reconstruction: 0.00968063436448574\n",
            "loss in epoch 4/11 iteration 32/111: 1.3886805772781372 / BPR loss: 1.3861873149871826 / Matching loss: 5.3535935876425356e-05 / Item reconstruction: 0.0010280006099492311 / Text reconstruction: 0.009628726169466972\n",
            "loss in epoch 4/11 iteration 33/111: 1.388702630996704 / BPR loss: 1.3862159252166748 / Matching loss: 5.258544842945412e-05 / Item reconstruction: 0.0010417723096907139 / Text reconstruction: 0.009565651416778564\n",
            "loss in epoch 4/11 iteration 34/111: 1.388747215270996 / BPR loss: 1.3862733840942383 / Matching loss: 5.3039668273413554e-05 / Item reconstruction: 0.0010380139574408531 / Text reconstruction: 0.00950879417359829\n",
            "loss in epoch 4/11 iteration 35/111: 1.3886646032333374 / BPR loss: 1.3862181901931763 / Matching loss: 4.937228004564531e-05 / Item reconstruction: 0.0010076567996293306 / Text reconstruction: 0.009466222487390041\n",
            "loss in epoch 4/11 iteration 36/111: 1.3886382579803467 / BPR loss: 1.3862152099609375 / Matching loss: 5.121342110214755e-05 / Item reconstruction: 0.0009567445376887918 / Text reconstruction: 0.009467089548707008\n",
            "loss in epoch 4/11 iteration 37/111: 1.3886563777923584 / BPR loss: 1.3862465620040894 / Matching loss: 5.459304156829603e-05 / Item reconstruction: 0.0009659590432420373 / Text reconstruction: 0.00936113577336073\n",
            "loss in epoch 4/11 iteration 38/111: 1.3886642456054688 / BPR loss: 1.3862528800964355 / Matching loss: 5.446061186376028e-05 / Item reconstruction: 0.0009969198144972324 / Text reconstruction: 0.009292276576161385\n",
            "loss in epoch 4/11 iteration 39/111: 1.3885985612869263 / BPR loss: 1.3862247467041016 / Matching loss: 5.1286053349031135e-05 / Item reconstruction: 0.000945496023632586 / Text reconstruction: 0.009248691610991955\n",
            "loss in epoch 4/11 iteration 40/111: 1.3886055946350098 / BPR loss: 1.3862318992614746 / Matching loss: 5.112125290906988e-05 / Item reconstruction: 0.0009754279162734747 / Text reconstruction: 0.009174369275569916\n",
            "loss in epoch 4/11 iteration 41/111: 1.3885915279388428 / BPR loss: 1.3862545490264893 / Matching loss: 5.115267777000554e-05 / Item reconstruction: 0.0009206198737956583 / Text reconstruction: 0.009127880446612835\n",
            "loss in epoch 4/11 iteration 42/111: 1.3886255025863647 / BPR loss: 1.3862738609313965 / Matching loss: 5.12475598952733e-05 / Item reconstruction: 0.0009627219988033175 / Text reconstruction: 0.009095147252082825\n",
            "loss in epoch 4/11 iteration 43/111: 1.3885858058929443 / BPR loss: 1.3862318992614746 / Matching loss: 5.150547076482326e-05 / Item reconstruction: 0.0009913453832268715 / Text reconstruction: 0.009033912792801857\n",
            "loss in epoch 4/11 iteration 44/111: 1.388527750968933 / BPR loss: 1.386194109916687 / Matching loss: 4.932445153826848e-05 / Item reconstruction: 0.0009806108428165317 / Text reconstruction: 0.008970068767666817\n",
            "loss in epoch 4/11 iteration 45/111: 1.3885157108306885 / BPR loss: 1.3862080574035645 / Matching loss: 4.98922418046277e-05 / Item reconstruction: 0.0009476890554651618 / Text reconstruction: 0.00891917198896408\n",
            "loss in epoch 4/11 iteration 46/111: 1.388620138168335 / BPR loss: 1.3863158226013184 / Matching loss: 5.295893788570538e-05 / Item reconstruction: 0.0009692604071460664 / Text reconstruction: 0.008834196254611015\n",
            "loss in epoch 4/11 iteration 47/111: 1.3884731531143188 / BPR loss: 1.3862133026123047 / Matching loss: 5.096905442769639e-05 / Item reconstruction: 0.0009030524524860084 / Text reconstruction: 0.008786382153630257\n",
            "loss in epoch 4/11 iteration 48/111: 1.3884992599487305 / BPR loss: 1.38620126247406 / Matching loss: 5.037079972680658e-05 / Item reconstruction: 0.0009964705677703023 / Text reconstruction: 0.008746478706598282\n",
            "loss in epoch 4/11 iteration 49/111: 1.3884742259979248 / BPR loss: 1.3861926794052124 / Matching loss: 4.999242446501739e-05 / Item reconstruction: 0.000980952987447381 / Text reconstruction: 0.008705563843250275\n",
            "loss in epoch 4/11 iteration 50/111: 1.3884755373001099 / BPR loss: 1.386216402053833 / Matching loss: 5.220443563302979e-05 / Item reconstruction: 0.0009557370794937015 / Text reconstruction: 0.008644881658256054\n",
            "loss in epoch 4/11 iteration 51/111: 1.3884905576705933 / BPR loss: 1.3862485885620117 / Matching loss: 4.968343273503706e-05 / Item reconstruction: 0.0009491996606811881 / Text reconstruction: 0.008588621392846107\n",
            "loss in epoch 4/11 iteration 52/111: 1.3884047269821167 / BPR loss: 1.3861732482910156 / Matching loss: 5.027727092965506e-05 / Item reconstruction: 0.0009556647855788469 / Text reconstruction: 0.008516795933246613\n",
            "loss in epoch 4/11 iteration 53/111: 1.388329267501831 / BPR loss: 1.3861191272735596 / Matching loss: 5.079794573248364e-05 / Item reconstruction: 0.0009346177102997899 / Text reconstruction: 0.008460305631160736\n",
            "loss in epoch 4/11 iteration 54/111: 1.3884316682815552 / BPR loss: 1.38623046875 / Matching loss: 5.024523852625862e-05 / Item reconstruction: 0.0009286910062655807 / Text reconstruction: 0.008433667942881584\n",
            "loss in epoch 4/11 iteration 55/111: 1.388398289680481 / BPR loss: 1.386174201965332 / Matching loss: 5.038567542214878e-05 / Item reconstruction: 0.0009928673971444368 / Text reconstruction: 0.008386410772800446\n",
            "loss in epoch 4/11 iteration 56/111: 1.3884665966033936 / BPR loss: 1.386260747909546 / Matching loss: 5.0299604481551796e-05 / Item reconstruction: 0.0009753144113346934 / Text reconstruction: 0.008339225314557552\n",
            "loss in epoch 4/11 iteration 57/111: 1.3884806632995605 / BPR loss: 1.3862801790237427 / Matching loss: 5.075342414784245e-05 / Item reconstruction: 0.0009725217241793871 / Text reconstruction: 0.008317103609442711\n",
            "loss in epoch 4/11 iteration 58/111: 1.3883870840072632 / BPR loss: 1.3862007856369019 / Matching loss: 5.2172930736560374e-05 / Item reconstruction: 0.0009646196849644184 / Text reconstruction: 0.008258800953626633\n",
            "loss in epoch 4/11 iteration 59/111: 1.3883323669433594 / BPR loss: 1.3861923217773438 / Matching loss: 4.9134243454318494e-05 / Item reconstruction: 0.0009058600990101695 / Text reconstruction: 0.008190080523490906\n",
            "loss in epoch 4/11 iteration 60/111: 1.3883153200149536 / BPR loss: 1.3861680030822754 / Matching loss: 5.051769403507933e-05 / Item reconstruction: 0.0009371066698804498 / Text reconstruction: 0.008140604943037033\n",
            "loss in epoch 4/11 iteration 61/111: 1.3882982730865479 / BPR loss: 1.386136770248413 / Matching loss: 5.064473589300178e-05 / Item reconstruction: 0.0009799790568649769 / Text reconstruction: 0.00810474157333374\n",
            "loss in epoch 4/11 iteration 62/111: 1.3883517980575562 / BPR loss: 1.386230707168579 / Matching loss: 4.927082409267314e-05 / Item reconstruction: 0.0009277599165216088 / Text reconstruction: 0.008040109649300575\n",
            "loss in epoch 4/11 iteration 63/111: 1.3883092403411865 / BPR loss: 1.3861924409866333 / Matching loss: 4.8904999857768416e-05 / Item reconstruction: 0.0009474548860453069 / Text reconstruction: 0.007971227169036865\n",
            "loss in epoch 4/11 iteration 64/111: 1.388303518295288 / BPR loss: 1.3862154483795166 / Matching loss: 4.745399201055989e-05 / Item reconstruction: 0.0008944873698055744 / Text reconstruction: 0.00796685554087162\n",
            "loss in epoch 4/11 iteration 65/111: 1.3883074522018433 / BPR loss: 1.3862131834030151 / Matching loss: 5.041829354013316e-05 / Item reconstruction: 0.0009393743239343166 / Text reconstruction: 0.007870667614042759\n",
            "loss in epoch 4/11 iteration 66/111: 1.3882771730422974 / BPR loss: 1.3862059116363525 / Matching loss: 4.8868561862036586e-05 / Item reconstruction: 0.0009173417929559946 / Text reconstruction: 0.007818227633833885\n",
            "loss in epoch 4/11 iteration 67/111: 1.3882452249526978 / BPR loss: 1.3861591815948486 / Matching loss: 5.022430923418142e-05 / Item reconstruction: 0.0009587365784682333 / Text reconstruction: 0.007782817352563143\n",
            "loss in epoch 4/11 iteration 68/111: 1.388253927230835 / BPR loss: 1.3861762285232544 / Matching loss: 5.1409755542408675e-05 / Item reconstruction: 0.0009695225162431598 / Text reconstruction: 0.007707939017564058\n",
            "loss in epoch 4/11 iteration 69/111: 1.3883498907089233 / BPR loss: 1.386296033859253 / Matching loss: 5.0772519898600876e-05 / Item reconstruction: 0.0009375223307870328 / Text reconstruction: 0.007671726867556572\n",
            "loss in epoch 4/11 iteration 70/111: 1.3882524967193604 / BPR loss: 1.3862227201461792 / Matching loss: 4.6391367504838854e-05 / Item reconstruction: 0.0009059783769771457 / Text reconstruction: 0.007651783991605043\n",
            "loss in epoch 4/11 iteration 71/111: 1.3883165121078491 / BPR loss: 1.3862862586975098 / Matching loss: 4.3957101297564805e-05 / Item reconstruction: 0.000906366971321404 / Text reconstruction: 0.007664871402084827\n",
            "loss in epoch 4/11 iteration 72/111: 1.3880928754806519 / BPR loss: 1.386078119277954 / Matching loss: 4.6822300646454096e-05 / Item reconstruction: 0.0009210807620547712 / Text reconstruction: 0.007537181489169598\n",
            "loss in epoch 4/11 iteration 73/111: 1.3881973028182983 / BPR loss: 1.3862152099609375 / Matching loss: 4.832796912523918e-05 / Item reconstruction: 0.000881811254657805 / Text reconstruction: 0.007464264519512653\n",
            "loss in epoch 4/11 iteration 74/111: 1.388175368309021 / BPR loss: 1.3861606121063232 / Matching loss: 5.125995448906906e-05 / Item reconstruction: 0.0009636421455070376 / Text reconstruction: 0.007408333010971546\n",
            "loss in epoch 4/11 iteration 75/111: 1.3881884813308716 / BPR loss: 1.3861894607543945 / Matching loss: 4.7698395064799115e-05 / Item reconstruction: 0.0009460358414798975 / Text reconstruction: 0.007391434628516436\n",
            "loss in epoch 4/11 iteration 76/111: 1.3882346153259277 / BPR loss: 1.3862483501434326 / Matching loss: 4.851442645303905e-05 / Item reconstruction: 0.000940098543651402 / Text reconstruction: 0.00733835157006979\n",
            "loss in epoch 4/11 iteration 77/111: 1.3881862163543701 / BPR loss: 1.3862206935882568 / Matching loss: 4.589034506352618e-05 / Item reconstruction: 0.000922241248190403 / Text reconstruction: 0.007292804308235645\n",
            "loss in epoch 4/11 iteration 78/111: 1.3881604671478271 / BPR loss: 1.3862121105194092 / Matching loss: 4.5572378439828753e-05 / Item reconstruction: 0.0009069322841241956 / Text reconstruction: 0.007246886845678091\n",
            "loss in epoch 4/11 iteration 79/111: 1.3881251811981201 / BPR loss: 1.3861665725708008 / Matching loss: 4.653165888157673e-05 / Item reconstruction: 0.0009422501316294074 / Text reconstruction: 0.007205301895737648\n",
            "loss in epoch 4/11 iteration 80/111: 1.3882033824920654 / BPR loss: 1.3862700462341309 / Matching loss: 4.8209927626885474e-05 / Item reconstruction: 0.0009228402050212026 / Text reconstruction: 0.007118775509297848\n",
            "loss in epoch 4/11 iteration 81/111: 1.3881112337112427 / BPR loss: 1.3861746788024902 / Matching loss: 4.812313272850588e-05 / Item reconstruction: 0.0009306938736699522 / Text reconstruction: 0.007114944513887167\n",
            "loss in epoch 4/11 iteration 82/111: 1.388084888458252 / BPR loss: 1.386159896850586 / Matching loss: 4.61163763247896e-05 / Item reconstruction: 0.0009210279677063227 / Text reconstruction: 0.007091503590345383\n",
            "loss in epoch 4/11 iteration 83/111: 1.388192892074585 / BPR loss: 1.386277675628662 / Matching loss: 4.718589116237126e-05 / Item reconstruction: 0.0009291224414482713 / Text reconstruction: 0.007017032243311405\n",
            "loss in epoch 4/11 iteration 84/111: 1.3880966901779175 / BPR loss: 1.3861799240112305 / Matching loss: 4.5207889343146235e-05 / Item reconstruction: 0.0009446598123759031 / Text reconstruction: 0.006996392738074064\n",
            "loss in epoch 4/11 iteration 85/111: 1.3881487846374512 / BPR loss: 1.386229395866394 / Matching loss: 4.762083335663192e-05 / Item reconstruction: 0.0009843679144978523 / Text reconstruction: 0.0068982928059995174\n",
            "loss in epoch 4/11 iteration 86/111: 1.3881280422210693 / BPR loss: 1.3862359523773193 / Matching loss: 4.812782208318822e-05 / Item reconstruction: 0.0009513439144939184 / Text reconstruction: 0.006841316819190979\n",
            "loss in epoch 4/11 iteration 87/111: 1.3880985975265503 / BPR loss: 1.3862247467041016 / Matching loss: 4.57410751550924e-05 / Item reconstruction: 0.0009315033676102757 / Text reconstruction: 0.006811709143221378\n",
            "loss in epoch 4/11 iteration 88/111: 1.3880250453948975 / BPR loss: 1.3861719369888306 / Matching loss: 4.313166209612973e-05 / Item reconstruction: 0.0008924076100811362 / Text reconstruction: 0.006818945519626141\n",
            "loss in epoch 4/11 iteration 89/111: 1.3881142139434814 / BPR loss: 1.3862556219100952 / Matching loss: 4.425808583619073e-05 / Item reconstruction: 0.000941005942877382 / Text reconstruction: 0.006719390396028757\n",
            "loss in epoch 4/11 iteration 90/111: 1.3880658149719238 / BPR loss: 1.3861987590789795 / Matching loss: 4.599810563377105e-05 / Item reconstruction: 0.0009632945293560624 / Text reconstruction: 0.0066974349319934845\n",
            "loss in epoch 4/11 iteration 91/111: 1.3881003856658936 / BPR loss: 1.3862590789794922 / Matching loss: 4.870106204180047e-05 / Item reconstruction: 0.0009303494589403272 / Text reconstruction: 0.006637220270931721\n",
            "loss in epoch 4/11 iteration 92/111: 1.3880248069763184 / BPR loss: 1.3862006664276123 / Matching loss: 4.564546179608442e-05 / Item reconstruction: 0.0009203319204971194 / Text reconstruction: 0.0065918914042413235\n",
            "loss in epoch 4/11 iteration 93/111: 1.3879764080047607 / BPR loss: 1.3861510753631592 / Matching loss: 4.422312485985458e-05 / Item reconstruction: 0.0009428791236132383 / Text reconstruction: 0.006548252422362566\n",
            "loss in epoch 4/11 iteration 94/111: 1.388137936592102 / BPR loss: 1.3863195180892944 / Matching loss: 4.426779196364805e-05 / Item reconstruction: 0.0009460581932216883 / Text reconstruction: 0.006505575962364674\n",
            "loss in epoch 4/11 iteration 95/111: 1.3880730867385864 / BPR loss: 1.3862593173980713 / Matching loss: 4.4469434214988723e-05 / Item reconstruction: 0.0009528634836897254 / Text reconstruction: 0.006464173085987568\n",
            "loss in epoch 4/11 iteration 96/111: 1.3880481719970703 / BPR loss: 1.3862632513046265 / Matching loss: 4.422501660883427e-05 / Item reconstruction: 0.0009109266102313995 / Text reconstruction: 0.006425787694752216\n",
            "loss in epoch 4/11 iteration 97/111: 1.388094186782837 / BPR loss: 1.3863178491592407 / Matching loss: 4.521809023572132e-05 / Item reconstruction: 0.0008955216035246849 / Text reconstruction: 0.006417302880436182\n",
            "loss in epoch 4/11 iteration 98/111: 1.3880565166473389 / BPR loss: 1.3863037824630737 / Matching loss: 4.35243418905884e-05 / Item reconstruction: 0.0008820907096378505 / Text reconstruction: 0.006340664811432362\n",
            "loss in epoch 4/11 iteration 99/111: 1.3880518674850464 / BPR loss: 1.3862662315368652 / Matching loss: 4.462996730580926e-05 / Item reconstruction: 0.0009680291987024248 / Text reconstruction: 0.0062851691618561745\n",
            "loss in epoch 4/11 iteration 100/111: 1.3879621028900146 / BPR loss: 1.3862175941467285 / Matching loss: 4.317939601605758e-05 / Item reconstruction: 0.0009105436038225889 / Text reconstruction: 0.006230465602129698\n",
            "loss in epoch 4/11 iteration 101/111: 1.3879306316375732 / BPR loss: 1.386186957359314 / Matching loss: 4.3525404180400074e-05 / Item reconstruction: 0.0009173693251796067 / Text reconstruction: 0.0062075224705040455\n",
            "loss in epoch 4/11 iteration 102/111: 1.3879635334014893 / BPR loss: 1.3862226009368896 / Matching loss: 4.546635318547487e-05 / Item reconstruction: 0.0009394129738211632 / Text reconstruction: 0.006128991954028606\n",
            "loss in epoch 4/11 iteration 103/111: 1.3879996538162231 / BPR loss: 1.3862569332122803 / Matching loss: 4.257545151631348e-05 / Item reconstruction: 0.0009418879635632038 / Text reconstruction: 0.006146115250885487\n",
            "loss in epoch 4/11 iteration 104/111: 1.3879725933074951 / BPR loss: 1.386256456375122 / Matching loss: 4.192049891571514e-05 / Item reconstruction: 0.0008974575903266668 / Text reconstruction: 0.006127191241830587\n",
            "loss in epoch 4/11 iteration 105/111: 1.3879152536392212 / BPR loss: 1.386188268661499 / Matching loss: 4.309325959184207e-05 / Item reconstruction: 0.0009473678655922413 / Text reconstruction: 0.006051292642951012\n",
            "loss in epoch 4/11 iteration 106/111: 1.3879715204238892 / BPR loss: 1.3862543106079102 / Matching loss: 4.5179669541539624e-05 / Item reconstruction: 0.0009472642559558153 / Text reconstruction: 0.005991896614432335\n",
            "loss in epoch 4/11 iteration 107/111: 1.3878718614578247 / BPR loss: 1.3861985206604004 / Matching loss: 4.3827145418617874e-05 / Item reconstruction: 0.0008777338080108166 / Text reconstruction: 0.005953371524810791\n",
            "loss in epoch 4/11 iteration 108/111: 1.3879438638687134 / BPR loss: 1.3862519264221191 / Matching loss: 4.311518569011241e-05 / Item reconstruction: 0.0009285687119700015 / Text reconstruction: 0.0059221466071903706\n",
            "loss in epoch 4/11 iteration 109/111: 1.3879231214523315 / BPR loss: 1.3862470388412476 / Matching loss: 3.956730870413594e-05 / Item reconstruction: 0.0009132417617365718 / Text reconstruction: 0.005899867974221706\n",
            "loss in epoch 4/11 iteration 110/111: 1.3879038095474243 / BPR loss: 1.3862295150756836 / Matching loss: 3.7404832255560905e-05 / Item reconstruction: 0.0008425557753071189 / Text reconstruction: 0.006078159436583519\n",
            "loss in epoch 4/11 iteration 111/111: 1.387925624847412 / BPR loss: 1.3863000869750977 / Matching loss: 3.7774774682475254e-05 / Item reconstruction: 0.0007159567903727293 / Text reconstruction: 0.006148864049464464\n",
            " 40% 4/10 [21:31<32:14, 322.34s/it]loss in epoch 5/11 iteration 0/111: 1.3878930807113647 / BPR loss: 1.3862242698669434 / Matching loss: 4.756585985887796e-05 / Item reconstruction: 0.0009569375542923808 / Text reconstruction: 0.005713711958378553\n",
            "loss in epoch 5/11 iteration 1/111: 1.3878989219665527 / BPR loss: 1.3862380981445312 / Matching loss: 4.2115585529245436e-05 / Item reconstruction: 0.0009426657343283296 / Text reconstruction: 0.0057370783761143684\n",
            "loss in epoch 5/11 iteration 2/111: 1.3879042863845825 / BPR loss: 1.3862240314483643 / Matching loss: 4.281968722352758e-05 / Item reconstruction: 0.000985453138127923 / Text reconstruction: 0.005723883397877216\n",
            "loss in epoch 5/11 iteration 3/111: 1.3877943754196167 / BPR loss: 1.3861205577850342 / Matching loss: 4.1482693632133305e-05 / Item reconstruction: 0.0009872507071122527 / Text reconstruction: 0.0056935520842671394\n",
            "loss in epoch 5/11 iteration 4/111: 1.3878954648971558 / BPR loss: 1.3862099647521973 / Matching loss: 4.481911309994757e-05 / Item reconstruction: 0.0010369946248829365 / Text reconstruction: 0.0056114718317985535\n",
            "loss in epoch 5/11 iteration 5/111: 1.3878638744354248 / BPR loss: 1.386245846748352 / Matching loss: 4.1433333535678685e-05 / Item reconstruction: 0.0009108128724619746 / Text reconstruction: 0.0056059942580759525\n",
            "loss in epoch 5/11 iteration 6/111: 1.3879050016403198 / BPR loss: 1.3862749338150024 / Matching loss: 4.4357333536027e-05 / Item reconstruction: 0.0009857332333922386 / Text reconstruction: 0.00546463206410408\n",
            "loss in epoch 5/11 iteration 7/111: 1.3878291845321655 / BPR loss: 1.386225700378418 / Matching loss: 4.786534555023536e-05 / Item reconstruction: 0.0009473385289311409 / Text reconstruction: 0.0054098572582006454\n",
            "loss in epoch 5/11 iteration 8/111: 1.3878518342971802 / BPR loss: 1.3862344026565552 / Matching loss: 4.536280903266743e-05 / Item reconstruction: 0.0009419734124094248 / Text reconstruction: 0.005505143664777279\n",
            "loss in epoch 5/11 iteration 9/111: 1.3878623247146606 / BPR loss: 1.3862403631210327 / Matching loss: 3.984659269917756e-05 / Item reconstruction: 0.0009847929468378425 / Text reconstruction: 0.005448206327855587\n",
            "loss in epoch 5/11 iteration 10/111: 1.3873926401138306 / BPR loss: 1.3855922222137451 / Matching loss: 3.7884477933403105e-05 / Item reconstruction: 0.0013307101326063275 / Text reconstruction: 0.005486130714416504\n",
            "loss in epoch 5/11 iteration 11/111: 1.387412190437317 / BPR loss: 1.3856704235076904 / Matching loss: 3.56251148332376e-05 / Item reconstruction: 0.0012114429846405983 / Text reconstruction: 0.005502011626958847\n",
            "loss in epoch 5/11 iteration 12/111: 1.387272834777832 / BPR loss: 1.3855667114257812 / Matching loss: 4.183128839940764e-05 / Item reconstruction: 0.0011316293384879827 / Text reconstruction: 0.005492611788213253\n",
            "loss in epoch 5/11 iteration 13/111: 1.3873084783554077 / BPR loss: 1.385661244392395 / Matching loss: 4.6193585149012506e-05 / Item reconstruction: 0.001043235999532044 / Text reconstruction: 0.0053969924338161945\n",
            "loss in epoch 5/11 iteration 14/111: 1.3873316049575806 / BPR loss: 1.385731816291809 / Matching loss: 4.5234031858853996e-05 / Item reconstruction: 0.0009917241986840963 / Text reconstruction: 0.005293695256114006\n",
            "loss in epoch 5/11 iteration 15/111: 1.3872407674789429 / BPR loss: 1.3856587409973145 / Matching loss: 3.966582880821079e-05 / Item reconstruction: 0.000972252688370645 / Text reconstruction: 0.005280844867229462\n",
            "loss in epoch 5/11 iteration 16/111: 1.3875679969787598 / BPR loss: 1.3859138488769531 / Matching loss: 3.4627730201464146e-05 / Item reconstruction: 0.0011132888030260801 / Text reconstruction: 0.005314719397574663\n",
            "loss in epoch 5/11 iteration 17/111: 1.3880951404571533 / BPR loss: 1.3862016201019287 / Matching loss: 3.921625466318801e-05 / Item reconstruction: 0.0015435516834259033 / Text reconstruction: 0.005412654019892216\n",
            "loss in epoch 5/11 iteration 18/111: 1.3881192207336426 / BPR loss: 1.3862543106079102 / Matching loss: 4.150485619902611e-05 / Item reconstruction: 0.0014976926613599062 / Text reconstruction: 0.005372615065425634\n",
            "loss in epoch 5/11 iteration 19/111: 1.388033390045166 / BPR loss: 1.3862178325653076 / Matching loss: 4.16444381698966e-05 / Item reconstruction: 0.0014216634444892406 / Text reconstruction: 0.0053153447806835175\n",
            "loss in epoch 5/11 iteration 20/111: 1.3880163431167603 / BPR loss: 1.3862733840942383 / Matching loss: 4.3677086068782955e-05 / Item reconstruction: 0.0013433670392259955 / Text reconstruction: 0.0051384069956839085\n",
            "loss in epoch 5/11 iteration 21/111: 1.3879157304763794 / BPR loss: 1.3862848281860352 / Matching loss: 4.0690021705813706e-05 / Item reconstruction: 0.001132937497459352 / Text reconstruction: 0.005118933506309986\n",
            "loss in epoch 5/11 iteration 22/111: 1.387839674949646 / BPR loss: 1.3862699270248413 / Matching loss: 4.1026440158020705e-05 / Item reconstruction: 0.001079109264537692 / Text reconstruction: 0.004946211352944374\n",
            "loss in epoch 5/11 iteration 23/111: 1.3876712322235107 / BPR loss: 1.3861405849456787 / Matching loss: 3.98075717384927e-05 / Item reconstruction: 0.001024496741592884 / Text reconstruction: 0.004892732948064804\n",
            "loss in epoch 5/11 iteration 24/111: 1.3876980543136597 / BPR loss: 1.3862042427062988 / Matching loss: 4.20013748225756e-05 / Item reconstruction: 0.0009684825199656188 / Text reconstruction: 0.004838206805288792\n",
            "loss in epoch 5/11 iteration 25/111: 1.3876796960830688 / BPR loss: 1.386203408241272 / Matching loss: 4.1947077988879755e-05 / Item reconstruction: 0.0009515810525044799 / Text reconstruction: 0.004792904015630484\n",
            "loss in epoch 5/11 iteration 26/111: 1.3877838850021362 / BPR loss: 1.3862595558166504 / Matching loss: 4.296957195037976e-05 / Item reconstruction: 0.0010385341010987759 / Text reconstruction: 0.004810783080756664\n",
            "loss in epoch 5/11 iteration 27/111: 1.387719750404358 / BPR loss: 1.3862053155899048 / Matching loss: 3.794071380980313e-05 / Item reconstruction: 0.0010446305386722088 / Text reconstruction: 0.0047715031541883945\n",
            "loss in epoch 5/11 iteration 28/111: 1.3877471685409546 / BPR loss: 1.3862452507019043 / Matching loss: 3.9676480810157955e-05 / Item reconstruction: 0.0010165693238377571 / Text reconstruction: 0.004769784398376942\n",
            "loss in epoch 5/11 iteration 29/111: 1.3877559900283813 / BPR loss: 1.3862404823303223 / Matching loss: 4.131726745981723e-05 / Item reconstruction: 0.001094216713681817 / Text reconstruction: 0.004635671619325876\n",
            "loss in epoch 5/11 iteration 30/111: 1.3877402544021606 / BPR loss: 1.3862277269363403 / Matching loss: 4.3632819142658263e-05 / Item reconstruction: 0.001102642621845007 / Text reconstruction: 0.004587758332490921\n",
            "loss in epoch 5/11 iteration 31/111: 1.3877302408218384 / BPR loss: 1.38624906539917 / Matching loss: 3.879377982229926e-05 / Item reconstruction: 0.001062957919202745 / Text reconstruction: 0.004555029794573784\n",
            "loss in epoch 5/11 iteration 32/111: 1.3877043724060059 / BPR loss: 1.3862543106079102 / Matching loss: 3.809005647781305e-05 / Item reconstruction: 0.0010146196000277996 / Text reconstruction: 0.004522999282926321\n",
            "loss in epoch 5/11 iteration 33/111: 1.3876681327819824 / BPR loss: 1.3862032890319824 / Matching loss: 4.0072503907140344e-05 / Item reconstruction: 0.0010627905139699578 / Text reconstruction: 0.004466530401259661\n",
            "loss in epoch 5/11 iteration 34/111: 1.3876765966415405 / BPR loss: 1.386239767074585 / Matching loss: 3.98168085666839e-05 / Item reconstruction: 0.0010267171310260892 / Text reconstruction: 0.004418348893523216\n",
            "loss in epoch 5/11 iteration 35/111: 1.3875967264175415 / BPR loss: 1.3861982822418213 / Matching loss: 3.724676207639277e-05 / Item reconstruction: 0.0009649720741435885 / Text reconstruction: 0.004393863491714001\n",
            "loss in epoch 5/11 iteration 36/111: 1.3876217603683472 / BPR loss: 1.3862013816833496 / Matching loss: 3.568315878510475e-05 / Item reconstruction: 0.0010098015191033483 / Text reconstruction: 0.004399508237838745\n",
            "loss in epoch 5/11 iteration 37/111: 1.3875881433486938 / BPR loss: 1.386221170425415 / Matching loss: 3.8344835047610104e-05 / Item reconstruction: 0.0009337791707366705 / Text reconstruction: 0.0043084328062832355\n",
            "loss in epoch 5/11 iteration 38/111: 1.3875774145126343 / BPR loss: 1.386197805404663 / Matching loss: 4.1322127799503505e-05 / Item reconstruction: 0.000974970986135304 / Text reconstruction: 0.0042542582377791405\n",
            "loss in epoch 5/11 iteration 39/111: 1.3875471353530884 / BPR loss: 1.3861902952194214 / Matching loss: 4.0397375414613634e-05 / Item reconstruction: 0.0009415734093636274 / Text reconstruction: 0.004228583537042141\n",
            "loss in epoch 5/11 iteration 40/111: 1.387582540512085 / BPR loss: 1.3862295150756836 / Matching loss: 3.754424687940627e-05 / Item reconstruction: 0.000948278815485537 / Text reconstruction: 0.004206825979053974\n",
            "loss in epoch 5/11 iteration 41/111: 1.3875782489776611 / BPR loss: 1.3862285614013672 / Matching loss: 3.689412915264256e-05 / Item reconstruction: 0.0009594750590622425 / Text reconstruction: 0.004165793769061565\n",
            "loss in epoch 5/11 iteration 42/111: 1.3875552415847778 / BPR loss: 1.3862056732177734 / Matching loss: 3.503299012663774e-05 / Item reconstruction: 0.0009684007382020354 / Text reconstruction: 0.004151433706283569\n",
            "loss in epoch 5/11 iteration 43/111: 1.3875927925109863 / BPR loss: 1.386234998703003 / Matching loss: 3.851720248349011e-05 / Item reconstruction: 0.0010011186823248863 / Text reconstruction: 0.004093665163964033\n",
            "loss in epoch 5/11 iteration 44/111: 1.3875188827514648 / BPR loss: 1.3861989974975586 / Matching loss: 3.71546957467217e-05 / Item reconstruction: 0.0009451153455302119 / Text reconstruction: 0.0040509882383048534\n",
            "loss in epoch 5/11 iteration 45/111: 1.387544870376587 / BPR loss: 1.386235237121582 / Matching loss: 3.661767550511286e-05 / Item reconstruction: 0.000935868127271533 / Text reconstruction: 0.004025772679597139\n",
            "loss in epoch 5/11 iteration 46/111: 1.387540340423584 / BPR loss: 1.3862472772598267 / Matching loss: 3.623960219556466e-05 / Item reconstruction: 0.0009230588912032545 / Text reconstruction: 0.003976287320256233\n",
            "loss in epoch 5/11 iteration 47/111: 1.387457251548767 / BPR loss: 1.3861687183380127 / Matching loss: 3.845061655738391e-05 / Item reconstruction: 0.0009276660275645554 / Text reconstruction: 0.00393094215542078\n",
            "loss in epoch 5/11 iteration 48/111: 1.387523889541626 / BPR loss: 1.3862059116363525 / Matching loss: 3.926905628759414e-05 / Item reconstruction: 0.0009959943126887083 / Text reconstruction: 0.003903761273249984\n",
            "loss in epoch 5/11 iteration 49/111: 1.3874567747116089 / BPR loss: 1.3861684799194336 / Matching loss: 3.667515920824371e-05 / Item reconstruction: 0.0009529597591608763 / Text reconstruction: 0.0038753310218453407\n",
            "loss in epoch 5/11 iteration 50/111: 1.387458086013794 / BPR loss: 1.3861689567565918 / Matching loss: 3.6757326597580686e-05 / Item reconstruction: 0.0009675276232883334 / Text reconstruction: 0.0038431365974247456\n",
            "loss in epoch 5/11 iteration 51/111: 1.3874895572662354 / BPR loss: 1.3862100839614868 / Matching loss: 3.491280222078785e-05 / Item reconstruction: 0.0009661120129749179 / Text reconstruction: 0.003807567525655031\n",
            "loss in epoch 5/11 iteration 52/111: 1.387407660484314 / BPR loss: 1.386161208152771 / Matching loss: 3.5516095522325486e-05 / Item reconstruction: 0.0009190913988277316 / Text reconstruction: 0.003756933845579624\n",
            "loss in epoch 5/11 iteration 53/111: 1.3873291015625 / BPR loss: 1.3861006498336792 / Matching loss: 3.64543629984837e-05 / Item reconstruction: 0.0008934101788327098 / Text reconstruction: 0.003726230701431632\n",
            "loss in epoch 5/11 iteration 54/111: 1.3875110149383545 / BPR loss: 1.3862485885620117 / Matching loss: 3.7858848372707143e-05 / Item reconstruction: 0.0009724840056151152 / Text reconstruction: 0.0036914157681167126\n",
            "loss in epoch 5/11 iteration 55/111: 1.3874067068099976 / BPR loss: 1.3861539363861084 / Matching loss: 3.664413088699803e-05 / Item reconstruction: 0.0009628930129110813 / Text reconstruction: 0.003673650324344635\n",
            "loss in epoch 5/11 iteration 56/111: 1.3875463008880615 / BPR loss: 1.3862931728363037 / Matching loss: 3.876992195728235e-05 / Item reconstruction: 0.0009728696895763278 / Text reconstruction: 0.0036395681090652943\n",
            "loss in epoch 5/11 iteration 57/111: 1.387494683265686 / BPR loss: 1.3862602710723877 / Matching loss: 3.580959310056642e-05 / Item reconstruction: 0.0009492236422374845 / Text reconstruction: 0.003620650852099061\n",
            "loss in epoch 5/11 iteration 58/111: 1.3873286247253418 / BPR loss: 1.386115550994873 / Matching loss: 3.4123608202207834e-05 / Item reconstruction: 0.0009198120096698403 / Text reconstruction: 0.003595382208004594\n",
            "loss in epoch 5/11 iteration 59/111: 1.3873971700668335 / BPR loss: 1.3861854076385498 / Matching loss: 3.610018757171929e-05 / Item reconstruction: 0.0009272905299440026 / Text reconstruction: 0.003560137702152133\n",
            "loss in epoch 5/11 iteration 60/111: 1.3873324394226074 / BPR loss: 1.3861361742019653 / Matching loss: 3.7223002436803654e-05 / Item reconstruction: 0.0009183242218568921 / Text reconstruction: 0.0034995642490684986\n",
            "loss in epoch 5/11 iteration 61/111: 1.3873403072357178 / BPR loss: 1.3861123323440552 / Matching loss: 3.8050409784773365e-05 / Item reconstruction: 0.0009861423168331385 / Text reconstruction: 0.00348458718508482\n",
            "loss in epoch 5/11 iteration 62/111: 1.38749098777771 / BPR loss: 1.3863093852996826 / Matching loss: 3.485113848000765e-05 / Item reconstruction: 0.0009127464727498591 / Text reconstruction: 0.003452444449067116\n",
            "loss in epoch 5/11 iteration 63/111: 1.3874140977859497 / BPR loss: 1.3862249851226807 / Matching loss: 3.6132940294919536e-05 / Item reconstruction: 0.0009404731099493802 / Text reconstruction: 0.0034134886227548122\n",
            "loss in epoch 5/11 iteration 64/111: 1.3874804973602295 / BPR loss: 1.3862990140914917 / Matching loss: 3.46962406183593e-05 / Item reconstruction: 0.0009247139096260071 / Text reconstruction: 0.00342218647710979\n",
            "loss in epoch 5/11 iteration 65/111: 1.38734769821167 / BPR loss: 1.386165738105774 / Matching loss: 3.542028571246192e-05 / Item reconstruction: 0.0009522594045847654 / Text reconstruction: 0.0033521740697324276\n",
            "loss in epoch 5/11 iteration 66/111: 1.3874536752700806 / BPR loss: 1.3863039016723633 / Matching loss: 3.4672688343562186e-05 / Item reconstruction: 0.0009034153772518039 / Text reconstruction: 0.0033170697279274464\n",
            "loss in epoch 5/11 iteration 67/111: 1.3873989582061768 / BPR loss: 1.3862214088439941 / Matching loss: 3.6670917324954644e-05 / Item reconstruction: 0.0009667168487794697 / Text reconstruction: 0.0032874783501029015\n",
            "loss in epoch 5/11 iteration 68/111: 1.3873628377914429 / BPR loss: 1.3862104415893555 / Matching loss: 3.5286942875245586e-05 / Item reconstruction: 0.0009321820689365268 / Text reconstruction: 0.0032551998738199472\n",
            "loss in epoch 5/11 iteration 69/111: 1.3873622417449951 / BPR loss: 1.3862287998199463 / Matching loss: 3.4974495065398514e-05 / Item reconstruction: 0.000906050787307322 / Text reconstruction: 0.00322745181620121\n",
            "loss in epoch 5/11 iteration 70/111: 1.3873449563980103 / BPR loss: 1.3862037658691406 / Matching loss: 3.6091798392590135e-05 / Item reconstruction: 0.0009274788317270577 / Text reconstruction: 0.0032066290732473135\n",
            "loss in epoch 5/11 iteration 71/111: 1.387334942817688 / BPR loss: 1.386205792427063 / Matching loss: 3.477358040981926e-05 / Item reconstruction: 0.0009068603976629674 / Text reconstruction: 0.003204549662768841\n",
            "loss in epoch 5/11 iteration 72/111: 1.3873252868652344 / BPR loss: 1.3862004280090332 / Matching loss: 3.366308374097571e-05 / Item reconstruction: 0.000923380721360445 / Text reconstruction: 0.0031478453893214464\n",
            "loss in epoch 5/11 iteration 73/111: 1.3872766494750977 / BPR loss: 1.3861794471740723 / Matching loss: 3.26941953971982e-05 / Item reconstruction: 0.0008796063484624028 / Text reconstruction: 0.0031239702366292477\n",
            "loss in epoch 5/11 iteration 74/111: 1.3873275518417358 / BPR loss: 1.3862063884735107 / Matching loss: 3.67639004252851e-05 / Item reconstruction: 0.000935515621677041 / Text reconstruction: 0.003083282383158803\n",
            "loss in epoch 5/11 iteration 75/111: 1.3873038291931152 / BPR loss: 1.386183261871338 / Matching loss: 3.5442491935100406e-05 / Item reconstruction: 0.0009595694136805832 / Text reconstruction: 0.0030264612287282944\n",
            "loss in epoch 5/11 iteration 76/111: 1.387306571006775 / BPR loss: 1.3861753940582275 / Matching loss: 3.6128549254499376e-05 / Item reconstruction: 0.0009805909357964993 / Text reconstruction: 0.0030236721504479647\n",
            "loss in epoch 5/11 iteration 77/111: 1.3873941898345947 / BPR loss: 1.3863136768341064 / Matching loss: 3.2331416150555015e-05 / Item reconstruction: 0.0009012658847495914 / Text reconstruction: 0.0029881431255489588\n",
            "loss in epoch 5/11 iteration 78/111: 1.3873223066329956 / BPR loss: 1.3862425088882446 / Matching loss: 3.161959830322303e-05 / Item reconstruction: 0.000909948255866766 / Text reconstruction: 0.002966094296425581\n",
            "loss in epoch 5/11 iteration 79/111: 1.3873218297958374 / BPR loss: 1.3862310647964478 / Matching loss: 3.307652150397189e-05 / Item reconstruction: 0.0009350591572001576 / Text reconstruction: 0.0029509011656045914\n",
            "loss in epoch 5/11 iteration 80/111: 1.3872655630111694 / BPR loss: 1.3861937522888184 / Matching loss: 3.525008651195094e-05 / Item reconstruction: 0.0009160160552710295 / Text reconstruction: 0.002892805729061365\n",
            "loss in epoch 5/11 iteration 81/111: 1.3872426748275757 / BPR loss: 1.3861498832702637 / Matching loss: 3.726723662111908e-05 / Item reconstruction: 0.0009516667923890054 / Text reconstruction: 0.0028981762006878853\n",
            "loss in epoch 5/11 iteration 82/111: 1.3871848583221436 / BPR loss: 1.386110544204712 / Matching loss: 3.308707891847007e-05 / Item reconstruction: 0.0009306103456765413 / Text reconstruction: 0.0028795702382922173\n",
            "loss in epoch 5/11 iteration 83/111: 1.3873813152313232 / BPR loss: 1.386309266090393 / Matching loss: 3.165866655763239e-05 / Item reconstruction: 0.0009536431171000004 / Text reconstruction: 0.002817449625581503\n",
            "loss in epoch 5/11 iteration 84/111: 1.3872289657592773 / BPR loss: 1.3861627578735352 / Matching loss: 3.113161801593378e-05 / Item reconstruction: 0.0009388795588165522 / Text reconstruction: 0.002828007098287344\n",
            "loss in epoch 5/11 iteration 85/111: 1.3872531652450562 / BPR loss: 1.3861989974975586 / Matching loss: 3.476372512523085e-05 / Item reconstruction: 0.0009379790863022208 / Text reconstruction: 0.002752136904746294\n",
            "loss in epoch 5/11 iteration 86/111: 1.3871928453445435 / BPR loss: 1.386150598526001 / Matching loss: 3.530427784426138e-05 / Item reconstruction: 0.0009281886741518974 / Text reconstruction: 0.0027140986640006304\n",
            "loss in epoch 5/11 iteration 87/111: 1.3873319625854492 / BPR loss: 1.386272668838501 / Matching loss: 3.403549635550007e-05 / Item reconstruction: 0.0009724567644298077 / Text reconstruction: 0.0026946361176669598\n",
            "loss in epoch 5/11 iteration 88/111: 1.3872195482254028 / BPR loss: 1.3861736059188843 / Matching loss: 3.222084342269227e-05 / Item reconstruction: 0.000931987538933754 / Text reconstruction: 0.0027391095645725727\n",
            "loss in epoch 5/11 iteration 89/111: 1.387282133102417 / BPR loss: 1.3862446546554565 / Matching loss: 3.239847137592733e-05 / Item reconstruction: 0.0009473645477555692 / Text reconstruction: 0.002656306605786085\n",
            "loss in epoch 5/11 iteration 90/111: 1.3871055841445923 / BPR loss: 1.3860905170440674 / Matching loss: 3.2629755878588185e-05 / Item reconstruction: 0.0009100981988012791 / Text reconstruction: 0.0026367453392595053\n",
            "loss in epoch 5/11 iteration 91/111: 1.3871848583221436 / BPR loss: 1.3861740827560425 / Matching loss: 3.303386256447993e-05 / Item reconstruction: 0.0009178683394566178 / Text reconstruction: 0.002593802288174629\n",
            "loss in epoch 5/11 iteration 92/111: 1.3871145248413086 / BPR loss: 1.3861242532730103 / Matching loss: 3.295219721621834e-05 / Item reconstruction: 0.0008894772036001086 / Text reconstruction: 0.0025629934389144182\n",
            "loss in epoch 5/11 iteration 93/111: 1.3872015476226807 / BPR loss: 1.3861756324768066 / Matching loss: 3.326146907056682e-05 / Item reconstruction: 0.0009715086780488491 / Text reconstruction: 0.0025343105662614107\n",
            "loss in epoch 5/11 iteration 94/111: 1.3871924877166748 / BPR loss: 1.3861777782440186 / Matching loss: 3.273212132626213e-05 / Item reconstruction: 0.0009514682460576296 / Text reconstruction: 0.002530581783503294\n",
            "loss in epoch 5/11 iteration 95/111: 1.387135624885559 / BPR loss: 1.3861545324325562 / Matching loss: 3.082969124079682e-05 / Item reconstruction: 0.0008911909535527229 / Text reconstruction: 0.002522885799407959\n",
            "loss in epoch 5/11 iteration 96/111: 1.3872967958450317 / BPR loss: 1.3863177299499512 / Matching loss: 3.1102092179935426e-05 / Item reconstruction: 0.0008947851601988077 / Text reconstruction: 0.0025029275566339493\n",
            "loss in epoch 5/11 iteration 97/111: 1.3872435092926025 / BPR loss: 1.3862401247024536 / Matching loss: 3.401080903131515e-05 / Item reconstruction: 0.0009431231301277876 / Text reconstruction: 0.002488969825208187\n",
            "loss in epoch 5/11 iteration 98/111: 1.3872936964035034 / BPR loss: 1.3863298892974854 / Matching loss: 3.2599396945443004e-05 / Item reconstruction: 0.0008946043672040105 / Text reconstruction: 0.0024199087638407946\n",
            "loss in epoch 5/11 iteration 99/111: 1.3871515989303589 / BPR loss: 1.3861652612686157 / Matching loss: 3.1962437788024545e-05 / Item reconstruction: 0.0009502962930127978 / Text reconstruction: 0.00239607784897089\n",
            "loss in epoch 5/11 iteration 100/111: 1.3871773481369019 / BPR loss: 1.3861894607543945 / Matching loss: 3.100361209362745e-05 / Item reconstruction: 0.0009613041183911264 / Text reconstruction: 0.002381260273978114\n",
            "loss in epoch 5/11 iteration 101/111: 1.3871430158615112 / BPR loss: 1.386184811592102 / Matching loss: 2.9397620892268606e-05 / Item reconstruction: 0.000912674586288631 / Text reconstruction: 0.002362278290092945\n",
            "loss in epoch 5/11 iteration 102/111: 1.3871978521347046 / BPR loss: 1.3862202167510986 / Matching loss: 3.293177724117413e-05 / Item reconstruction: 0.0009650749852880836 / Text reconstruction: 0.0023110555484890938\n",
            "loss in epoch 5/11 iteration 103/111: 1.3872841596603394 / BPR loss: 1.386314868927002 / Matching loss: 3.161734275636263e-05 / Item reconstruction: 0.0009382708230987191 / Text reconstruction: 0.002343263477087021\n",
            "loss in epoch 5/11 iteration 104/111: 1.3872123956680298 / BPR loss: 1.386260986328125 / Matching loss: 3.1891322578303516e-05 / Item reconstruction: 0.0009165608789771795 / Text reconstruction: 0.0023059886880218983\n",
            "loss in epoch 5/11 iteration 105/111: 1.3871310949325562 / BPR loss: 1.3862007856369019 / Matching loss: 3.088571975240484e-05 / Item reconstruction: 0.0008958575781434774 / Text reconstruction: 0.0022577447816729546\n",
            "loss in epoch 5/11 iteration 106/111: 1.3871548175811768 / BPR loss: 1.3862004280090332 / Matching loss: 3.2017152989283204e-05 / Item reconstruction: 0.0009447833290323615 / Text reconstruction: 0.0022495859302580357\n",
            "loss in epoch 5/11 iteration 107/111: 1.387097954750061 / BPR loss: 1.3861806392669678 / Matching loss: 2.9238675779197365e-05 / Item reconstruction: 0.0008900186512619257 / Text reconstruction: 0.002215418964624405\n",
            "loss in epoch 5/11 iteration 108/111: 1.3871434926986694 / BPR loss: 1.3862025737762451 / Matching loss: 3.059064329136163e-05 / Item reconstruction: 0.0009417763212695718 / Text reconstruction: 0.0021970150992274284\n",
            "loss in epoch 5/11 iteration 109/111: 1.3872531652450562 / BPR loss: 1.3863253593444824 / Matching loss: 2.9669079594896175e-05 / Item reconstruction: 0.0009133054409176111 / Text reconstruction: 0.0022069935221225023\n",
            "loss in epoch 5/11 iteration 110/111: 1.3871656656265259 / BPR loss: 1.3862431049346924 / Matching loss: 2.6453590180608444e-05 / Item reconstruction: 0.0008333385339938104 / Text reconstruction: 0.002397538162767887\n",
            "loss in epoch 5/11 iteration 111/111: 1.3871599435806274 / BPR loss: 1.3862924575805664 / Matching loss: 2.5560679205227643e-05 / Item reconstruction: 0.0006922881584614515 / Text reconstruction: 0.002479164395481348\n",
            " 50% 5/10 [26:50<26:46, 321.31s/it]loss in epoch 6/11 iteration 0/111: 1.3871240615844727 / BPR loss: 1.386205792427063 / Matching loss: 3.326527075842023e-05 / Item reconstruction: 0.0009364065481349826 / Text reconstruction: 0.0020838899072259665\n",
            "loss in epoch 6/11 iteration 1/111: 1.3870905637741089 / BPR loss: 1.3861637115478516 / Matching loss: 3.135009319521487e-05 / Item reconstruction: 0.000952661270275712 / Text reconstruction: 0.0020955668296664953\n",
            "loss in epoch 6/11 iteration 2/111: 1.3870550394058228 / BPR loss: 1.3861100673675537 / Matching loss: 3.1492447305936366e-05 / Item reconstruction: 0.0009889977518469095 / Text reconstruction: 0.0020953472703695297\n",
            "loss in epoch 6/11 iteration 3/111: 1.3871170282363892 / BPR loss: 1.3861819505691528 / Matching loss: 3.002337871294003e-05 / Item reconstruction: 0.0009760512039065361 / Text reconstruction: 0.0020848780404776335\n",
            "loss in epoch 6/11 iteration 4/111: 1.3871632814407349 / BPR loss: 1.3862214088439941 / Matching loss: 3.052211832255125e-05 / Item reconstruction: 0.0010084384121000767 / Text reconstruction: 0.0020353002473711967\n",
            "loss in epoch 6/11 iteration 5/111: 1.387153148651123 / BPR loss: 1.386254072189331 / Matching loss: 2.9562597774202004e-05 / Item reconstruction: 0.0009186773677356541 / Text reconstruction: 0.0020508114248514175\n",
            "loss in epoch 6/11 iteration 6/111: 1.3871524333953857 / BPR loss: 1.3862566947937012 / Matching loss: 3.227453998988494e-05 / Item reconstruction: 0.0009510546224191785 / Text reconstruction: 0.0019398329313844442\n",
            "loss in epoch 6/11 iteration 7/111: 1.387165904045105 / BPR loss: 1.386277675628662 / Matching loss: 3.3226860978174955e-05 / Item reconstruction: 0.0009449384524486959 / Text reconstruction: 0.001912430627271533\n",
            "loss in epoch 6/11 iteration 8/111: 1.387120008468628 / BPR loss: 1.3862252235412598 / Matching loss: 3.114723222097382e-05 / Item reconstruction: 0.0009351872140541673 / Text reconstruction: 0.0019808453507721424\n",
            "loss in epoch 6/11 iteration 9/111: 1.387115716934204 / BPR loss: 1.3861455917358398 / Matching loss: 2.8750095225404948e-05 / Item reconstruction: 0.0010451825801283121 / Text reconstruction: 0.002093688817694783\n",
            "loss in epoch 6/11 iteration 10/111: 1.3866232633590698 / BPR loss: 1.3854451179504395 / Matching loss: 2.3479591618524864e-05 / Item reconstruction: 0.0013233122881501913 / Text reconstruction: 0.0024651603307574987\n",
            "loss in epoch 6/11 iteration 11/111: 1.3865737915039062 / BPR loss: 1.3854891061782837 / Matching loss: 2.4859098630258814e-05 / Item reconstruction: 0.0012082771863788366 / Text reconstruction: 0.0022783740423619747\n",
            "loss in epoch 6/11 iteration 12/111: 1.3865357637405396 / BPR loss: 1.3854960203170776 / Matching loss: 2.6595027520670556e-05 / Item reconstruction: 0.0011406218400225043 / Text reconstruction: 0.0022141439840197563\n",
            "loss in epoch 6/11 iteration 13/111: 1.3863632678985596 / BPR loss: 1.3853986263275146 / Matching loss: 2.5812158128246665e-05 / Item reconstruction: 0.0010028209071606398 / Text reconstruction: 0.0021867165341973305\n",
            "loss in epoch 6/11 iteration 14/111: 1.386502742767334 / BPR loss: 1.3855727910995483 / Matching loss: 2.694470458664e-05 / Item reconstruction: 0.0009918418945744634 / Text reconstruction: 0.002035352401435375\n",
            "loss in epoch 6/11 iteration 15/111: 1.386420726776123 / BPR loss: 1.3855063915252686 / Matching loss: 2.335258795937989e-05 / Item reconstruction: 0.0009743829723447561 / Text reconstruction: 0.0020185934845358133\n",
            "loss in epoch 6/11 iteration 16/111: 1.3868447542190552 / BPR loss: 1.3858287334442139 / Matching loss: 2.5172786990879104e-05 / Item reconstruction: 0.0011480438988655806 / Text reconstruction: 0.0020845108665525913\n",
            "loss in epoch 6/11 iteration 17/111: 1.3874815702438354 / BPR loss: 1.3861908912658691 / Matching loss: 3.020372059836518e-05 / Item reconstruction: 0.0016104911919683218 / Text reconstruction: 0.0022764981258660555\n",
            "loss in epoch 6/11 iteration 18/111: 1.3874422311782837 / BPR loss: 1.3861808776855469 / Matching loss: 2.8330015993560664e-05 / Item reconstruction: 0.0015836259117349982 / Text reconstruction: 0.002206250559538603\n",
            "loss in epoch 6/11 iteration 19/111: 1.3873307704925537 / BPR loss: 1.3861503601074219 / Matching loss: 2.898147613450419e-05 / Item reconstruction: 0.0014618632849305868 / Text reconstruction: 0.0021031079813838005\n",
            "loss in epoch 6/11 iteration 20/111: 1.3873114585876465 / BPR loss: 1.386242389678955 / Matching loss: 3.13648342853412e-05 / Item reconstruction: 0.0013142727548256516 / Text reconstruction: 0.001902945339679718\n",
            "loss in epoch 6/11 iteration 21/111: 1.3872312307357788 / BPR loss: 1.3862767219543457 / Matching loss: 2.7854377549374476e-05 / Item reconstruction: 0.001128921052441001 / Text reconstruction: 0.001810709247365594\n",
            "loss in epoch 6/11 iteration 22/111: 1.3871413469314575 / BPR loss: 1.3862278461456299 / Matching loss: 3.161247514071874e-05 / Item reconstruction: 0.0010853323619812727 / Text reconstruction: 0.0016964019741863012\n",
            "loss in epoch 6/11 iteration 23/111: 1.3870792388916016 / BPR loss: 1.3862131834030151 / Matching loss: 2.8217778890393674e-05 / Item reconstruction: 0.0010162641992792487 / Text reconstruction: 0.0016479592304676771\n",
            "loss in epoch 6/11 iteration 24/111: 1.3870458602905273 / BPR loss: 1.386195182800293 / Matching loss: 2.938944635388907e-05 / Item reconstruction: 0.0009899984579533339 / Text reconstruction: 0.0016311680665239692\n",
            "loss in epoch 6/11 iteration 25/111: 1.3870575428009033 / BPR loss: 1.386213779449463 / Matching loss: 2.9260083465487696e-05 / Item reconstruction: 0.0009812144562602043 / Text reconstruction: 0.0016195281641557813\n",
            "loss in epoch 6/11 iteration 26/111: 1.3870644569396973 / BPR loss: 1.3861908912658691 / Matching loss: 3.0044328013900667e-05 / Item reconstruction: 0.0010421384358778596 / Text reconstruction: 0.0016124045941978693\n",
            "loss in epoch 6/11 iteration 27/111: 1.3870387077331543 / BPR loss: 1.3861734867095947 / Matching loss: 2.781060538836755e-05 / Item reconstruction: 0.0010260115377604961 / Text reconstruction: 0.001622397918254137\n",
            "loss in epoch 6/11 iteration 28/111: 1.3870707750320435 / BPR loss: 1.386204719543457 / Matching loss: 2.7477221010485664e-05 / Item reconstruction: 0.0010245488956570625 / Text reconstruction: 0.001631914870813489\n",
            "loss in epoch 6/11 iteration 29/111: 1.3871722221374512 / BPR loss: 1.3862651586532593 / Matching loss: 3.0765811970923096e-05 / Item reconstruction: 0.0011310428380966187 / Text reconstruction: 0.0015536213759332895\n",
            "loss in epoch 6/11 iteration 30/111: 1.3871797323226929 / BPR loss: 1.3862571716308594 / Matching loss: 3.366657983860932e-05 / Item reconstruction: 0.001161069143563509 / Text reconstruction: 0.0015420201234519482\n",
            "loss in epoch 6/11 iteration 31/111: 1.3871784210205078 / BPR loss: 1.386297583580017 / Matching loss: 2.9538898161263205e-05 / Item reconstruction: 0.001104047056287527 / Text reconstruction: 0.0014962806599214673\n",
            "loss in epoch 6/11 iteration 32/111: 1.3869845867156982 / BPR loss: 1.3861448764801025 / Matching loss: 2.7771395252784714e-05 / Item reconstruction: 0.0010285940952599049 / Text reconstruction: 0.0014883491676300764\n",
            "loss in epoch 6/11 iteration 33/111: 1.3869798183441162 / BPR loss: 1.3861454725265503 / Matching loss: 2.8106254831072874e-05 / Item reconstruction: 0.0010271456558257341 / Text reconstruction: 0.0014631194062530994\n",
            "loss in epoch 6/11 iteration 34/111: 1.3869351148605347 / BPR loss: 1.386113166809082 / Matching loss: 2.9392660508165136e-05 / Item reconstruction: 0.001023801858536899 / Text reconstruction: 0.0014033333864063025\n",
            "loss in epoch 6/11 iteration 35/111: 1.3869779109954834 / BPR loss: 1.3861873149871826 / Matching loss: 2.8753742299159057e-05 / Item reconstruction: 0.0009757234947755933 / Text reconstruction: 0.0013700324343517423\n",
            "loss in epoch 6/11 iteration 36/111: 1.3870207071304321 / BPR loss: 1.3862444162368774 / Matching loss: 2.845341805368662e-05 / Item reconstruction: 0.0009349096799269319 / Text reconstruction: 0.0014018416404724121\n",
            "loss in epoch 6/11 iteration 37/111: 1.3869519233703613 / BPR loss: 1.3861851692199707 / Matching loss: 2.7831825718749315e-05 / Item reconstruction: 0.0009313143091276288 / Text reconstruction: 0.0013667733874171972\n",
            "loss in epoch 6/11 iteration 38/111: 1.3869843482971191 / BPR loss: 1.3861820697784424 / Matching loss: 2.9728422305197455e-05 / Item reconstruction: 0.00100994692184031 / Text reconstruction: 0.001338377594947815\n",
            "loss in epoch 6/11 iteration 39/111: 1.3870408535003662 / BPR loss: 1.3862866163253784 / Matching loss: 2.713799403863959e-05 / Item reconstruction: 0.0009327561128884554 / Text reconstruction: 0.0013034066651016474\n",
            "loss in epoch 6/11 iteration 40/111: 1.3869071006774902 / BPR loss: 1.3861486911773682 / Matching loss: 2.732273787842132e-05 / Item reconstruction: 0.0009521113825030625 / Text reconstruction: 0.0012752996990457177\n",
            "loss in epoch 6/11 iteration 41/111: 1.3869879245758057 / BPR loss: 1.3862229585647583 / Matching loss: 2.6960287868860178e-05 / Item reconstruction: 0.0009704503463581204 / Text reconstruction: 0.0012639244087040424\n",
            "loss in epoch 6/11 iteration 42/111: 1.386939287185669 / BPR loss: 1.3861637115478516 / Matching loss: 2.7006855816580355e-05 / Item reconstruction: 0.0009834455559030175 / Text reconstruction: 0.0012837370159104466\n",
            "loss in epoch 6/11 iteration 43/111: 1.3869792222976685 / BPR loss: 1.386213779449463 / Matching loss: 2.7812337066279724e-05 / Item reconstruction: 0.0009782833512872458 / Text reconstruction: 0.0012430081842467189\n",
            "loss in epoch 6/11 iteration 44/111: 1.38702392578125 / BPR loss: 1.3862779140472412 / Matching loss: 2.7235386369284242e-05 / Item reconstruction: 0.0009491769596934319 / Text reconstruction: 0.001221507671289146\n",
            "loss in epoch 6/11 iteration 45/111: 1.3869856595993042 / BPR loss: 1.3862512111663818 / Matching loss: 2.600566040200647e-05 / Item reconstruction: 0.0009327124571427703 / Text reconstruction: 0.0012106380891054869\n",
            "loss in epoch 6/11 iteration 46/111: 1.3869153261184692 / BPR loss: 1.3861802816390991 / Matching loss: 2.7365558707970195e-05 / Item reconstruction: 0.0009462482994422317 / Text reconstruction: 0.0011723439674824476\n",
            "loss in epoch 6/11 iteration 47/111: 1.3867599964141846 / BPR loss: 1.3860293626785278 / Matching loss: 2.851115641533397e-05 / Item reconstruction: 0.0009428513003513217 / Text reconstruction: 0.0011534136720001698\n",
            "loss in epoch 6/11 iteration 48/111: 1.3868969678878784 / BPR loss: 1.3861254453659058 / Matching loss: 2.958758341264911e-05 / Item reconstruction: 0.0010302409064024687 / Text reconstruction: 0.0011340572964400053\n",
            "loss in epoch 6/11 iteration 49/111: 1.3869277238845825 / BPR loss: 1.386204481124878 / Matching loss: 2.6865167455980554e-05 / Item reconstruction: 0.0009461259469389915 / Text reconstruction: 0.0011171151418238878\n",
            "loss in epoch 6/11 iteration 50/111: 1.3869075775146484 / BPR loss: 1.3861768245697021 / Matching loss: 2.6662013624445535e-05 / Item reconstruction: 0.0009639865602366626 / Text reconstruction: 0.0011105767916887999\n",
            "loss in epoch 6/11 iteration 51/111: 1.3869507312774658 / BPR loss: 1.3862228393554688 / Matching loss: 2.602680069685448e-05 / Item reconstruction: 0.0009642986115068197 / Text reconstruction: 0.001098256791010499\n",
            "loss in epoch 6/11 iteration 52/111: 1.3868037462234497 / BPR loss: 1.386073112487793 / Matching loss: 2.6966141376760788e-05 / Item reconstruction: 0.0009784265421330929 / Text reconstruction: 0.0010723115410655737\n",
            "loss in epoch 6/11 iteration 53/111: 1.3867884874343872 / BPR loss: 1.386093258857727 / Matching loss: 2.648570080054924e-05 / Item reconstruction: 0.0009184276568703353 / Text reconstruction: 0.0010477007599547505\n",
            "loss in epoch 6/11 iteration 54/111: 1.3868402242660522 / BPR loss: 1.3861181735992432 / Matching loss: 2.7074416721006855e-05 / Item reconstruction: 0.0009759450331330299 / Text reconstruction: 0.0010355620179325342\n",
            "loss in epoch 6/11 iteration 55/111: 1.386742353439331 / BPR loss: 1.3860528469085693 / Matching loss: 2.713314097491093e-05 / Item reconstruction: 0.0009149059187620878 / Text reconstruction: 0.0010246855672448874\n",
            "loss in epoch 6/11 iteration 56/111: 1.38690984249115 / BPR loss: 1.3861901760101318 / Matching loss: 2.9335660656215623e-05 / Item reconstruction: 0.0009732842445373535 / Text reconstruction: 0.0010184752754867077\n",
            "loss in epoch 6/11 iteration 57/111: 1.3869991302490234 / BPR loss: 1.3862966299057007 / Matching loss: 2.6029425498563796e-05 / Item reconstruction: 0.0009409307385794818 / Text reconstruction: 0.00103019829839468\n",
            "loss in epoch 6/11 iteration 58/111: 1.3867406845092773 / BPR loss: 1.3860414028167725 / Matching loss: 2.5385759727214463e-05 / Item reconstruction: 0.0009367410093545914 / Text reconstruction: 0.0010273954831063747\n",
            "loss in epoch 6/11 iteration 59/111: 1.3868324756622314 / BPR loss: 1.3861478567123413 / Matching loss: 2.5483315766905434e-05 / Item reconstruction: 0.0009180671768262982 / Text reconstruction: 0.0009998856112360954\n",
            "loss in epoch 6/11 iteration 60/111: 1.3868318796157837 / BPR loss: 1.3861465454101562 / Matching loss: 2.7070580472354777e-05 / Item reconstruction: 0.0009370242478325963 / Text reconstruction: 0.0009491043165326118\n",
            "loss in epoch 6/11 iteration 61/111: 1.3868972063064575 / BPR loss: 1.3861925601959229 / Matching loss: 2.8362153898342513e-05 / Item reconstruction: 0.0009726070566102862 / Text reconstruction: 0.0009501352906227112\n",
            "loss in epoch 6/11 iteration 62/111: 1.3869249820709229 / BPR loss: 1.3862327337265015 / Matching loss: 2.7935506295762025e-05 / Item reconstruction: 0.0009565812069922686 / Text reconstruction: 0.0009306649444624782\n",
            "loss in epoch 6/11 iteration 63/111: 1.3868085145950317 / BPR loss: 1.3861439228057861 / Matching loss: 2.628020229167305e-05 / Item reconstruction: 0.0009172436548396945 / Text reconstruction: 0.0008989072521217167\n",
            "loss in epoch 6/11 iteration 64/111: 1.3868515491485596 / BPR loss: 1.3861836194992065 / Matching loss: 2.5450208340771496e-05 / Item reconstruction: 0.0009072895627468824 / Text reconstruction: 0.0009445424657315016\n",
            "loss in epoch 6/11 iteration 65/111: 1.3868381977081299 / BPR loss: 1.3861663341522217 / Matching loss: 2.5639399609644897e-05 / Item reconstruction: 0.000937373552005738 / Text reconstruction: 0.000887613685335964\n",
            "loss in epoch 6/11 iteration 66/111: 1.3868420124053955 / BPR loss: 1.3861751556396484 / Matching loss: 2.447973020025529e-05 / Item reconstruction: 0.0009335937211290002 / Text reconstruction: 0.0008778452756814659\n",
            "loss in epoch 6/11 iteration 67/111: 1.3867896795272827 / BPR loss: 1.386112928390503 / Matching loss: 2.632821269799024e-05 / Item reconstruction: 0.0009502905886620283 / Text reconstruction: 0.0008763489313423634\n",
            "loss in epoch 6/11 iteration 68/111: 1.386836051940918 / BPR loss: 1.38619065284729 / Matching loss: 2.6870980946114287e-05 / Item reconstruction: 0.0009027635678648949 / Text reconstruction: 0.0008362020016647875\n",
            "loss in epoch 6/11 iteration 69/111: 1.3867319822311401 / BPR loss: 1.3860973119735718 / Matching loss: 2.6136189262615517e-05 / Item reconstruction: 0.0008877454092726111 / Text reconstruction: 0.0008238668669946492\n",
            "loss in epoch 6/11 iteration 70/111: 1.3867863416671753 / BPR loss: 1.3861289024353027 / Matching loss: 2.6819423510460183e-05 / Item reconstruction: 0.0009263065876439214 / Text reconstruction: 0.0008374474127776921\n",
            "loss in epoch 6/11 iteration 71/111: 1.3868920803070068 / BPR loss: 1.3862279653549194 / Matching loss: 2.5319965061498806e-05 / Item reconstruction: 0.0009318358497694135 / Text reconstruction: 0.000865123700350523\n",
            "loss in epoch 6/11 iteration 72/111: 1.3867738246917725 / BPR loss: 1.3861348628997803 / Matching loss: 2.4664677766850218e-05 / Item reconstruction: 0.0009128468809649348 / Text reconstruction: 0.0007894069422036409\n",
            "loss in epoch 6/11 iteration 73/111: 1.3867148160934448 / BPR loss: 1.3861010074615479 / Matching loss: 2.5181745513691567e-05 / Item reconstruction: 0.0008624798501841724 / Text reconstruction: 0.0007867980748414993\n",
            "loss in epoch 6/11 iteration 74/111: 1.3867547512054443 / BPR loss: 1.3860900402069092 / Matching loss: 2.6461091692908667e-05 / Item reconstruction: 0.0009631973807699978 / Text reconstruction: 0.0007831793045625091\n",
            "loss in epoch 6/11 iteration 75/111: 1.3868165016174316 / BPR loss: 1.3861727714538574 / Matching loss: 2.6455145416548476e-05 / Item reconstruction: 0.0009327539009973407 / Text reconstruction: 0.0007547478890046477\n",
            "loss in epoch 6/11 iteration 76/111: 1.386894702911377 / BPR loss: 1.386230230331421 / Matching loss: 2.756600224529393e-05 / Item reconstruction: 0.0009763269335962832 / Text reconstruction: 0.0007440963527187705\n",
            "loss in epoch 6/11 iteration 77/111: 1.386841058731079 / BPR loss: 1.3862059116363525 / Matching loss: 2.4644363293191418e-05 / Item reconstruction: 0.0009241125080734491 / Text reconstruction: 0.0007419240428134799\n",
            "loss in epoch 6/11 iteration 78/111: 1.3867627382278442 / BPR loss: 1.3861229419708252 / Matching loss: 2.5428518711123616e-05 / Item reconstruction: 0.0009358305251225829 / Text reconstruction: 0.0007324115722440183\n",
            "loss in epoch 6/11 iteration 79/111: 1.3867692947387695 / BPR loss: 1.386136770248413 / Matching loss: 2.4599648895673454e-05 / Item reconstruction: 0.000922313192859292 / Text reconstruction: 0.0007342207827605307\n",
            "loss in epoch 6/11 iteration 80/111: 1.3868272304534912 / BPR loss: 1.38621187210083 / Matching loss: 2.5304621885879897e-05 / Item reconstruction: 0.0009034727700054646 / Text reconstruction: 0.0006918164435774088\n",
            "loss in epoch 6/11 iteration 81/111: 1.38667631149292 / BPR loss: 1.3860334157943726 / Matching loss: 2.6828824047697708e-05 / Item reconstruction: 0.0009492533281445503 / Text reconstruction: 0.0007072581793181598\n",
            "loss in epoch 6/11 iteration 82/111: 1.3867664337158203 / BPR loss: 1.386132001876831 / Matching loss: 2.5328214178443886e-05 / Item reconstruction: 0.0009379100520163774 / Text reconstruction: 0.0007006659870967269\n",
            "loss in epoch 6/11 iteration 83/111: 1.386936902999878 / BPR loss: 1.3863070011138916 / Matching loss: 2.5488676328677684e-05 / Item reconstruction: 0.0009444896131753922 / Text reconstruction: 0.0006612250581383705\n",
            "loss in epoch 6/11 iteration 84/111: 1.3868074417114258 / BPR loss: 1.3862086534500122 / Matching loss: 2.321570718777366e-05 / Item reconstruction: 0.0008750755805522203 / Text reconstruction: 0.0006902001914568245\n",
            "loss in epoch 6/11 iteration 85/111: 1.3868409395217896 / BPR loss: 1.386206865310669 / Matching loss: 2.6065399651997723e-05 / Item reconstruction: 0.0009625609964132309 / Text reconstruction: 0.000633654126431793\n",
            "loss in epoch 6/11 iteration 86/111: 1.3866311311721802 / BPR loss: 1.386017084121704 / Matching loss: 2.60986817011144e-05 / Item reconstruction: 0.0009302638936787844 / Text reconstruction: 0.0006139678880572319\n",
            "loss in epoch 6/11 iteration 87/111: 1.3867861032485962 / BPR loss: 1.3861808776855469 / Matching loss: 2.5957084289984778e-05 / Item reconstruction: 0.0009149030083790421 / Text reconstruction: 0.0006092713447287679\n",
            "loss in epoch 6/11 iteration 88/111: 1.3867855072021484 / BPR loss: 1.386173129081726 / Matching loss: 2.4494711396982893e-05 / Item reconstruction: 0.0009079208830371499 / Text reconstruction: 0.0006696723285131156\n",
            "loss in epoch 6/11 iteration 89/111: 1.3868571519851685 / BPR loss: 1.3862502574920654 / Matching loss: 2.3255672203958966e-05 / Item reconstruction: 0.0009269915753975511 / Text reconstruction: 0.0006007559131830931\n",
            "loss in epoch 6/11 iteration 90/111: 1.3867708444595337 / BPR loss: 1.386139154434204 / Matching loss: 2.5027056835824624e-05 / Item reconstruction: 0.0009690305450931191 / Text reconstruction: 0.0006108326488174498\n",
            "loss in epoch 6/11 iteration 91/111: 1.3867813348770142 / BPR loss: 1.3861840963363647 / Matching loss: 2.496759952919092e-05 / Item reconstruction: 0.0009136139415204525 / Text reconstruction: 0.0005777573678642511\n",
            "loss in epoch 6/11 iteration 92/111: 1.3866995573043823 / BPR loss: 1.3861099481582642 / Matching loss: 2.4484725145157427e-05 / Item reconstruction: 0.0009065168560482562 / Text reconstruction: 0.000559954671189189\n",
            "loss in epoch 6/11 iteration 93/111: 1.3867889642715454 / BPR loss: 1.3861560821533203 / Matching loss: 2.6505915229790844e-05 / Item reconstruction: 0.0009900960139930248 / Text reconstruction: 0.0005564207676798105\n",
            "loss in epoch 6/11 iteration 94/111: 1.3868614435195923 / BPR loss: 1.3862601518630981 / Matching loss: 2.5115368771366775e-05 / Item reconstruction: 0.0009297116775996983 / Text reconstruction: 0.0005568871274590492\n",
            "loss in epoch 6/11 iteration 95/111: 1.3866934776306152 / BPR loss: 1.3860851526260376 / Matching loss: 2.3863807655288838e-05 / Item reconstruction: 0.0009386491728946567 / Text reconstruction: 0.0005758741172030568\n",
            "loss in epoch 6/11 iteration 96/111: 1.3867716789245605 / BPR loss: 1.386172890663147 / Matching loss: 2.423670593998395e-05 / Item reconstruction: 0.0009240268846042454 / Text reconstruction: 0.0005623931065201759\n",
            "loss in epoch 6/11 iteration 97/111: 1.386800765991211 / BPR loss: 1.3862111568450928 / Matching loss: 2.3113521820050664e-05 / Item reconstruction: 0.0008974691154435277 / Text reconstruction: 0.0005889111198484898\n",
            "loss in epoch 6/11 iteration 98/111: 1.3868637084960938 / BPR loss: 1.3862929344177246 / Matching loss: 2.310777927050367e-05 / Item reconstruction: 0.0008816446643322706 / Text reconstruction: 0.0005342698423191905\n",
            "loss in epoch 6/11 iteration 99/111: 1.386715054512024 / BPR loss: 1.3861148357391357 / Matching loss: 2.398237666056957e-05 / Item reconstruction: 0.0009508416405878961 / Text reconstruction: 0.0005043960991315544\n",
            "loss in epoch 6/11 iteration 100/111: 1.386802315711975 / BPR loss: 1.3861994743347168 / Matching loss: 2.5841152819339186e-05 / Item reconstruction: 0.0009550161194056273 / Text reconstruction: 0.0004970105946995318\n",
            "loss in epoch 6/11 iteration 101/111: 1.3867244720458984 / BPR loss: 1.3861337900161743 / Matching loss: 2.4051314539974555e-05 / Item reconstruction: 0.0009225017856806517 / Text reconstruction: 0.0005267939995974302\n",
            "loss in epoch 6/11 iteration 102/111: 1.3867113590240479 / BPR loss: 1.3861067295074463 / Matching loss: 2.4743158064666204e-05 / Item reconstruction: 0.0009590329136699438 / Text reconstruction: 0.0005017565563321114\n",
            "loss in epoch 6/11 iteration 103/111: 1.386791706085205 / BPR loss: 1.3861982822418213 / Matching loss: 2.3501081159338355e-05 / Item reconstruction: 0.0009189519914798439 / Text reconstruction: 0.0005526028107851744\n",
            "loss in epoch 6/11 iteration 104/111: 1.3867332935333252 / BPR loss: 1.3861665725708008 / Matching loss: 2.226080687250942e-05 / Item reconstruction: 0.0008804989047348499 / Text reconstruction: 0.0005209898808971047\n",
            "loss in epoch 6/11 iteration 105/111: 1.3867123126983643 / BPR loss: 1.3861435651779175 / Matching loss: 2.314639641554095e-05 / Item reconstruction: 0.0008967717876657844 / Text reconstruction: 0.0004862145578954369\n",
            "loss in epoch 6/11 iteration 106/111: 1.386789083480835 / BPR loss: 1.3862073421478271 / Matching loss: 2.447698534524534e-05 / Item reconstruction: 0.0009197618928737938 / Text reconstruction: 0.00048715248703956604\n",
            "loss in epoch 6/11 iteration 107/111: 1.3867456912994385 / BPR loss: 1.3861937522888184 / Matching loss: 2.2738306142855436e-05 / Item reconstruction: 0.0008749215630814433 / Text reconstruction: 0.00045854103518649936\n",
            "loss in epoch 6/11 iteration 108/111: 1.3867852687835693 / BPR loss: 1.3862196207046509 / Matching loss: 2.4347909857169725e-05 / Item reconstruction: 0.000900611630640924 / Text reconstruction: 0.0004552377504296601\n",
            "loss in epoch 6/11 iteration 109/111: 1.3869589567184448 / BPR loss: 1.3863866329193115 / Matching loss: 2.28737171710236e-05 / Item reconstruction: 0.0009042589226737618 / Text reconstruction: 0.00048632087418809533\n",
            "loss in epoch 6/11 iteration 110/111: 1.3868390321731567 / BPR loss: 1.3862957954406738 / Matching loss: 1.7908376321429387e-05 / Item reconstruction: 0.0007894102018326521 / Text reconstruction: 0.0006533082341775298\n",
            "loss in epoch 6/11 iteration 111/111: 1.38681960105896 / BPR loss: 1.3863004446029663 / Matching loss: 1.6654041246511042e-05 / Item reconstruction: 0.0007052357541397214 / Text reconstruction: 0.0007490245625376701\n",
            " 60% 6/10 [32:10<21:23, 320.76s/it]loss in epoch 7/11 iteration 0/111: 1.3867698907852173 / BPR loss: 1.386186122894287 / Matching loss: 2.4385743017774075e-05 / Item reconstruction: 0.0009576261509209871 / Text reconstruction: 0.000402210425818339\n",
            "loss in epoch 7/11 iteration 1/111: 1.3866946697235107 / BPR loss: 1.386096715927124 / Matching loss: 2.3525868527940474e-05 / Item reconstruction: 0.0009706940036267042 / Text reconstruction: 0.00044572007027454674\n",
            "loss in epoch 7/11 iteration 2/111: 1.3866724967956543 / BPR loss: 1.3860821723937988 / Matching loss: 2.370201764279045e-05 / Item reconstruction: 0.0009542774641886353 / Text reconstruction: 0.0004468118422664702\n",
            "loss in epoch 7/11 iteration 3/111: 1.3866949081420898 / BPR loss: 1.3860890865325928 / Matching loss: 2.3954575226525776e-05 / Item reconstruction: 0.0009882468730211258 / Text reconstruction: 0.00043864158215001225\n",
            "loss in epoch 7/11 iteration 4/111: 1.386731505393982 / BPR loss: 1.386143445968628 / Matching loss: 2.3095642973203212e-05 / Item reconstruction: 0.000966403167694807 / Text reconstruction: 0.00040914263809099793\n",
            "loss in epoch 7/11 iteration 5/111: 1.3867615461349487 / BPR loss: 1.3861901760101318 / Matching loss: 2.29532815865241e-05 / Item reconstruction: 0.0009235197212547064 / Text reconstruction: 0.0004329587391112\n",
            "loss in epoch 7/11 iteration 6/111: 1.386936068534851 / BPR loss: 1.3863599300384521 / Matching loss: 2.4765926355030388e-05 / Item reconstruction: 0.0009648131090216339 / Text reconstruction: 0.000344352622050792\n",
            "loss in epoch 7/11 iteration 7/111: 1.3868237733840942 / BPR loss: 1.386251449584961 / Matching loss: 2.4884035155992024e-05 / Item reconstruction: 0.0009616967290639877 / Text reconstruction: 0.0003328163002151996\n",
            "loss in epoch 7/11 iteration 8/111: 1.3867932558059692 / BPR loss: 1.3862297534942627 / Matching loss: 2.266474621137604e-05 / Item reconstruction: 0.0009122507181018591 / Text reconstruction: 0.0004239652189426124\n",
            "loss in epoch 7/11 iteration 9/111: 1.3866350650787354 / BPR loss: 1.385986566543579 / Matching loss: 2.1259693312458694e-05 / Item reconstruction: 0.0010221259435638785 / Text reconstruction: 0.0005812522722408175\n",
            "loss in epoch 7/11 iteration 10/111: 1.386301040649414 / BPR loss: 1.3853861093521118 / Matching loss: 1.9454415451036766e-05 / Item reconstruction: 0.0013370160013437271 / Text reconstruction: 0.0011350063141435385\n",
            "loss in epoch 7/11 iteration 11/111: 1.3861913681030273 / BPR loss: 1.3854084014892578 / Matching loss: 1.844939833972603e-05 / Item reconstruction: 0.0011901933467015624 / Text reconstruction: 0.0008470600587315857\n",
            "loss in epoch 7/11 iteration 12/111: 1.3860349655151367 / BPR loss: 1.385313868522644 / Matching loss: 1.780418460839428e-05 / Item reconstruction: 0.001099680084735155 / Text reconstruction: 0.0007675306987948716\n",
            "loss in epoch 7/11 iteration 13/111: 1.3859508037567139 / BPR loss: 1.385292649269104 / Matching loss: 1.6853835404617712e-05 / Item reconstruction: 0.0009884274331852794 / Text reconstruction: 0.0007356500136666\n",
            "loss in epoch 7/11 iteration 14/111: 1.386109709739685 / BPR loss: 1.3854984045028687 / Matching loss: 1.8950509911519475e-05 / Item reconstruction: 0.0009411069331690669 / Text reconstruction: 0.0006092346739023924\n",
            "loss in epoch 7/11 iteration 15/111: 1.3859039545059204 / BPR loss: 1.3852978944778442 / Matching loss: 1.6927800970734097e-05 / Item reconstruction: 0.0009316666983067989 / Text reconstruction: 0.0006162500358186662\n",
            "loss in epoch 7/11 iteration 16/111: 1.3864432573318481 / BPR loss: 1.3857252597808838 / Matching loss: 1.921466719068121e-05 / Item reconstruction: 0.0011230290401726961 / Text reconstruction: 0.0006867638439871371\n",
            "loss in epoch 7/11 iteration 17/111: 1.3871883153915405 / BPR loss: 1.3861865997314453 / Matching loss: 2.4157769075827673e-05 / Item reconstruction: 0.0015950155211612582 / Text reconstruction: 0.0009002250153571367\n",
            "loss in epoch 7/11 iteration 18/111: 1.3872616291046143 / BPR loss: 1.3862696886062622 / Matching loss: 2.4005692466744222e-05 / Item reconstruction: 0.0015954263508319855 / Text reconstruction: 0.0008510187617503107\n",
            "loss in epoch 7/11 iteration 19/111: 1.3872551918029785 / BPR loss: 1.3863625526428223 / Matching loss: 2.4314713300555013e-05 / Item reconstruction: 0.001438803388737142 / Text reconstruction: 0.0007444668444804847\n",
            "loss in epoch 7/11 iteration 20/111: 1.3870165348052979 / BPR loss: 1.3862122297286987 / Matching loss: 2.546839095884934e-05 / Item reconstruction: 0.0013311917427927256 / Text reconstruction: 0.0005661962786689401\n",
            "loss in epoch 7/11 iteration 21/111: 1.3868969678878784 / BPR loss: 1.3862053155899048 / Matching loss: 2.2927230020286515e-05 / Item reconstruction: 0.0011336621828377247 / Text reconstruction: 0.0005099008558318019\n",
            "loss in epoch 7/11 iteration 22/111: 1.3868165016174316 / BPR loss: 1.386185646057129 / Matching loss: 2.415683957224246e-05 / Item reconstruction: 0.0010561266681179404 / Text reconstruction: 0.0003926983044948429\n",
            "loss in epoch 7/11 iteration 23/111: 1.3867861032485962 / BPR loss: 1.386204481124878 / Matching loss: 2.181910167564638e-05 / Item reconstruction: 0.0009827970061451197 / Text reconstruction: 0.0003421297878958285\n",
            "loss in epoch 7/11 iteration 24/111: 1.3866978883743286 / BPR loss: 1.3861358165740967 / Matching loss: 2.2513795556733385e-05 / Item reconstruction: 0.0009440904250368476 / Text reconstruction: 0.0003374768130015582\n",
            "loss in epoch 7/11 iteration 25/111: 1.3867706060409546 / BPR loss: 1.386207103729248 / Matching loss: 2.341650179005228e-05 / Item reconstruction: 0.0009478508727625012 / Text reconstruction: 0.0003310551983304322\n",
            "loss in epoch 7/11 iteration 26/111: 1.3866819143295288 / BPR loss: 1.3860841989517212 / Matching loss: 2.428797415632289e-05 / Item reconstruction: 0.0010102544911205769 / Text reconstruction: 0.0003415264654904604\n",
            "loss in epoch 7/11 iteration 27/111: 1.3868067264556885 / BPR loss: 1.3861896991729736 / Matching loss: 2.2696898668073118e-05 / Item reconstruction: 0.0010500196367502213 / Text reconstruction: 0.00034697161754593253\n",
            "loss in epoch 7/11 iteration 28/111: 1.3868099451065063 / BPR loss: 1.3861799240112305 / Matching loss: 2.4005857994779944e-05 / Item reconstruction: 0.0010520024225115776 / Text reconstruction: 0.0004005769151262939\n",
            "loss in epoch 7/11 iteration 29/111: 1.3868696689605713 / BPR loss: 1.3862230777740479 / Matching loss: 2.4069346181931905e-05 / Item reconstruction: 0.0010993084870278835 / Text reconstruction: 0.00036400463432073593\n",
            "loss in epoch 7/11 iteration 30/111: 1.3868186473846436 / BPR loss: 1.3861753940582275 / Matching loss: 2.6939043891616166e-05 / Item reconstruction: 0.0011014726478606462 / Text reconstruction: 0.0003276767674833536\n",
            "loss in epoch 7/11 iteration 31/111: 1.386949896812439 / BPR loss: 1.3863286972045898 / Matching loss: 2.4322818717337213e-05 / Item reconstruction: 0.0010774210095405579 / Text reconstruction: 0.0002911397023126483\n",
            "loss in epoch 7/11 iteration 32/111: 1.386795163154602 / BPR loss: 1.3861932754516602 / Matching loss: 2.2736832761438563e-05 / Item reconstruction: 0.0010449392721056938 / Text reconstruction: 0.0002832441823557019\n",
            "loss in epoch 7/11 iteration 33/111: 1.3867056369781494 / BPR loss: 1.3861215114593506 / Matching loss: 2.2748739866074175e-05 / Item reconstruction: 0.001005771104246378 / Text reconstruction: 0.0002918566460721195\n",
            "loss in epoch 7/11 iteration 34/111: 1.3868170976638794 / BPR loss: 1.3862171173095703 / Matching loss: 2.4632030545035377e-05 / Item reconstruction: 0.0010427557863295078 / Text reconstruction: 0.0002695968432817608\n",
            "loss in epoch 7/11 iteration 35/111: 1.3867528438568115 / BPR loss: 1.3861896991729736 / Matching loss: 2.3672382667427883e-05 / Item reconstruction: 0.0009806856978684664 / Text reconstruction: 0.0002456298389006406\n",
            "loss in epoch 7/11 iteration 36/111: 1.3867430686950684 / BPR loss: 1.3861825466156006 / Matching loss: 2.39652072195895e-05 / Item reconstruction: 0.0009611485875211656 / Text reconstruction: 0.000279890897218138\n",
            "loss in epoch 7/11 iteration 37/111: 1.3867141008377075 / BPR loss: 1.3861606121063232 / Matching loss: 2.2668406018055975e-05 / Item reconstruction: 0.0009686568519100547 / Text reconstruction: 0.00023270744713954628\n",
            "loss in epoch 7/11 iteration 38/111: 1.3868074417114258 / BPR loss: 1.386263370513916 / Matching loss: 2.396065610810183e-05 / Item reconstruction: 0.000950432091485709 / Text reconstruction: 0.00022447931405622512\n",
            "loss in epoch 7/11 iteration 39/111: 1.3867688179016113 / BPR loss: 1.3862181901931763 / Matching loss: 2.329547714907676e-05 / Item reconstruction: 0.0009704594849608839 / Text reconstruction: 0.00021080259466543794\n",
            "loss in epoch 7/11 iteration 40/111: 1.386596441268921 / BPR loss: 1.386057734489441 / Matching loss: 2.2535870812134817e-05 / Item reconstruction: 0.0009531176183372736 / Text reconstruction: 0.00019787636119872332\n",
            "loss in epoch 7/11 iteration 41/111: 1.386667251586914 / BPR loss: 1.3861310482025146 / Matching loss: 2.3324519133893773e-05 / Item reconstruction: 0.0009467926574870944 / Text reconstruction: 0.000197340123122558\n",
            "loss in epoch 7/11 iteration 42/111: 1.3867682218551636 / BPR loss: 1.3862215280532837 / Matching loss: 2.2459269530372694e-05 / Item reconstruction: 0.000961966929025948 / Text reconstruction: 0.00021663311053998768\n",
            "loss in epoch 7/11 iteration 43/111: 1.3868473768234253 / BPR loss: 1.3863083124160767 / Matching loss: 2.3068318114383146e-05 / Item reconstruction: 0.0009518606821075082 / Text reconstruction: 0.00020054580818396062\n",
            "loss in epoch 7/11 iteration 44/111: 1.386770486831665 / BPR loss: 1.3862131834030151 / Matching loss: 2.3001352019491605e-05 / Item reconstruction: 0.000995480571873486 / Text reconstruction: 0.00018298400391358882\n",
            "loss in epoch 7/11 iteration 45/111: 1.38668692111969 / BPR loss: 1.386165976524353 / Matching loss: 2.1715517505072057e-05 / Item reconstruction: 0.000918801233638078 / Text reconstruction: 0.0001990749005926773\n",
            "loss in epoch 7/11 iteration 46/111: 1.3867007493972778 / BPR loss: 1.3861913681030273 / Matching loss: 2.2887767045176588e-05 / Item reconstruction: 0.0009016389376483858 / Text reconstruction: 0.0001780626189429313\n",
            "loss in epoch 7/11 iteration 47/111: 1.3865143060684204 / BPR loss: 1.3859983682632446 / Matching loss: 2.2640371753368527e-05 / Item reconstruction: 0.0009205370442941785 / Text reconstruction: 0.0001651267521083355\n",
            "loss in epoch 7/11 iteration 48/111: 1.386610507965088 / BPR loss: 1.3860595226287842 / Matching loss: 2.4909822968766093e-05 / Item reconstruction: 0.000986428465694189 / Text reconstruction: 0.00016474194126203656\n",
            "loss in epoch 7/11 iteration 49/111: 1.3865978717803955 / BPR loss: 1.3860489130020142 / Matching loss: 2.3897282517282292e-05 / Item reconstruction: 0.0009873652597889304 / Text reconstruction: 0.00015711922605987638\n",
            "loss in epoch 7/11 iteration 50/111: 1.386687994003296 / BPR loss: 1.3861308097839355 / Matching loss: 2.451614273013547e-05 / Item reconstruction: 0.0009990304242819548 / Text reconstruction: 0.00016559479990974069\n",
            "loss in epoch 7/11 iteration 51/111: 1.3867089748382568 / BPR loss: 1.3861753940582275 / Matching loss: 2.2554526367457584e-05 / Item reconstruction: 0.0009631083812564611 / Text reconstruction: 0.0001470028073526919\n",
            "loss in epoch 7/11 iteration 52/111: 1.3865926265716553 / BPR loss: 1.3860578536987305 / Matching loss: 2.3233540559886023e-05 / Item reconstruction: 0.0009656652109697461 / Text reconstruction: 0.00014387740520760417\n",
            "loss in epoch 7/11 iteration 53/111: 1.386562466621399 / BPR loss: 1.3860750198364258 / Matching loss: 2.2728221665602177e-05 / Item reconstruction: 0.000872300355695188 / Text reconstruction: 0.00014221762830857188\n",
            "loss in epoch 7/11 iteration 54/111: 1.3866608142852783 / BPR loss: 1.386117696762085 / Matching loss: 2.3522294213762507e-05 / Item reconstruction: 0.00098419061396271 / Text reconstruction: 0.00013798011059407145\n",
            "loss in epoch 7/11 iteration 55/111: 1.3865686655044556 / BPR loss: 1.3860399723052979 / Matching loss: 2.299708648934029e-05 / Item reconstruction: 0.0009519837331026793 / Text reconstruction: 0.00014841246593277901\n",
            "loss in epoch 7/11 iteration 56/111: 1.3867604732513428 / BPR loss: 1.3862215280532837 / Matching loss: 2.4377995941904373e-05 / Item reconstruction: 0.0009675270412117243 / Text reconstruction: 0.00015442734002135694\n",
            "loss in epoch 7/11 iteration 57/111: 1.3866389989852905 / BPR loss: 1.3861067295074463 / Matching loss: 2.2792173695052043e-05 / Item reconstruction: 0.0009573600254952908 / Text reconstruction: 0.0001543572434457019\n",
            "loss in epoch 7/11 iteration 58/111: 1.3865021467208862 / BPR loss: 1.3859847784042358 / Matching loss: 2.2448608433478512e-05 / Item reconstruction: 0.0009248060523532331 / Text reconstruction: 0.0001629566540941596\n",
            "loss in epoch 7/11 iteration 59/111: 1.3865060806274414 / BPR loss: 1.3859912157058716 / Matching loss: 2.238192973891273e-05 / Item reconstruction: 0.0009247633861377835 / Text reconstruction: 0.00014990776253398508\n",
            "loss in epoch 7/11 iteration 60/111: 1.3865797519683838 / BPR loss: 1.386066198348999 / Matching loss: 2.2818130673840642e-05 / Item reconstruction: 0.0009364432189613581 / Text reconstruction: 0.00011268519301665947\n",
            "loss in epoch 7/11 iteration 61/111: 1.3866618871688843 / BPR loss: 1.3861267566680908 / Matching loss: 2.2611515305470675e-05 / Item reconstruction: 0.0009684821125119925 / Text reconstruction: 0.00014141964493319392\n",
            "loss in epoch 7/11 iteration 62/111: 1.3865981101989746 / BPR loss: 1.3860907554626465 / Matching loss: 2.332208168809302e-05 / Item reconstruction: 0.0009138447931036353 / Text reconstruction: 0.0001350891252513975\n",
            "loss in epoch 7/11 iteration 63/111: 1.3866297006607056 / BPR loss: 1.3861125707626343 / Matching loss: 2.2983556846156716e-05 / Item reconstruction: 0.0009428774355910718 / Text reconstruction: 0.0001132925899582915\n",
            "loss in epoch 7/11 iteration 64/111: 1.3866453170776367 / BPR loss: 1.3861459493637085 / Matching loss: 2.1214033040450886e-05 / Item reconstruction: 0.0008963297004811466 / Text reconstruction: 0.00014995704987086356\n",
            "loss in epoch 7/11 iteration 65/111: 1.3866404294967651 / BPR loss: 1.3861325979232788 / Matching loss: 2.2779695427743718e-05 / Item reconstruction: 0.0009274652693420649 / Text reconstruction: 0.00010677672980818897\n",
            "loss in epoch 7/11 iteration 66/111: 1.3866002559661865 / BPR loss: 1.3861111402511597 / Matching loss: 2.1343803382478654e-05 / Item reconstruction: 0.0008930448675528169 / Text reconstruction: 0.00010598211520118639\n",
            "loss in epoch 7/11 iteration 67/111: 1.3866366147994995 / BPR loss: 1.3861122131347656 / Matching loss: 2.2517258912557736e-05 / Item reconstruction: 0.0009613882284611464 / Text reconstruction: 0.00010584859410300851\n",
            "loss in epoch 7/11 iteration 68/111: 1.386600136756897 / BPR loss: 1.3860965967178345 / Matching loss: 2.2433127014664933e-05 / Item reconstruction: 0.0009217496262863278 / Text reconstruction: 0.00010122306412085891\n",
            "loss in epoch 7/11 iteration 69/111: 1.3867167234420776 / BPR loss: 1.3862192630767822 / Matching loss: 2.163622048101388e-05 / Item reconstruction: 0.0009032187517732382 / Text reconstruction: 0.00012133551354054362\n",
            "loss in epoch 7/11 iteration 70/111: 1.3864965438842773 / BPR loss: 1.385995626449585 / Matching loss: 2.1467947590281256e-05 / Item reconstruction: 0.0009067284408956766 / Text reconstruction: 0.00013057590695098042\n",
            "loss in epoch 7/11 iteration 71/111: 1.3867560625076294 / BPR loss: 1.3862431049346924 / Matching loss: 2.1242703951429576e-05 / Item reconstruction: 0.0009224723326042295 / Text reconstruction: 0.0001523862883914262\n",
            "loss in epoch 7/11 iteration 72/111: 1.386507272720337 / BPR loss: 1.3860135078430176 / Matching loss: 2.2098996851127595e-05 / Item reconstruction: 0.0009063833858817816 / Text reconstruction: 9.261512605007738e-05\n",
            "loss in epoch 7/11 iteration 73/111: 1.3866807222366333 / BPR loss: 1.3861898183822632 / Matching loss: 2.2105141397332773e-05 / Item reconstruction: 0.0008996301330626011 / Text reconstruction: 9.517739090370014e-05\n",
            "loss in epoch 7/11 iteration 74/111: 1.3865221738815308 / BPR loss: 1.3859971761703491 / Matching loss: 2.236420550616458e-05 / Item reconstruction: 0.0009591705747880042 / Text reconstruction: 0.00011480700050015002\n",
            "loss in epoch 7/11 iteration 75/111: 1.386662483215332 / BPR loss: 1.3861584663391113 / Matching loss: 2.1555899365921505e-05 / Item reconstruction: 0.0009273006580770016 / Text reconstruction: 9.397452959092334e-05\n",
            "loss in epoch 7/11 iteration 76/111: 1.3867489099502563 / BPR loss: 1.3862367868423462 / Matching loss: 2.2403575712814927e-05 / Item reconstruction: 0.0009423812152817845 / Text reconstruction: 9.245976980309933e-05\n",
            "loss in epoch 7/11 iteration 77/111: 1.3866701126098633 / BPR loss: 1.3861842155456543 / Matching loss: 2.1447016479214653e-05 / Item reconstruction: 0.0008882933761924505 / Text reconstruction: 0.00010141648817807436\n",
            "loss in epoch 7/11 iteration 78/111: 1.3865677118301392 / BPR loss: 1.3860613107681274 / Matching loss: 2.2111868020147085e-05 / Item reconstruction: 0.000930196896661073 / Text reconstruction: 9.618070180295035e-05\n",
            "loss in epoch 7/11 iteration 79/111: 1.3866764307022095 / BPR loss: 1.3861627578735352 / Matching loss: 2.1529383957386017e-05 / Item reconstruction: 0.0009405200835317373 / Text reconstruction: 0.00010914582526311278\n",
            "loss in epoch 7/11 iteration 80/111: 1.3866719007492065 / BPR loss: 1.3861587047576904 / Matching loss: 2.2654170606983826e-05 / Item reconstruction: 0.0009446420590393245 / Text reconstruction: 9.130760736297816e-05\n",
            "loss in epoch 7/11 iteration 81/111: 1.3865185976028442 / BPR loss: 1.3860317468643188 / Matching loss: 2.1626390662277117e-05 / Item reconstruction: 0.0008821514202281833 / Text reconstruction: 0.00012094467820134014\n",
            "loss in epoch 7/11 iteration 82/111: 1.3865991830825806 / BPR loss: 1.3860971927642822 / Matching loss: 2.0643306925194338e-05 / Item reconstruction: 0.0009177584433928132 / Text reconstruction: 0.00011249120143475011\n",
            "loss in epoch 7/11 iteration 83/111: 1.386866807937622 / BPR loss: 1.3863661289215088 / Matching loss: 2.298954495927319e-05 / Item reconstruction: 0.0009190500131808221 / Text reconstruction: 9.047024650499225e-05\n",
            "loss in epoch 7/11 iteration 84/111: 1.3866297006607056 / BPR loss: 1.3861167430877686 / Matching loss: 2.1729843865614384e-05 / Item reconstruction: 0.0009255860350094736 / Text reconstruction: 0.00014219348668120801\n",
            "loss in epoch 7/11 iteration 85/111: 1.3866488933563232 / BPR loss: 1.3861520290374756 / Matching loss: 2.2712918507750146e-05 / Item reconstruction: 0.0009141588816419244 / Text reconstruction: 8.496099326293916e-05\n",
            "loss in epoch 7/11 iteration 86/111: 1.3864728212356567 / BPR loss: 1.3859636783599854 / Matching loss: 2.2829570298199542e-05 / Item reconstruction: 0.000938352313823998 / Text reconstruction: 8.547755714971572e-05\n",
            "loss in epoch 7/11 iteration 87/111: 1.3866188526153564 / BPR loss: 1.3861172199249268 / Matching loss: 2.2977626940701157e-05 / Item reconstruction: 0.0009211002616211772 / Text reconstruction: 9.065748599823564e-05\n",
            "loss in epoch 7/11 iteration 88/111: 1.3866181373596191 / BPR loss: 1.3861150741577148 / Matching loss: 2.1079064026707783e-05 / Item reconstruction: 0.0009069739608094096 / Text reconstruction: 0.00014226490748114884\n",
            "loss in epoch 7/11 iteration 89/111: 1.3866482973098755 / BPR loss: 1.3861536979675293 / Matching loss: 2.116721952916123e-05 / Item reconstruction: 0.0009110621176660061 / Text reconstruction: 8.924101712182164e-05\n",
            "loss in epoch 7/11 iteration 90/111: 1.386549472808838 / BPR loss: 1.3860316276550293 / Matching loss: 2.2633685148321092e-05 / Item reconstruction: 0.0009484895272180438 / Text reconstruction: 0.00010489729174878448\n",
            "loss in epoch 7/11 iteration 91/111: 1.3866578340530396 / BPR loss: 1.3861547708511353 / Matching loss: 2.25124185817549e-05 / Item reconstruction: 0.0009252557065337896 / Text reconstruction: 8.95457633305341e-05\n",
            "loss in epoch 7/11 iteration 92/111: 1.386598825454712 / BPR loss: 1.386101245880127 / Matching loss: 2.195444903918542e-05 / Item reconstruction: 0.0009115487337112427 / Text reconstruction: 9.92769782897085e-05\n",
            "loss in epoch 7/11 iteration 93/111: 1.3866688013076782 / BPR loss: 1.3861596584320068 / Matching loss: 2.2563159291166812e-05 / Item reconstruction: 0.0009407311445102096 / Text reconstruction: 8.127798355417326e-05\n",
            "loss in epoch 7/11 iteration 94/111: 1.3866329193115234 / BPR loss: 1.386134386062622 / Matching loss: 2.2258212993619964e-05 / Item reconstruction: 0.0009181075729429722 / Text reconstruction: 8.591578807681799e-05\n",
            "loss in epoch 7/11 iteration 95/111: 1.3866583108901978 / BPR loss: 1.38614821434021 / Matching loss: 2.2751417418476194e-05 / Item reconstruction: 0.0009276086930185556 / Text reconstruction: 0.00011770089622586966\n",
            "loss in epoch 7/11 iteration 96/111: 1.3866727352142334 / BPR loss: 1.3861708641052246 / Matching loss: 2.2900909243617207e-05 / Item reconstruction: 0.0009092905092984438 / Text reconstruction: 0.00012141784827690572\n",
            "loss in epoch 7/11 iteration 97/111: 1.3866972923278809 / BPR loss: 1.386195182800293 / Matching loss: 2.1132989786565304e-05 / Item reconstruction: 0.00090444041416049 / Text reconstruction: 0.00014430415467359126\n",
            "loss in epoch 7/11 iteration 98/111: 1.3867658376693726 / BPR loss: 1.3862721920013428 / Matching loss: 2.208292789873667e-05 / Item reconstruction: 0.0009012161754071712 / Text reconstruction: 0.00010472217400092632\n",
            "loss in epoch 7/11 iteration 99/111: 1.386675477027893 / BPR loss: 1.3861639499664307 / Matching loss: 2.294563273608219e-05 / Item reconstruction: 0.0009397759567946196 / Text reconstruction: 9.380741539644077e-05\n",
            "loss in epoch 7/11 iteration 100/111: 1.386648178100586 / BPR loss: 1.386130690574646 / Matching loss: 2.331470932404045e-05 / Item reconstruction: 0.0009577368618920445 / Text reconstruction: 7.649917824892327e-05\n",
            "loss in epoch 7/11 iteration 101/111: 1.386521339416504 / BPR loss: 1.386034369468689 / Matching loss: 2.155086440325249e-05 / Item reconstruction: 0.0008803891250863671 / Text reconstruction: 0.0001254878588952124\n",
            "loss in epoch 7/11 iteration 102/111: 1.3865376710891724 / BPR loss: 1.3860167264938354 / Matching loss: 2.2652693587588146e-05 / Item reconstruction: 0.0009551958064548671 / Text reconstruction: 0.00010376557474955916\n",
            "loss in epoch 7/11 iteration 103/111: 1.3867441415786743 / BPR loss: 1.3862309455871582 / Matching loss: 2.151352964574471e-05 / Item reconstruction: 0.000923202489502728 / Text reconstruction: 0.00015097837604116648\n",
            "loss in epoch 7/11 iteration 104/111: 1.386634349822998 / BPR loss: 1.38612699508667 / Matching loss: 2.211590799561236e-05 / Item reconstruction: 0.0009118546731770039 / Text reconstruction: 0.00014592429215554148\n",
            "loss in epoch 7/11 iteration 105/111: 1.3865960836410522 / BPR loss: 1.3861008882522583 / Matching loss: 2.207139004894998e-05 / Item reconstruction: 0.0009036078699864447 / Text reconstruction: 0.0001066630138666369\n",
            "loss in epoch 7/11 iteration 106/111: 1.3866957426071167 / BPR loss: 1.3861804008483887 / Matching loss: 2.3294916900340468e-05 / Item reconstruction: 0.0009365801233798265 / Text reconstruction: 0.00011917680967599154\n",
            "loss in epoch 7/11 iteration 107/111: 1.3866724967956543 / BPR loss: 1.3861830234527588 / Matching loss: 2.118844713550061e-05 / Item reconstruction: 0.0008971269708126783 / Text reconstruction: 9.818452963372692e-05\n",
            "loss in epoch 7/11 iteration 108/111: 1.3867480754852295 / BPR loss: 1.3862597942352295 / Matching loss: 2.1603464119834825e-05 / Item reconstruction: 0.0008947354508563876 / Text reconstruction: 9.650080755818635e-05\n",
            "loss in epoch 7/11 iteration 109/111: 1.3868558406829834 / BPR loss: 1.3863565921783447 / Matching loss: 2.237049920950085e-05 / Item reconstruction: 0.0009061489254236221 / Text reconstruction: 0.00011880478268722072\n",
            "loss in epoch 7/11 iteration 110/111: 1.3866968154907227 / BPR loss: 1.3862110376358032 / Matching loss: 1.7631780792726204e-05 / Item reconstruction: 0.0008162137819454074 / Text reconstruction: 0.00030025208252482116\n",
            "loss in epoch 7/11 iteration 111/111: 1.3866583108901978 / BPR loss: 1.3862096071243286 / Matching loss: 1.5333402188844047e-05 / Item reconstruction: 0.0006986779044382274 / Text reconstruction: 0.0004204086144454777\n",
            " 70% 7/10 [37:32<16:03, 321.09s/it]loss in epoch 8/11 iteration 0/111: 1.386595606803894 / BPR loss: 1.3860645294189453 / Matching loss: 2.3528176825493574e-05 / Item reconstruction: 0.0009753552731126547 / Text reconstruction: 9.943981422111392e-05\n",
            "loss in epoch 8/11 iteration 1/111: 1.3864904642105103 / BPR loss: 1.3859856128692627 / Matching loss: 2.0915174900437705e-05 / Item reconstruction: 0.0009096835856325924 / Text reconstruction: 0.00014595295942854136\n",
            "loss in epoch 8/11 iteration 2/111: 1.3866132497787476 / BPR loss: 1.3860787153244019 / Matching loss: 2.247062548121903e-05 / Item reconstruction: 0.0009659122442826629 / Text reconstruction: 0.00014611160440836102\n",
            "loss in epoch 8/11 iteration 3/111: 1.3865766525268555 / BPR loss: 1.3860509395599365 / Matching loss: 2.227521690656431e-05 / Item reconstruction: 0.0009563287021592259 / Text reconstruction: 0.00012646906543523073\n",
            "loss in epoch 8/11 iteration 4/111: 1.3866862058639526 / BPR loss: 1.3861467838287354 / Matching loss: 2.24629147851374e-05 / Item reconstruction: 0.0009886210318654776 / Text reconstruction: 0.00011347448889864609\n",
            "loss in epoch 8/11 iteration 5/111: 1.3866716623306274 / BPR loss: 1.3861688375473022 / Matching loss: 2.1854335500393063e-05 / Item reconstruction: 0.0009128128294833004 / Text reconstruction: 0.00012269208673387766\n",
            "loss in epoch 8/11 iteration 6/111: 1.3868530988693237 / BPR loss: 1.3863362073898315 / Matching loss: 2.3703785700490698e-05 / Item reconstruction: 0.000956352218054235 / Text reconstruction: 7.48349993955344e-05\n",
            "loss in epoch 8/11 iteration 7/111: 1.3868191242218018 / BPR loss: 1.3863046169281006 / Matching loss: 2.4144381313817576e-05 / Item reconstruction: 0.0009494638070464134 / Text reconstruction: 7.784078479744494e-05\n",
            "loss in epoch 8/11 iteration 8/111: 1.3867424726486206 / BPR loss: 1.3862254619598389 / Matching loss: 2.212121762568131e-05 / Item reconstruction: 0.0009319667587988079 / Text reconstruction: 0.0001441003696527332\n",
            "loss in epoch 8/11 iteration 9/111: 1.3867311477661133 / BPR loss: 1.386106014251709 / Matching loss: 2.2028209059499204e-05 / Item reconstruction: 0.0010486278915777802 / Text reconstruction: 0.00039423577254638076\n",
            "loss in epoch 8/11 iteration 10/111: 1.3860175609588623 / BPR loss: 1.3851521015167236 / Matching loss: 1.7186699551530182e-05 / Item reconstruction: 0.0012802451383322477 / Text reconstruction: 0.0010407607769593596\n",
            "loss in epoch 8/11 iteration 11/111: 1.3859130144119263 / BPR loss: 1.3851549625396729 / Matching loss: 1.865507147158496e-05 / Item reconstruction: 0.0012311567552387714 / Text reconstruction: 0.0006193265435285866\n",
            "loss in epoch 8/11 iteration 12/111: 1.3858022689819336 / BPR loss: 1.3851335048675537 / Matching loss: 1.7575517631485127e-05 / Item reconstruction: 0.0011042661499232054 / Text reconstruction: 0.0004952863091602921\n",
            "loss in epoch 8/11 iteration 13/111: 1.3857719898223877 / BPR loss: 1.3851830959320068 / Matching loss: 1.462340424041031e-05 / Item reconstruction: 0.0009691186132840812 / Text reconstruction: 0.00044805777724832296\n",
            "loss in epoch 8/11 iteration 14/111: 1.3859416246414185 / BPR loss: 1.3853776454925537 / Matching loss: 1.7158503396785818e-05 / Item reconstruction: 0.0009404324227944016 / Text reconstruction: 0.0003834424715023488\n",
            "loss in epoch 8/11 iteration 15/111: 1.385661005973816 / BPR loss: 1.3851113319396973 / Matching loss: 1.589408202562481e-05 / Item reconstruction: 0.0009133466519415379 / Text reconstruction: 0.00038562482222914696\n",
            "loss in epoch 8/11 iteration 16/111: 1.3863438367843628 / BPR loss: 1.3856635093688965 / Matching loss: 1.9054943550145254e-05 / Item reconstruction: 0.00115210993681103 / Text reconstruction: 0.00042640056926757097\n",
            "loss in epoch 8/11 iteration 17/111: 1.3871365785598755 / BPR loss: 1.3862035274505615 / Matching loss: 2.2839401935925707e-05 / Item reconstruction: 0.0015538716688752174 / Text reconstruction: 0.0006664912798441947\n",
            "loss in epoch 8/11 iteration 18/111: 1.3871021270751953 / BPR loss: 1.3861747980117798 / Matching loss: 2.357382618356496e-05 / Item reconstruction: 0.0015379837714135647 / Text reconstruction: 0.000673405360430479\n",
            "loss in epoch 8/11 iteration 19/111: 1.3869606256484985 / BPR loss: 1.3861138820648193 / Matching loss: 2.4516059056622908e-05 / Item reconstruction: 0.0014204748440533876 / Text reconstruction: 0.0005599199212156236\n",
            "loss in epoch 8/11 iteration 20/111: 1.3869056701660156 / BPR loss: 1.38615083694458 / Matching loss: 2.385083462286275e-05 / Item reconstruction: 0.0012942536268383265 / Text reconstruction: 0.00041986096766777337\n",
            "loss in epoch 8/11 iteration 21/111: 1.387028694152832 / BPR loss: 1.3863657712936401 / Matching loss: 2.2266038286034018e-05 / Item reconstruction: 0.0011498997919261456 / Text reconstruction: 0.0003281802637502551\n",
            "loss in epoch 8/11 iteration 22/111: 1.386695146560669 / BPR loss: 1.3861193656921387 / Matching loss: 2.3026765120448545e-05 / Item reconstruction: 0.001023447373881936 / Text reconstruction: 0.00020517996745184064\n",
            "loss in epoch 8/11 iteration 23/111: 1.3867487907409668 / BPR loss: 1.3862038850784302 / Matching loss: 2.1935313270660117e-05 / Item reconstruction: 0.0009821667335927486 / Text reconstruction: 0.00015918040298856795\n",
            "loss in epoch 8/11 iteration 24/111: 1.3867487907409668 / BPR loss: 1.386232614517212 / Matching loss: 2.1247295080684125e-05 / Item reconstruction: 0.0009261677041649818 / Text reconstruction: 0.00015937992429826409\n",
            "loss in epoch 8/11 iteration 25/111: 1.386674404144287 / BPR loss: 1.3861351013183594 / Matching loss: 2.2562238882528618e-05 / Item reconstruction: 0.0009690187871456146 / Text reconstruction: 0.00016124725516419858\n",
            "loss in epoch 8/11 iteration 26/111: 1.3866546154022217 / BPR loss: 1.3860878944396973 / Matching loss: 2.4194792786147445e-05 / Item reconstruction: 0.001017685979604721 / Text reconstruction: 0.00016854186833370477\n",
            "loss in epoch 8/11 iteration 27/111: 1.3866914510726929 / BPR loss: 1.3861135244369507 / Matching loss: 2.278601095895283e-05 / Item reconstruction: 0.001035522436723113 / Text reconstruction: 0.0001872895227279514\n",
            "loss in epoch 8/11 iteration 28/111: 1.386810302734375 / BPR loss: 1.3862407207489014 / Matching loss: 2.1644329535774887e-05 / Item reconstruction: 0.0010082339867949486 / Text reconstruction: 0.00021855214436072856\n",
            "loss in epoch 8/11 iteration 29/111: 1.3868249654769897 / BPR loss: 1.386209487915039 / Matching loss: 2.289845360792242e-05 / Item reconstruction: 0.0011023895349353552 / Text reconstruction: 0.0002069998881779611\n",
            "loss in epoch 8/11 iteration 30/111: 1.3867599964141846 / BPR loss: 1.3861610889434814 / Matching loss: 2.4097904315567575e-05 / Item reconstruction: 0.0010754496324807405 / Text reconstruction: 0.00018535114941187203\n",
            "loss in epoch 8/11 iteration 31/111: 1.3867638111114502 / BPR loss: 1.386192798614502 / Matching loss: 2.1654073862009682e-05 / Item reconstruction: 0.001038441783748567 / Text reconstruction: 0.0001501111255493015\n",
            "loss in epoch 8/11 iteration 32/111: 1.3866790533065796 / BPR loss: 1.3861100673675537 / Matching loss: 2.1712499801651575e-05 / Item reconstruction: 0.0010349343065172434 / Text reconstruction: 0.00014919351087883115\n",
            "loss in epoch 8/11 iteration 33/111: 1.3867005109786987 / BPR loss: 1.3861339092254639 / Matching loss: 2.384353501838632e-05 / Item reconstruction: 0.0010266725439578295 / Text reconstruction: 0.00014739844482392073\n",
            "loss in epoch 8/11 iteration 34/111: 1.3865739107131958 / BPR loss: 1.3860230445861816 / Matching loss: 2.2756044927518815e-05 / Item reconstruction: 0.001004116958938539 / Text reconstruction: 0.00012965996575076133\n",
            "loss in epoch 8/11 iteration 35/111: 1.386736512184143 / BPR loss: 1.3862148523330688 / Matching loss: 2.2106265532784164e-05 / Item reconstruction: 0.0009520212188363075 / Text reconstruction: 0.00011826235277112573\n",
            "loss in epoch 8/11 iteration 36/111: 1.3866019248962402 / BPR loss: 1.3860739469528198 / Matching loss: 2.3385786334984004e-05 / Item reconstruction: 0.000944450031965971 / Text reconstruction: 0.00016223658167291433\n",
            "loss in epoch 8/11 iteration 37/111: 1.3867809772491455 / BPR loss: 1.3862557411193848 / Matching loss: 2.2661653929390013e-05 / Item reconstruction: 0.0009496948914602399 / Text reconstruction: 0.00013876381854061037\n",
            "loss in epoch 8/11 iteration 38/111: 1.386700987815857 / BPR loss: 1.3861417770385742 / Matching loss: 2.4870507331797853e-05 / Item reconstruction: 0.0010199546813964844 / Text reconstruction: 0.00012131541734561324\n",
            "loss in epoch 8/11 iteration 39/111: 1.3866140842437744 / BPR loss: 1.3861021995544434 / Matching loss: 2.3187751139630564e-05 / Item reconstruction: 0.0009351779008284211 / Text reconstruction: 0.00010555073822615668\n",
            "loss in epoch 8/11 iteration 40/111: 1.3865803480148315 / BPR loss: 1.3860565423965454 / Matching loss: 2.3471649910788983e-05 / Item reconstruction: 0.0009608226828277111 / Text reconstruction: 9.924460027832538e-05\n",
            "loss in epoch 8/11 iteration 41/111: 1.3866664171218872 / BPR loss: 1.386152982711792 / Matching loss: 2.217660039605107e-05 / Item reconstruction: 0.0009422022267244756 / Text reconstruction: 0.00010072768054669723\n",
            "loss in epoch 8/11 iteration 42/111: 1.3867768049240112 / BPR loss: 1.3862435817718506 / Matching loss: 2.3230437363963574e-05 / Item reconstruction: 0.0009649532148614526 / Text reconstruction: 0.0001374329294776544\n",
            "loss in epoch 8/11 iteration 43/111: 1.3865995407104492 / BPR loss: 1.3860764503479004 / Matching loss: 2.247056909254752e-05 / Item reconstruction: 0.0009597302414476871 / Text reconstruction: 0.00010429180110804737\n",
            "loss in epoch 8/11 iteration 44/111: 1.3866771459579468 / BPR loss: 1.3861498832702637 / Matching loss: 2.278765350638423e-05 / Item reconstruction: 0.0009700863156467676 / Text reconstruction: 9.74528375081718e-05\n",
            "loss in epoch 8/11 iteration 45/111: 1.3866857290267944 / BPR loss: 1.3861675262451172 / Matching loss: 2.187963946198579e-05 / Item reconstruction: 0.000945607665926218 / Text reconstruction: 0.00011763308430090547\n",
            "loss in epoch 8/11 iteration 46/111: 1.3867607116699219 / BPR loss: 1.3862390518188477 / Matching loss: 2.4072443920886144e-05 / Item reconstruction: 0.000955844996497035 / Text reconstruction: 9.850570495473221e-05\n",
            "loss in epoch 8/11 iteration 47/111: 1.3865118026733398 / BPR loss: 1.386002540588379 / Matching loss: 2.2600157535634935e-05 / Item reconstruction: 0.0009349157335236669 / Text reconstruction: 9.592834248906001e-05\n",
            "loss in epoch 8/11 iteration 48/111: 1.3866016864776611 / BPR loss: 1.3860690593719482 / Matching loss: 2.4430504709016532e-05 / Item reconstruction: 0.000979947973974049 / Text reconstruction: 9.144316572928801e-05\n",
            "loss in epoch 8/11 iteration 49/111: 1.386540412902832 / BPR loss: 1.3860243558883667 / Matching loss: 2.3100665202946402e-05 / Item reconstruction: 0.0009495606645941734 / Text reconstruction: 9.033622336573899e-05\n",
            "loss in epoch 8/11 iteration 50/111: 1.386560082435608 / BPR loss: 1.3860515356063843 / Matching loss: 2.291956843691878e-05 / Item reconstruction: 0.000935270800255239 / Text reconstruction: 8.991684444481507e-05\n",
            "loss in epoch 8/11 iteration 51/111: 1.3867127895355225 / BPR loss: 1.386204481124878 / Matching loss: 2.358522942813579e-05 / Item reconstruction: 0.0009375467197969556 / Text reconstruction: 7.999803347047418e-05\n",
            "loss in epoch 8/11 iteration 52/111: 1.3865845203399658 / BPR loss: 1.3860689401626587 / Matching loss: 2.445435711706523e-05 / Item reconstruction: 0.0009478898136876523 / Text reconstruction: 8.561275899410248e-05\n",
            "loss in epoch 8/11 iteration 53/111: 1.386405110359192 / BPR loss: 1.3859214782714844 / Matching loss: 2.268010939587839e-05 / Item reconstruction: 0.0008886728901416063 / Text reconstruction: 8.334732410730794e-05\n",
            "loss in epoch 8/11 iteration 54/111: 1.386622667312622 / BPR loss: 1.386103868484497 / Matching loss: 2.3240399968926795e-05 / Item reconstruction: 0.0009546254295855761 / Text reconstruction: 9.130513353738934e-05\n",
            "loss in epoch 8/11 iteration 55/111: 1.3864973783493042 / BPR loss: 1.3859761953353882 / Matching loss: 2.3503998818341643e-05 / Item reconstruction: 0.0009479520376771688 / Text reconstruction: 0.00011888421431649476\n",
            "loss in epoch 8/11 iteration 56/111: 1.3866896629333496 / BPR loss: 1.386163353919983 / Matching loss: 2.45020510192262e-05 / Item reconstruction: 0.0009675528854131699 / Text reconstruction: 8.99468213901855e-05\n",
            "loss in epoch 8/11 iteration 57/111: 1.3866530656814575 / BPR loss: 1.3861396312713623 / Matching loss: 2.3305074137169868e-05 / Item reconstruction: 0.0009370813495479524 / Text reconstruction: 0.00010827755613718182\n",
            "loss in epoch 8/11 iteration 58/111: 1.3865519762039185 / BPR loss: 1.3860564231872559 / Matching loss: 2.3778546164976433e-05 / Item reconstruction: 0.0008983306470327079 / Text reconstruction: 0.00011339085176587105\n",
            "loss in epoch 8/11 iteration 59/111: 1.386576771736145 / BPR loss: 1.3860712051391602 / Matching loss: 2.30903024203144e-05 / Item reconstruction: 0.000927693210542202 / Text reconstruction: 9.29300585994497e-05\n",
            "loss in epoch 8/11 iteration 60/111: 1.3865339756011963 / BPR loss: 1.3860291242599487 / Matching loss: 2.2866861399961635e-05 / Item reconstruction: 0.0009285816340707242 / Text reconstruction: 8.824447286315262e-05\n",
            "loss in epoch 8/11 iteration 61/111: 1.3865821361541748 / BPR loss: 1.3860583305358887 / Matching loss: 2.3193118977360427e-05 / Item reconstruction: 0.0009561601327732205 / Text reconstruction: 0.00011269981041550636\n",
            "loss in epoch 8/11 iteration 62/111: 1.386763095855713 / BPR loss: 1.3862591981887817 / Matching loss: 2.2046737285563722e-05 / Item reconstruction: 0.000930069712921977 / Text reconstruction: 8.38270498206839e-05\n",
            "loss in epoch 8/11 iteration 63/111: 1.38657546043396 / BPR loss: 1.386070728302002 / Matching loss: 2.3701850295765325e-05 / Item reconstruction: 0.0009299266384914517 / Text reconstruction: 8.038552186917514e-05\n",
            "loss in epoch 8/11 iteration 64/111: 1.3865631818771362 / BPR loss: 1.3860727548599243 / Matching loss: 2.163001408916898e-05 / Item reconstruction: 0.0008889474556781352 / Text reconstruction: 0.00012171215348644182\n",
            "loss in epoch 8/11 iteration 65/111: 1.3865808248519897 / BPR loss: 1.3860883712768555 / Matching loss: 2.2149582946440205e-05 / Item reconstruction: 0.0009083328768610954 / Text reconstruction: 8.042759145610034e-05\n",
            "loss in epoch 8/11 iteration 66/111: 1.3866641521453857 / BPR loss: 1.386181354522705 / Matching loss: 2.1525775082409382e-05 / Item reconstruction: 0.0008885653805918992 / Text reconstruction: 8.470075408695266e-05\n",
            "loss in epoch 8/11 iteration 67/111: 1.3865669965744019 / BPR loss: 1.3860520124435425 / Matching loss: 2.339212005608715e-05 / Item reconstruction: 0.0009518606821075082 / Text reconstruction: 7.864140206947923e-05\n",
            "loss in epoch 8/11 iteration 68/111: 1.38661527633667 / BPR loss: 1.3861263990402222 / Matching loss: 2.2132187950774096e-05 / Item reconstruction: 0.0009025452891364694 / Text reconstruction: 7.704705058131367e-05\n",
            "loss in epoch 8/11 iteration 69/111: 1.3866597414016724 / BPR loss: 1.3861613273620605 / Matching loss: 2.2336804249789566e-05 / Item reconstruction: 0.0009118335437960923 / Text reconstruction: 0.00010102864325745031\n",
            "loss in epoch 8/11 iteration 70/111: 1.3865677118301392 / BPR loss: 1.3860795497894287 / Matching loss: 2.1208052203292027e-05 / Item reconstruction: 0.0008898529340513051 / Text reconstruction: 0.00011026151332771406\n",
            "loss in epoch 8/11 iteration 71/111: 1.3866592645645142 / BPR loss: 1.3861485719680786 / Matching loss: 2.1954641852062196e-05 / Item reconstruction: 0.0009248516289517283 / Text reconstruction: 0.0001314374094363302\n",
            "loss in epoch 8/11 iteration 72/111: 1.3865256309509277 / BPR loss: 1.3860130310058594 / Matching loss: 2.4075419787550345e-05 / Item reconstruction: 0.0009407582692801952 / Text reconstruction: 9.051415690919384e-05\n",
            "loss in epoch 8/11 iteration 73/111: 1.3866146802902222 / BPR loss: 1.386134147644043 / Matching loss: 2.2518321202369407e-05 / Item reconstruction: 0.0008802985539659858 / Text reconstruction: 8.914863428799435e-05\n",
            "loss in epoch 8/11 iteration 74/111: 1.386438012123108 / BPR loss: 1.3859179019927979 / Matching loss: 2.1994121198076755e-05 / Item reconstruction: 0.0009408537880517542 / Text reconstruction: 0.00013826014765072614\n",
            "loss in epoch 8/11 iteration 75/111: 1.3865795135498047 / BPR loss: 1.3860739469528198 / Matching loss: 2.2673859348287806e-05 / Item reconstruction: 0.0009334769565612078 / Text reconstruction: 8.089219045359641e-05\n",
            "loss in epoch 8/11 iteration 76/111: 1.3867119550704956 / BPR loss: 1.3861724138259888 / Matching loss: 2.416650931991171e-05 / Item reconstruction: 0.0009857539553195238 / Text reconstruction: 0.00011217148130526766\n",
            "loss in epoch 8/11 iteration 77/111: 1.386716365814209 / BPR loss: 1.386225938796997 / Matching loss: 2.2290392735158093e-05 / Item reconstruction: 0.0008983408915810287 / Text reconstruction: 9.455032704863697e-05\n",
            "loss in epoch 8/11 iteration 78/111: 1.3865748643875122 / BPR loss: 1.386067271232605 / Matching loss: 2.2577016352443025e-05 / Item reconstruction: 0.0009361268021166325 / Text reconstruction: 8.50003634695895e-05\n",
            "loss in epoch 8/11 iteration 79/111: 1.3866560459136963 / BPR loss: 1.3861361742019653 / Matching loss: 2.3237069399328902e-05 / Item reconstruction: 0.0009478706051595509 / Text reconstruction: 0.00011341948265908286\n",
            "loss in epoch 8/11 iteration 80/111: 1.3866022825241089 / BPR loss: 1.386122226715088 / Matching loss: 2.20428264583461e-05 / Item reconstruction: 0.0008856499334797263 / Text reconstruction: 7.541527156718075e-05\n",
            "loss in epoch 8/11 iteration 81/111: 1.3865385055541992 / BPR loss: 1.386051893234253 / Matching loss: 2.166589183616452e-05 / Item reconstruction: 0.0008879872038960457 / Text reconstruction: 0.00010466295498190448\n",
            "loss in epoch 8/11 iteration 82/111: 1.3865530490875244 / BPR loss: 1.3860485553741455 / Matching loss: 2.1796113287564367e-05 / Item reconstruction: 0.0009207784314639866 / Text reconstruction: 0.00011142746370751411\n",
            "loss in epoch 8/11 iteration 83/111: 1.386743426322937 / BPR loss: 1.386223554611206 / Matching loss: 2.4923880118876696e-05 / Item reconstruction: 0.0009594714501872659 / Text reconstruction: 7.651734631508589e-05\n",
            "loss in epoch 8/11 iteration 84/111: 1.386563777923584 / BPR loss: 1.386077880859375 / Matching loss: 2.0996489183744416e-05 / Item reconstruction: 0.0008734202710911632 / Text reconstruction: 0.0001410501281498\n",
            "loss in epoch 8/11 iteration 85/111: 1.386719822883606 / BPR loss: 1.3862061500549316 / Matching loss: 2.3716384021099657e-05 / Item reconstruction: 0.000947491149418056 / Text reconstruction: 8.12554353615269e-05\n",
            "loss in epoch 8/11 iteration 86/111: 1.3864736557006836 / BPR loss: 1.3859835863113403 / Matching loss: 2.202972609666176e-05 / Item reconstruction: 0.0008996968390420079 / Text reconstruction: 9.083930490305647e-05\n",
            "loss in epoch 8/11 iteration 87/111: 1.3866485357284546 / BPR loss: 1.386151909828186 / Matching loss: 2.2635455025010742e-05 / Item reconstruction: 0.0009074044064618647 / Text reconstruction: 0.00010142351675312966\n",
            "loss in epoch 8/11 iteration 88/111: 1.3865807056427002 / BPR loss: 1.3860687017440796 / Matching loss: 2.276972372783348e-05 / Item reconstruction: 0.0009034458780661225 / Text reconstruction: 0.0001879795454442501\n",
            "loss in epoch 8/11 iteration 89/111: 1.3867629766464233 / BPR loss: 1.3862649202346802 / Matching loss: 2.2110805730335414e-05 / Item reconstruction: 0.000921902246773243 / Text reconstruction: 7.48761958675459e-05\n",
            "loss in epoch 8/11 iteration 90/111: 1.3864773511886597 / BPR loss: 1.3859573602676392 / Matching loss: 2.2402513423003256e-05 / Item reconstruction: 0.0009523510234430432 / Text reconstruction: 0.00010705659224186093\n",
            "loss in epoch 8/11 iteration 91/111: 1.3865820169448853 / BPR loss: 1.386096477508545 / Matching loss: 2.175637928303331e-05 / Item reconstruction: 0.0008892318001016974 / Text reconstruction: 9.508553921477869e-05\n",
            "loss in epoch 8/11 iteration 92/111: 1.3866349458694458 / BPR loss: 1.386141300201416 / Matching loss: 2.1315623598638922e-05 / Item reconstruction: 0.0009041409939527512 / Text reconstruction: 0.00010104743705596775\n",
            "loss in epoch 8/11 iteration 93/111: 1.386609435081482 / BPR loss: 1.386103630065918 / Matching loss: 2.239496461697854e-05 / Item reconstruction: 0.0009252260206267238 / Text reconstruction: 0.00010367791401222348\n",
            "loss in epoch 8/11 iteration 94/111: 1.3866990804672241 / BPR loss: 1.3861989974975586 / Matching loss: 2.285070513607934e-05 / Item reconstruction: 0.0009187028044834733 / Text reconstruction: 8.923584391595796e-05\n",
            "loss in epoch 8/11 iteration 95/111: 1.3866316080093384 / BPR loss: 1.386122226715088 / Matching loss: 2.1982794351060875e-05 / Item reconstruction: 0.0009284947300329804 / Text reconstruction: 0.00011636455019470304\n",
            "loss in epoch 8/11 iteration 96/111: 1.386688470840454 / BPR loss: 1.3861823081970215 / Matching loss: 2.2811094822827727e-05 / Item reconstruction: 0.0009261718951165676 / Text reconstruction: 0.00010151543392566964\n",
            "loss in epoch 8/11 iteration 97/111: 1.3866223096847534 / BPR loss: 1.3861360549926758 / Matching loss: 2.1264622773742303e-05 / Item reconstruction: 0.0008800807408988476 / Text reconstruction: 0.0001250789500772953\n",
            "loss in epoch 8/11 iteration 98/111: 1.3868423700332642 / BPR loss: 1.3863639831542969 / Matching loss: 2.1030173229519278e-05 / Item reconstruction: 0.0008779061608947814 / Text reconstruction: 9.245428373105824e-05\n",
            "loss in epoch 8/11 iteration 99/111: 1.3865835666656494 / BPR loss: 1.3860670328140259 / Matching loss: 2.2671520127914846e-05 / Item reconstruction: 0.0009494264959357679 / Text reconstruction: 9.566669905325398e-05\n",
            "loss in epoch 8/11 iteration 100/111: 1.3866504430770874 / BPR loss: 1.3861502408981323 / Matching loss: 2.2196434656507336e-05 / Item reconstruction: 0.0009277425124309957 / Text reconstruction: 7.069647836033255e-05\n",
            "loss in epoch 8/11 iteration 101/111: 1.3865935802459717 / BPR loss: 1.386101484298706 / Matching loss: 2.081459024338983e-05 / Item reconstruction: 0.0008957505924627185 / Text reconstruction: 0.0001167996961157769\n",
            "loss in epoch 8/11 iteration 102/111: 1.3865514993667603 / BPR loss: 1.386040449142456 / Matching loss: 2.1187141101108864e-05 / Item reconstruction: 0.0009322959231212735 / Text reconstruction: 0.00011845138214994222\n",
            "loss in epoch 8/11 iteration 103/111: 1.386728048324585 / BPR loss: 1.386223316192627 / Matching loss: 2.2080832422943786e-05 / Item reconstruction: 0.0009137394372373819 / Text reconstruction: 0.0001288415223825723\n",
            "loss in epoch 8/11 iteration 104/111: 1.386630892753601 / BPR loss: 1.3861427307128906 / Matching loss: 2.047833550022915e-05 / Item reconstruction: 0.0008836015476845205 / Text reconstruction: 0.00012946443166583776\n",
            "loss in epoch 8/11 iteration 105/111: 1.386553406715393 / BPR loss: 1.38606858253479 / Matching loss: 2.2372583771357313e-05 / Item reconstruction: 0.0008841470698826015 / Text reconstruction: 0.00010190453758696094\n",
            "loss in epoch 8/11 iteration 106/111: 1.386608600616455 / BPR loss: 1.3860864639282227 / Matching loss: 2.2846214051241986e-05 / Item reconstruction: 0.0009597798925824463 / Text reconstruction: 9.643664816394448e-05\n",
            "loss in epoch 8/11 iteration 107/111: 1.3865023851394653 / BPR loss: 1.386013150215149 / Matching loss: 2.169159779441543e-05 / Item reconstruction: 0.0008979518315754831 / Text reconstruction: 9.27866276470013e-05\n",
            "loss in epoch 8/11 iteration 108/111: 1.3866854906082153 / BPR loss: 1.3861839771270752 / Matching loss: 2.1863805159227923e-05 / Item reconstruction: 0.0009228069102391601 / Text reconstruction: 9.09904483705759e-05\n",
            "loss in epoch 8/11 iteration 109/111: 1.3867868185043335 / BPR loss: 1.3862844705581665 / Matching loss: 2.214504092989955e-05 / Item reconstruction: 0.0009168097749352455 / Text reconstruction: 0.00010912294965237379\n",
            "loss in epoch 8/11 iteration 110/111: 1.3866561651229858 / BPR loss: 1.3861806392669678 / Matching loss: 1.6989757568808272e-05 / Item reconstruction: 0.0007993660401552916 / Text reconstruction: 0.00029411763534881175\n",
            "loss in epoch 8/11 iteration 111/111: 1.3866751194000244 / BPR loss: 1.3862485885620117 / Matching loss: 1.4251545508159325e-05 / Item reconstruction: 0.0006664650281891227 / Text reconstruction: 0.00039531971560791135\n",
            " 80% 8/10 [42:56<10:44, 322.07s/it]loss in epoch 9/11 iteration 0/111: 1.386500358581543 / BPR loss: 1.3859760761260986 / Matching loss: 2.2848731532576494e-05 / Item reconstruction: 0.0009688681457191706 / Text reconstruction: 8.463584526907653e-05\n",
            "loss in epoch 9/11 iteration 1/111: 1.386512041091919 / BPR loss: 1.3859899044036865 / Matching loss: 2.0502884581219405e-05 / Item reconstruction: 0.0009437844855710864 / Text reconstruction: 0.00014823334640823305\n",
            "loss in epoch 9/11 iteration 2/111: 1.3864822387695312 / BPR loss: 1.385949969291687 / Matching loss: 2.2422376787289977e-05 / Item reconstruction: 0.0009511474054306746 / Text reconstruction: 0.00017178323469124734\n",
            "loss in epoch 9/11 iteration 3/111: 1.3866045475006104 / BPR loss: 1.3860498666763306 / Matching loss: 2.3145785235101357e-05 / Item reconstruction: 0.000998566159978509 / Text reconstruction: 0.00016124601825140417\n",
            "loss in epoch 9/11 iteration 4/111: 1.386702060699463 / BPR loss: 1.3861587047576904 / Matching loss: 2.3863402020651847e-05 / Item reconstruction: 0.000984751619398594 / Text reconstruction: 0.00013596637290902436\n",
            "loss in epoch 9/11 iteration 5/111: 1.3865745067596436 / BPR loss: 1.386077642440796 / Matching loss: 2.0868370484095067e-05 / Item reconstruction: 0.0009023952297866344 / Text reconstruction: 0.0001240219862665981\n",
            "loss in epoch 9/11 iteration 6/111: 1.3869106769561768 / BPR loss: 1.3863885402679443 / Matching loss: 2.3394070012727752e-05 / Item reconstruction: 0.0009650316787883639 / Text reconstruction: 8.097387762973085e-05\n",
            "loss in epoch 9/11 iteration 7/111: 1.3867383003234863 / BPR loss: 1.386218547821045 / Matching loss: 2.3062279069563374e-05 / Item reconstruction: 0.000956915901042521 / Text reconstruction: 9.128309466177598e-05\n",
            "loss in epoch 9/11 iteration 8/111: 1.386649489402771 / BPR loss: 1.3861463069915771 / Matching loss: 2.1389891116996296e-05 / Item reconstruction: 0.0009086951613426208 / Text reconstruction: 0.0001375594874843955\n",
            "loss in epoch 9/11 iteration 9/111: 1.3866022825241089 / BPR loss: 1.3860344886779785 / Matching loss: 2.0776071323780343e-05 / Item reconstruction: 0.0009898680727928877 / Text reconstruction: 0.00026047180290333927\n",
            "loss in epoch 9/11 iteration 10/111: 1.3857223987579346 / BPR loss: 1.3849304914474487 / Matching loss: 1.7289838069700636e-05 / Item reconstruction: 0.0012437752448022366 / Text reconstruction: 0.0007633910281583667\n",
            "loss in epoch 9/11 iteration 11/111: 1.385623812675476 / BPR loss: 1.384899616241455 / Matching loss: 1.6597034118603915e-05 / Item reconstruction: 0.0011698872549459338 / Text reconstruction: 0.0006134691648185253\n",
            "loss in epoch 9/11 iteration 12/111: 1.3856128454208374 / BPR loss: 1.3849555253982544 / Matching loss: 1.5637553588021547e-05 / Item reconstruction: 0.001069782068952918 / Text reconstruction: 0.0005338711780495942\n",
            "loss in epoch 9/11 iteration 13/111: 1.3856079578399658 / BPR loss: 1.3850290775299072 / Matching loss: 1.4926434232620522e-05 / Item reconstruction: 0.0009261295781470835 / Text reconstruction: 0.0005049791652709246\n",
            "loss in epoch 9/11 iteration 14/111: 1.385722041130066 / BPR loss: 1.385138988494873 / Matching loss: 1.64612374646822e-05 / Item reconstruction: 0.0009554873686283827 / Text reconstruction: 0.0004441463388502598\n",
            "loss in epoch 9/11 iteration 15/111: 1.385507583618164 / BPR loss: 1.3849536180496216 / Matching loss: 1.4893203115207143e-05 / Item reconstruction: 0.0008863613475114107 / Text reconstruction: 0.0004789285594597459\n",
            "loss in epoch 9/11 iteration 16/111: 1.3861263990402222 / BPR loss: 1.3854286670684814 / Matching loss: 1.8047194316750392e-05 / Item reconstruction: 0.001153530552983284 / Text reconstruction: 0.0005150531651452184\n",
            "loss in epoch 9/11 iteration 17/111: 1.3870437145233154 / BPR loss: 1.3860881328582764 / Matching loss: 2.3809336198610254e-05 / Item reconstruction: 0.0015761274844408035 / Text reconstruction: 0.0007184050045907497\n",
            "loss in epoch 9/11 iteration 18/111: 1.3870759010314941 / BPR loss: 1.3861274719238281 / Matching loss: 2.5023029593285173e-05 / Item reconstruction: 0.0015768921002745628 / Text reconstruction: 0.000674871145747602\n",
            "loss in epoch 9/11 iteration 19/111: 1.3869540691375732 / BPR loss: 1.386098861694336 / Matching loss: 2.5834771804511547e-05 / Item reconstruction: 0.0014518771786242723 / Text reconstruction: 0.0005164755275472999\n",
            "loss in epoch 9/11 iteration 20/111: 1.3868860006332397 / BPR loss: 1.3861531019210815 / Matching loss: 2.405391205684282e-05 / Item reconstruction: 0.0012689847499132156 / Text reconstruction: 0.0003712851903401315\n",
            "loss in epoch 9/11 iteration 21/111: 1.386826753616333 / BPR loss: 1.3861972093582153 / Matching loss: 2.1260104404063895e-05 / Item reconstruction: 0.0010777604766190052 / Text reconstruction: 0.0003474208351690322\n",
            "loss in epoch 9/11 iteration 22/111: 1.3868006467819214 / BPR loss: 1.3862199783325195 / Matching loss: 2.290869269927498e-05 / Item reconstruction: 0.0010177541989833117 / Text reconstruction: 0.00024428937467746437\n",
            "loss in epoch 9/11 iteration 23/111: 1.3867201805114746 / BPR loss: 1.3861712217330933 / Matching loss: 2.201317147410009e-05 / Item reconstruction: 0.0009831252973526716 / Text reconstruction: 0.00017640006262809038\n",
            "loss in epoch 9/11 iteration 24/111: 1.386742115020752 / BPR loss: 1.3862180709838867 / Matching loss: 2.2263215214479715e-05 / Item reconstruction: 0.0009407160105183721 / Text reconstruction: 0.00015667495608795434\n",
            "loss in epoch 9/11 iteration 25/111: 1.3866766691207886 / BPR loss: 1.3861494064331055 / Matching loss: 2.268416210426949e-05 / Item reconstruction: 0.0009379149414598942 / Text reconstruction: 0.00017817752086557448\n",
            "loss in epoch 9/11 iteration 26/111: 1.3866089582443237 / BPR loss: 1.3860268592834473 / Matching loss: 2.436144495732151e-05 / Item reconstruction: 0.001040527131408453 / Text reconstruction: 0.0001874893787316978\n",
            "loss in epoch 9/11 iteration 27/111: 1.3866066932678223 / BPR loss: 1.3860183954238892 / Matching loss: 2.2841919417260215e-05 / Item reconstruction: 0.0010599484667181969 / Text reconstruction: 0.00017697009025141597\n",
            "loss in epoch 9/11 iteration 28/111: 1.3867279291152954 / BPR loss: 1.3861384391784668 / Matching loss: 2.217367000412196e-05 / Item reconstruction: 0.0010476848110556602 / Text reconstruction: 0.0002176011767005548\n",
            "loss in epoch 9/11 iteration 29/111: 1.3868248462677002 / BPR loss: 1.3862009048461914 / Matching loss: 2.4144554117810912e-05 / Item reconstruction: 0.0011083988938480616 / Text reconstruction: 0.00022745307069271803\n",
            "loss in epoch 9/11 iteration 30/111: 1.386784553527832 / BPR loss: 1.386174201965332 / Matching loss: 2.448284976708237e-05 / Item reconstruction: 0.0010891493875533342 / Text reconstruction: 0.00020666234195232391\n",
            "loss in epoch 9/11 iteration 31/111: 1.3868255615234375 / BPR loss: 1.386230707168579 / Matching loss: 2.2715041268384084e-05 / Item reconstruction: 0.0010677773971110582 / Text reconstruction: 0.0001905218668980524\n",
            "loss in epoch 9/11 iteration 32/111: 1.3866465091705322 / BPR loss: 1.3861021995544434 / Matching loss: 2.1884603484068066e-05 / Item reconstruction: 0.0009857697878032923 / Text reconstruction: 0.00014733598800376058\n",
            "loss in epoch 9/11 iteration 33/111: 1.3867266178131104 / BPR loss: 1.386167049407959 / Matching loss: 2.330593633814715e-05 / Item reconstruction: 0.0010147292632609606 / Text reconstruction: 0.00014429686416406184\n",
            "loss in epoch 9/11 iteration 34/111: 1.386552095413208 / BPR loss: 1.385998010635376 / Matching loss: 2.362937266298104e-05 / Item reconstruction: 0.0010059603955596685 / Text reconstruction: 0.00013755513646174222\n",
            "loss in epoch 9/11 iteration 35/111: 1.3866177797317505 / BPR loss: 1.3860979080200195 / Matching loss: 2.213134939665906e-05 / Item reconstruction: 0.0009425452444702387 / Text reconstruction: 0.00013215694343671203\n",
            "loss in epoch 9/11 iteration 36/111: 1.38670814037323 / BPR loss: 1.3861639499664307 / Matching loss: 2.3252610844792798e-05 / Item reconstruction: 0.0009772256016731262 / Text reconstruction: 0.00016164957196451724\n",
            "loss in epoch 9/11 iteration 37/111: 1.3866019248962402 / BPR loss: 1.3860845565795898 / Matching loss: 2.2431506295106374e-05 / Item reconstruction: 0.0009363862918689847 / Text reconstruction: 0.00013412462431006134\n",
            "loss in epoch 9/11 iteration 38/111: 1.3865102529525757 / BPR loss: 1.38596510887146 / Matching loss: 2.3231677914736792e-05 / Item reconstruction: 0.000997646595351398 / Text reconstruction: 0.00011583944433368742\n",
            "loss in epoch 9/11 iteration 39/111: 1.3866455554962158 / BPR loss: 1.3861134052276611 / Matching loss: 2.343725827813614e-05 / Item reconstruction: 0.0009753265185281634 / Text reconstruction: 0.00010507277329452336\n",
            "loss in epoch 9/11 iteration 40/111: 1.386568546295166 / BPR loss: 1.3860561847686768 / Matching loss: 2.1903610104345717e-05 / Item reconstruction: 0.0009350103791803122 / Text reconstruction: 0.00011420744704082608\n",
            "loss in epoch 9/11 iteration 41/111: 1.3865960836410522 / BPR loss: 1.3860805034637451 / Matching loss: 2.1346015273593366e-05 / Item reconstruction: 0.0009434153325855732 / Text reconstruction: 0.00011239643208682537\n",
            "loss in epoch 9/11 iteration 42/111: 1.3868424892425537 / BPR loss: 1.3863162994384766 / Matching loss: 2.171093365177512e-05 / Item reconstruction: 0.0009507642826065421 / Text reconstruction: 0.0001452272990718484\n",
            "loss in epoch 9/11 iteration 43/111: 1.3865975141525269 / BPR loss: 1.3860821723937988 / Matching loss: 2.1091575035825372e-05 / Item reconstruction: 0.0009489870280958712 / Text reconstruction: 9.891174704534933e-05\n",
            "loss in epoch 9/11 iteration 44/111: 1.3867590427398682 / BPR loss: 1.3862217664718628 / Matching loss: 2.2597894712816924e-05 / Item reconstruction: 0.0009905867045745254 / Text reconstruction: 9.684468386694789e-05\n",
            "loss in epoch 9/11 iteration 45/111: 1.3866369724273682 / BPR loss: 1.3861223459243774 / Matching loss: 2.238740853499621e-05 / Item reconstruction: 0.0009403001749888062 / Text reconstruction: 0.00011009033914888278\n",
            "loss in epoch 9/11 iteration 46/111: 1.3866699934005737 / BPR loss: 1.3861570358276367 / Matching loss: 2.2026402803021483e-05 / Item reconstruction: 0.0009443528251722455 / Text reconstruction: 9.343787678517401e-05\n",
            "loss in epoch 9/11 iteration 47/111: 1.3865002393722534 / BPR loss: 1.386005163192749 / Matching loss: 2.258109816466458e-05 / Item reconstruction: 0.0008997545810416341 / Text reconstruction: 0.00011322442151140422\n",
            "loss in epoch 9/11 iteration 48/111: 1.3865501880645752 / BPR loss: 1.3860341310501099 / Matching loss: 2.280609987792559e-05 / Item reconstruction: 0.0009448981145396829 / Text reconstruction: 0.00010417919111205265\n",
            "loss in epoch 9/11 iteration 49/111: 1.3863991498947144 / BPR loss: 1.3858789205551147 / Matching loss: 2.29753914027242e-05 / Item reconstruction: 0.0009557774756103754 / Text reconstruction: 9.654557652538642e-05\n",
            "loss in epoch 9/11 iteration 50/111: 1.386603832244873 / BPR loss: 1.386094093322754 / Matching loss: 2.18066470552003e-05 / Item reconstruction: 0.0009309332235716283 / Text reconstruction: 0.00011229807569179684\n",
            "loss in epoch 9/11 iteration 51/111: 1.3866745233535767 / BPR loss: 1.3861428499221802 / Matching loss: 2.3813903681002557e-05 / Item reconstruction: 0.0009769496973603964 / Text reconstruction: 9.627494728192687e-05\n",
            "loss in epoch 9/11 iteration 52/111: 1.3864998817443848 / BPR loss: 1.385984182357788 / Matching loss: 2.2086722310632467e-05 / Item reconstruction: 0.0009413647931069136 / Text reconstruction: 0.00011482313857413828\n",
            "loss in epoch 9/11 iteration 53/111: 1.3864240646362305 / BPR loss: 1.3859412670135498 / Matching loss: 2.157222479581833e-05 / Item reconstruction: 0.000874210731126368 / Text reconstruction: 0.00012065062765032053\n",
            "loss in epoch 9/11 iteration 54/111: 1.3866255283355713 / BPR loss: 1.3861002922058105 / Matching loss: 2.282566492795013e-05 / Item reconstruction: 0.0009635616443119943 / Text reconstruction: 0.0001038008340401575\n",
            "loss in epoch 9/11 iteration 55/111: 1.3864428997039795 / BPR loss: 1.3859347105026245 / Matching loss: 2.1804968127980828e-05 / Item reconstruction: 0.0009256043704226613 / Text reconstruction: 0.00011782672663684934\n",
            "loss in epoch 9/11 iteration 56/111: 1.3866517543792725 / BPR loss: 1.3861291408538818 / Matching loss: 2.2932017600396648e-05 / Item reconstruction: 0.0009600231423974037 / Text reconstruction: 9.807579044718295e-05\n",
            "loss in epoch 9/11 iteration 57/111: 1.3867310285568237 / BPR loss: 1.3861992359161377 / Matching loss: 2.3101343685993925e-05 / Item reconstruction: 0.0009787962771952152 / Text reconstruction: 9.655437315814197e-05\n",
            "loss in epoch 9/11 iteration 58/111: 1.3864831924438477 / BPR loss: 1.3859894275665283 / Matching loss: 2.1505904442165047e-05 / Item reconstruction: 0.0009009311906993389 / Text reconstruction: 0.00010886922245845199\n",
            "loss in epoch 9/11 iteration 59/111: 1.3866264820098877 / BPR loss: 1.3861162662506104 / Matching loss: 2.2610134692513384e-05 / Item reconstruction: 0.000933870323933661 / Text reconstruction: 0.00010335160186514258\n",
            "loss in epoch 9/11 iteration 60/111: 1.3864693641662598 / BPR loss: 1.3859751224517822 / Matching loss: 2.1997491785441525e-05 / Item reconstruction: 0.0009105438366532326 / Text reconstruction: 8.443003753200173e-05\n",
            "loss in epoch 9/11 iteration 61/111: 1.3865678310394287 / BPR loss: 1.3860300779342651 / Matching loss: 2.2422191250370815e-05 / Item reconstruction: 0.0009772717021405697 / Text reconstruction: 0.00013331149239093065\n",
            "loss in epoch 9/11 iteration 62/111: 1.386577844619751 / BPR loss: 1.3860821723937988 / Matching loss: 2.157506241928786e-05 / Item reconstruction: 0.0009132419945672154 / Text reconstruction: 8.741346391616389e-05\n",
            "loss in epoch 9/11 iteration 63/111: 1.3864961862564087 / BPR loss: 1.3859999179840088 / Matching loss: 2.3083634005161002e-05 / Item reconstruction: 0.0009078899165615439 / Text reconstruction: 9.58328673732467e-05\n",
            "loss in epoch 9/11 iteration 64/111: 1.3865656852722168 / BPR loss: 1.3860657215118408 / Matching loss: 2.145209873560816e-05 / Item reconstruction: 0.0009040458826348186 / Text reconstruction: 0.00013225601287558675\n",
            "loss in epoch 9/11 iteration 65/111: 1.3865379095077515 / BPR loss: 1.3860158920288086 / Matching loss: 2.3507087462348863e-05 / Item reconstruction: 0.0009631424327380955 / Text reconstruction: 8.4719191363547e-05\n",
            "loss in epoch 9/11 iteration 66/111: 1.386591911315918 / BPR loss: 1.3861035108566284 / Matching loss: 2.0600244170054793e-05 / Item reconstruction: 0.0008946487214416265 / Text reconstruction: 0.00010243349242955446\n",
            "loss in epoch 9/11 iteration 67/111: 1.3864763975143433 / BPR loss: 1.3859751224517822 / Matching loss: 2.142909579561092e-05 / Item reconstruction: 0.000922216335311532 / Text reconstruction: 9.385916928295046e-05\n",
            "loss in epoch 9/11 iteration 68/111: 1.3865183591842651 / BPR loss: 1.3860158920288086 / Matching loss: 2.314855737495236e-05 / Item reconstruction: 0.0009292036993429065 / Text reconstruction: 7.36803121981211e-05\n",
            "loss in epoch 9/11 iteration 69/111: 1.3864672183990479 / BPR loss: 1.3859537839889526 / Matching loss: 2.207797660958022e-05 / Item reconstruction: 0.000941406178753823 / Text reconstruction: 0.00010319609282305464\n",
            "loss in epoch 9/11 iteration 70/111: 1.3864375352859497 / BPR loss: 1.3859280347824097 / Matching loss: 2.054811739071738e-05 / Item reconstruction: 0.0009332020999863744 / Text reconstruction: 0.00011179161083418876\n",
            "loss in epoch 9/11 iteration 71/111: 1.3866100311279297 / BPR loss: 1.3861041069030762 / Matching loss: 2.1179970644880086e-05 / Item reconstruction: 0.0009096401045098901 / Text reconstruction: 0.00014973322686273605\n",
            "loss in epoch 9/11 iteration 72/111: 1.3864609003067017 / BPR loss: 1.3859459161758423 / Matching loss: 2.2589543732465245e-05 / Item reconstruction: 0.0009438402485102415 / Text reconstruction: 0.00010238910908810794\n",
            "loss in epoch 9/11 iteration 73/111: 1.3865183591842651 / BPR loss: 1.3860464096069336 / Matching loss: 2.1311203454388306e-05 / Item reconstruction: 0.0008684846106916666 / Text reconstruction: 8.170726505341008e-05\n",
            "loss in epoch 9/11 iteration 74/111: 1.3864388465881348 / BPR loss: 1.3859155178070068 / Matching loss: 2.1528479919652455e-05 / Item reconstruction: 0.0009326547151431441 / Text reconstruction: 0.000176894900505431\n",
            "loss in epoch 9/11 iteration 75/111: 1.386431097984314 / BPR loss: 1.3859137296676636 / Matching loss: 2.288322320964653e-05 / Item reconstruction: 0.0009486238704994321 / Text reconstruction: 0.00010052865400211886\n",
            "loss in epoch 9/11 iteration 76/111: 1.3866022825241089 / BPR loss: 1.3860883712768555 / Matching loss: 2.317985490662977e-05 / Item reconstruction: 0.0009326295694336295 / Text reconstruction: 0.00012207466352265328\n",
            "loss in epoch 9/11 iteration 77/111: 1.386570930480957 / BPR loss: 1.386075735092163 / Matching loss: 2.101807331200689e-05 / Item reconstruction: 0.0009087136713787913 / Text reconstruction: 9.975249122362584e-05\n",
            "loss in epoch 9/11 iteration 78/111: 1.3866127729415894 / BPR loss: 1.3861080408096313 / Matching loss: 2.3217602574732155e-05 / Item reconstruction: 0.0009224117966368794 / Text reconstruction: 0.00010147818102268502\n",
            "loss in epoch 9/11 iteration 79/111: 1.386697769165039 / BPR loss: 1.3861899375915527 / Matching loss: 2.2465337679022923e-05 / Item reconstruction: 0.0009234155877493322 / Text reconstruction: 0.00011848448775708675\n",
            "loss in epoch 9/11 iteration 80/111: 1.3866372108459473 / BPR loss: 1.3861500024795532 / Matching loss: 2.1378649762482382e-05 / Item reconstruction: 0.0008963465224951506 / Text reconstruction: 8.807457197690383e-05\n",
            "loss in epoch 9/11 iteration 81/111: 1.3863732814788818 / BPR loss: 1.3858758211135864 / Matching loss: 2.1707313862862065e-05 / Item reconstruction: 0.0009049436775967479 / Text reconstruction: 0.00011646241182461381\n",
            "loss in epoch 9/11 iteration 82/111: 1.3865324258804321 / BPR loss: 1.3860137462615967 / Matching loss: 2.1746331185568124e-05 / Item reconstruction: 0.0009400974377058446 / Text reconstruction: 0.00013455160660669208\n",
            "loss in epoch 9/11 iteration 83/111: 1.3868467807769775 / BPR loss: 1.3863415718078613 / Matching loss: 2.2649108359473757e-05 / Item reconstruction: 0.0009249465074390173 / Text reconstruction: 0.00010002007911680266\n",
            "loss in epoch 9/11 iteration 84/111: 1.386582612991333 / BPR loss: 1.3860843181610107 / Matching loss: 2.1162426492082886e-05 / Item reconstruction: 0.0008830963051877916 / Text reconstruction: 0.0001777754514478147\n",
            "loss in epoch 9/11 iteration 85/111: 1.3866034746170044 / BPR loss: 1.3860867023468018 / Matching loss: 2.290297743456904e-05 / Item reconstruction: 0.0009462888119742274 / Text reconstruction: 0.00010366742208134383\n",
            "loss in epoch 9/11 iteration 86/111: 1.3864413499832153 / BPR loss: 1.3859440088272095 / Matching loss: 2.247043084935285e-05 / Item reconstruction: 0.0008999918354675174 / Text reconstruction: 0.00012427600449882448\n",
            "loss in epoch 9/11 iteration 87/111: 1.3865278959274292 / BPR loss: 1.3860199451446533 / Matching loss: 2.2251730115385726e-05 / Item reconstruction: 0.0009339685784652829 / Text reconstruction: 9.364684228785336e-05\n",
            "loss in epoch 9/11 iteration 88/111: 1.3864761590957642 / BPR loss: 1.385990858078003 / Matching loss: 2.0052983018103987e-05 / Item reconstruction: 0.0008660323219373822 / Text reconstruction: 0.00016169168520718813\n",
            "loss in epoch 9/11 iteration 89/111: 1.386661171913147 / BPR loss: 1.3861446380615234 / Matching loss: 2.276035229442641e-05 / Item reconstruction: 0.0009518422884866595 / Text reconstruction: 8.914250793168321e-05\n",
            "loss in epoch 9/11 iteration 90/111: 1.3865282535552979 / BPR loss: 1.386031150817871 / Matching loss: 2.0866096747340634e-05 / Item reconstruction: 0.0008943055872805417 / Text reconstruction: 0.00014570541679859161\n",
            "loss in epoch 9/11 iteration 91/111: 1.3865243196487427 / BPR loss: 1.3860371112823486 / Matching loss: 2.1385667423601262e-05 / Item reconstruction: 0.0008916688966564834 / Text reconstruction: 0.00010020494664786384\n",
            "loss in epoch 9/11 iteration 92/111: 1.38654625415802 / BPR loss: 1.3860517740249634 / Matching loss: 2.23629322135821e-05 / Item reconstruction: 0.0008929235627874732 / Text reconstruction: 0.0001279577991226688\n",
            "loss in epoch 9/11 iteration 93/111: 1.3865940570831299 / BPR loss: 1.3860840797424316 / Matching loss: 2.1931922674411908e-05 / Item reconstruction: 0.0009328057058155537 / Text reconstruction: 0.00010827009828062728\n",
            "loss in epoch 9/11 iteration 94/111: 1.38653564453125 / BPR loss: 1.3860502243041992 / Matching loss: 2.2122098016552627e-05 / Item reconstruction: 0.0008860675734467804 / Text reconstruction: 0.00010130209557246417\n",
            "loss in epoch 9/11 iteration 95/111: 1.38651442527771 / BPR loss: 1.3860011100769043 / Matching loss: 2.2338004782795906e-05 / Item reconstruction: 0.0009237710037268698 / Text reconstruction: 0.00014523640857078135\n",
            "loss in epoch 9/11 iteration 96/111: 1.3867080211639404 / BPR loss: 1.3862156867980957 / Matching loss: 2.1610967451124452e-05 / Item reconstruction: 0.0008977634133771062 / Text reconstruction: 0.00010983036190737039\n",
            "loss in epoch 9/11 iteration 97/111: 1.3865830898284912 / BPR loss: 1.3860960006713867 / Matching loss: 2.1598840248771012e-05 / Item reconstruction: 0.0008762943325564265 / Text reconstruction: 0.00013693561777472496\n",
            "loss in epoch 9/11 iteration 98/111: 1.3867909908294678 / BPR loss: 1.3863122463226318 / Matching loss: 2.2743719455320388e-05 / Item reconstruction: 0.000869827694259584 / Text reconstruction: 0.00010554826440056786\n",
            "loss in epoch 9/11 iteration 99/111: 1.386644959449768 / BPR loss: 1.386130452156067 / Matching loss: 2.1968764485791326e-05 / Item reconstruction: 0.0009429143974557519 / Text reconstruction: 0.00010569042933639139\n",
            "loss in epoch 9/11 iteration 100/111: 1.3866491317749023 / BPR loss: 1.3861569166183472 / Matching loss: 2.2628766600973904e-05 / Item reconstruction: 0.0009043896570801735 / Text reconstruction: 8.69235082063824e-05\n",
            "loss in epoch 9/11 iteration 101/111: 1.3864741325378418 / BPR loss: 1.3859705924987793 / Matching loss: 2.1613699573208578e-05 / Item reconstruction: 0.0009080056915991008 / Text reconstruction: 0.00013984087854623795\n",
            "loss in epoch 9/11 iteration 102/111: 1.3864678144454956 / BPR loss: 1.3859448432922363 / Matching loss: 2.1868116164114326e-05 / Item reconstruction: 0.0009588778484612703 / Text reconstruction: 0.00010854085849132389\n",
            "loss in epoch 9/11 iteration 103/111: 1.3866418600082397 / BPR loss: 1.386142373085022 / Matching loss: 2.1398820535978302e-05 / Item reconstruction: 0.0009035489638336003 / Text reconstruction: 0.0001313110551564023\n",
            "loss in epoch 9/11 iteration 104/111: 1.3866157531738281 / BPR loss: 1.3861172199249268 / Matching loss: 2.057395613519475e-05 / Item reconstruction: 0.000905639142729342 / Text reconstruction: 0.0001251222420250997\n",
            "loss in epoch 9/11 iteration 105/111: 1.386447787284851 / BPR loss: 1.3859586715698242 / Matching loss: 2.1701358491554856e-05 / Item reconstruction: 0.0008921478874981403 / Text reconstruction: 0.000106425752164796\n",
            "loss in epoch 9/11 iteration 106/111: 1.3866075277328491 / BPR loss: 1.3860936164855957 / Matching loss: 2.2656946384813637e-05 / Item reconstruction: 0.0009336998919025064 / Text reconstruction: 0.0001224143779836595\n",
            "loss in epoch 9/11 iteration 107/111: 1.3865779638290405 / BPR loss: 1.3860852718353271 / Matching loss: 2.15024938370334e-05 / Item reconstruction: 0.0008989938069134951 / Text reconstruction: 0.00010818432201631367\n",
            "loss in epoch 9/11 iteration 108/111: 1.3865834474563599 / BPR loss: 1.3860912322998047 / Matching loss: 2.1418903997982852e-05 / Item reconstruction: 0.0009029519860632718 / Text reconstruction: 9.656565089244395e-05\n",
            "loss in epoch 9/11 iteration 109/111: 1.3868743181228638 / BPR loss: 1.3863717317581177 / Matching loss: 2.1930978618911467e-05 / Item reconstruction: 0.0009102234034799039 / Text reconstruction: 0.00012766008148901165\n",
            "loss in epoch 9/11 iteration 110/111: 1.3867073059082031 / BPR loss: 1.3862347602844238 / Matching loss: 1.8248292690259404e-05 / Item reconstruction: 0.0007882493082433939 / Text reconstruction: 0.000300902669550851\n",
            "loss in epoch 9/11 iteration 111/111: 1.3866971731185913 / BPR loss: 1.386265516281128 / Matching loss: 1.4979828847572207e-05 / Item reconstruction: 0.0006747597362846136 / Text reconstruction: 0.00039609026862308383\n",
            " 90% 9/10 [48:13<05:20, 320.66s/it]loss in epoch 10/11 iteration 0/111: 1.3865913152694702 / BPR loss: 1.3860787153244019 / Matching loss: 2.2111344151198864e-05 / Item reconstruction: 0.0009345388389192522 / Text reconstruction: 0.00011595064279390499\n",
            "loss in epoch 10/11 iteration 1/111: 1.3864368200302124 / BPR loss: 1.3859169483184814 / Matching loss: 2.0652763851103373e-05 / Item reconstruction: 0.000922147766686976 / Text reconstruction: 0.00019076626631431282\n",
            "loss in epoch 10/11 iteration 2/111: 1.386469841003418 / BPR loss: 1.3859105110168457 / Matching loss: 2.2916816305951215e-05 / Item reconstruction: 0.0009918229188770056 / Text reconstruction: 0.00020282200421206653\n",
            "loss in epoch 10/11 iteration 3/111: 1.3865398168563843 / BPR loss: 1.385988712310791 / Matching loss: 2.3051679818308912e-05 / Item reconstruction: 0.0009845900349318981 / Text reconstruction: 0.00017893676704261452\n",
            "loss in epoch 10/11 iteration 4/111: 1.3865619897842407 / BPR loss: 1.386014699935913 / Matching loss: 2.357141420361586e-05 / Item reconstruction: 0.0009871459333226085 / Text reconstruction: 0.00015084717597346753\n",
            "loss in epoch 10/11 iteration 5/111: 1.386725664138794 / BPR loss: 1.3862007856369019 / Matching loss: 2.2202817490324378e-05 / Item reconstruction: 0.0009475954575464129 / Text reconstruction: 0.0001443171058781445\n",
            "loss in epoch 10/11 iteration 6/111: 1.3867483139038086 / BPR loss: 1.3862384557724 / Matching loss: 2.2743844965589233e-05 / Item reconstruction: 0.0009372865315526724 / Text reconstruction: 9.23893167055212e-05\n",
            "loss in epoch 10/11 iteration 7/111: 1.3867939710617065 / BPR loss: 1.3862731456756592 / Matching loss: 2.383730316068977e-05 / Item reconstruction: 0.0009476556442677975 / Text reconstruction: 0.00011557140533113852\n",
            "loss in epoch 10/11 iteration 8/111: 1.386672019958496 / BPR loss: 1.3861680030822754 / Matching loss: 2.1917352569289505e-05 / Item reconstruction: 0.0008982157451100647 / Text reconstruction: 0.0001650094782235101\n",
            "loss in epoch 10/11 iteration 9/111: 1.386609673500061 / BPR loss: 1.3860057592391968 / Matching loss: 2.1337960788514465e-05 / Item reconstruction: 0.001034060027450323 / Text reconstruction: 0.0003276641364209354\n",
            "loss in epoch 10/11 iteration 10/111: 1.3856227397918701 / BPR loss: 1.3847877979278564 / Matching loss: 1.7013006072374992e-05 / Item reconstruction: 0.0012319982051849365 / Text reconstruction: 0.001009963103570044\n",
            "loss in epoch 10/11 iteration 11/111: 1.3855496644973755 / BPR loss: 1.3848220109939575 / Matching loss: 1.5596981029375456e-05 / Item reconstruction: 0.0011304578511044383 / Text reconstruction: 0.0007342444732785225\n",
            "loss in epoch 10/11 iteration 12/111: 1.3853411674499512 / BPR loss: 1.3846979141235352 / Matching loss: 1.6301730283885263e-05 / Item reconstruction: 0.0010205598082393408 / Text reconstruction: 0.0005828917492181063\n",
            "loss in epoch 10/11 iteration 13/111: 1.3853498697280884 / BPR loss: 1.384767770767212 / Matching loss: 1.431041891919449e-05 / Item reconstruction: 0.0009240139042958617 / Text reconstruction: 0.0005288413958624005\n",
            "loss in epoch 10/11 iteration 14/111: 1.385494351387024 / BPR loss: 1.3849074840545654 / Matching loss: 1.7227750504389405e-05 / Item reconstruction: 0.0009470597724430263 / Text reconstruction: 0.00048027673619799316\n",
            "loss in epoch 10/11 iteration 15/111: 1.3851356506347656 / BPR loss: 1.384574055671692 / Matching loss: 1.4954945982026402e-05 / Item reconstruction: 0.0008892510668374598 / Text reconstruction: 0.000510502839460969\n",
            "loss in epoch 10/11 iteration 16/111: 1.3860223293304443 / BPR loss: 1.3853237628936768 / Matching loss: 1.760276427376084e-05 / Item reconstruction: 0.0011390686267986894 / Text reconstruction: 0.0005569953937083483\n",
            "loss in epoch 10/11 iteration 17/111: 1.387064814567566 / BPR loss: 1.3860702514648438 / Matching loss: 2.3705062631051987e-05 / Item reconstruction: 0.0016149879666045308 / Text reconstruction: 0.0008168653002940118\n",
            "loss in epoch 10/11 iteration 18/111: 1.3870922327041626 / BPR loss: 1.386154294013977 / Matching loss: 2.274257712997496e-05 / Item reconstruction: 0.0015436585526913404 / Text reconstruction: 0.0007165102288126945\n",
            "loss in epoch 10/11 iteration 19/111: 1.3869436979293823 / BPR loss: 1.3861067295074463 / Matching loss: 2.537827822379768e-05 / Item reconstruction: 0.0014105651061981916 / Text reconstruction: 0.0005315490998327732\n",
            "loss in epoch 10/11 iteration 20/111: 1.3867584466934204 / BPR loss: 1.3860290050506592 / Matching loss: 2.3952496121637523e-05 / Item reconstruction: 0.001248922199010849 / Text reconstruction: 0.00040532753337174654\n",
            "loss in epoch 10/11 iteration 21/111: 1.3868765830993652 / BPR loss: 1.3862287998199463 / Matching loss: 2.138407580787316e-05 / Item reconstruction: 0.0010920981876552105 / Text reconstruction: 0.00040172020089812577\n",
            "loss in epoch 10/11 iteration 22/111: 1.3866370916366577 / BPR loss: 1.3860423564910889 / Matching loss: 2.4772874894551933e-05 / Item reconstruction: 0.001012379303574562 / Text reconstruction: 0.00031917885644361377\n",
            "loss in epoch 10/11 iteration 23/111: 1.3866307735443115 / BPR loss: 1.3860890865325928 / Matching loss: 2.1742987883044407e-05 / Item reconstruction: 0.0009570077527314425 / Text reconstruction: 0.0002076890377793461\n",
            "loss in epoch 10/11 iteration 24/111: 1.3865870237350464 / BPR loss: 1.3860653638839722 / Matching loss: 2.2883852579980157e-05 / Item reconstruction: 0.0009280198719352484 / Text reconstruction: 0.00017401788500137627\n",
            "loss in epoch 10/11 iteration 25/111: 1.386763095855713 / BPR loss: 1.3862228393554688 / Matching loss: 2.2702140995534137e-05 / Item reconstruction: 0.0009637291077524424 / Text reconstruction: 0.00017879714141599834\n",
            "loss in epoch 10/11 iteration 26/111: 1.3866660594940186 / BPR loss: 1.3860924243927002 / Matching loss: 2.415806011413224e-05 / Item reconstruction: 0.0010169872548431158 / Text reconstruction: 0.00020415600738488138\n",
            "loss in epoch 10/11 iteration 27/111: 1.3866537809371948 / BPR loss: 1.3860752582550049 / Matching loss: 2.1972939066472463e-05 / Item reconstruction: 0.0010229661129415035 / Text reconstruction: 0.0002254491118947044\n",
            "loss in epoch 10/11 iteration 28/111: 1.3868176937103271 / BPR loss: 1.3862273693084717 / Matching loss: 2.225857497251127e-05 / Item reconstruction: 0.0010415981523692608 / Text reconstruction: 0.00023623013112228364\n",
            "loss in epoch 10/11 iteration 29/111: 1.3867218494415283 / BPR loss: 1.3860931396484375 / Matching loss: 2.3669876100029796e-05 / Item reconstruction: 0.0011136329267174006 / Text reconstruction: 0.0002408397413091734\n",
            "loss in epoch 10/11 iteration 30/111: 1.3868154287338257 / BPR loss: 1.386202096939087 / Matching loss: 2.4570070308982395e-05 / Item reconstruction: 0.00109586538746953 / Text reconstruction: 0.0002044835564447567\n",
            "loss in epoch 10/11 iteration 31/111: 1.386762022972107 / BPR loss: 1.3861584663391113 / Matching loss: 2.3162996512837708e-05 / Item reconstruction: 0.00108958943746984 / Text reconstruction: 0.00017795928579289466\n",
            "loss in epoch 10/11 iteration 32/111: 1.386733055114746 / BPR loss: 1.3861618041992188 / Matching loss: 2.3290180251933634e-05 / Item reconstruction: 0.001029694452881813 / Text reconstruction: 0.00016576724010519683\n",
            "loss in epoch 10/11 iteration 33/111: 1.3867323398590088 / BPR loss: 1.3861656188964844 / Matching loss: 2.2728600015398115e-05 / Item reconstruction: 0.0010137828066945076 / Text reconstruction: 0.00018550537060946226\n",
            "loss in epoch 10/11 iteration 34/111: 1.3865478038787842 / BPR loss: 1.3859918117523193 / Matching loss: 2.3143371436162852e-05 / Item reconstruction: 0.000991742592304945 / Text reconstruction: 0.00018480222206562757\n",
            "loss in epoch 10/11 iteration 35/111: 1.3866240978240967 / BPR loss: 1.3860938549041748 / Matching loss: 2.310208947164938e-05 / Item reconstruction: 0.0009550275281071663 / Text reconstruction: 0.0001476593897677958\n",
            "loss in epoch 10/11 iteration 36/111: 1.3866610527038574 / BPR loss: 1.3861172199249268 / Matching loss: 2.1980469682603143e-05 / Item reconstruction: 0.0009701322996988893 / Text reconstruction: 0.0001843457721406594\n",
            "loss in epoch 10/11 iteration 37/111: 1.3865007162094116 / BPR loss: 1.3859705924987793 / Matching loss: 2.293076613568701e-05 / Item reconstruction: 0.0009487785864621401 / Text reconstruction: 0.0001645224547246471\n",
            "loss in epoch 10/11 iteration 38/111: 1.3865731954574585 / BPR loss: 1.3860387802124023 / Matching loss: 2.3372751456918195e-05 / Item reconstruction: 0.0009565174696035683 / Text reconstruction: 0.00016380561282858253\n",
            "loss in epoch 10/11 iteration 39/111: 1.3866305351257324 / BPR loss: 1.386124610900879 / Matching loss: 2.1432006178656593e-05 / Item reconstruction: 0.0009205052629113197 / Text reconstruction: 0.00012091617827536538\n",
            "loss in epoch 10/11 iteration 40/111: 1.3864805698394775 / BPR loss: 1.3859608173370361 / Matching loss: 2.1917099729762413e-05 / Item reconstruction: 0.0009404311422258615 / Text reconstruction: 0.0001384304923703894\n",
            "loss in epoch 10/11 iteration 41/111: 1.3864564895629883 / BPR loss: 1.3859524726867676 / Matching loss: 2.1080886654090136e-05 / Item reconstruction: 0.0009093328844755888 / Text reconstruction: 0.00014109602489043027\n",
            "loss in epoch 10/11 iteration 42/111: 1.386709213256836 / BPR loss: 1.3861806392669678 / Matching loss: 2.134647911589127e-05 / Item reconstruction: 0.00094696314772591 / Text reconstruction: 0.00016848280210979283\n",
            "loss in epoch 10/11 iteration 43/111: 1.3866978883743286 / BPR loss: 1.386167049407959 / Matching loss: 2.1659292542608455e-05 / Item reconstruction: 0.0009672186570242047 / Text reconstruction: 0.00012745113053824753\n",
            "loss in epoch 10/11 iteration 44/111: 1.386695384979248 / BPR loss: 1.3861666917800903 / Matching loss: 2.1244170056888834e-05 / Item reconstruction: 0.0009663886157795787 / Text reconstruction: 0.0001215083320857957\n",
            "loss in epoch 10/11 iteration 45/111: 1.3865675926208496 / BPR loss: 1.3860714435577393 / Matching loss: 2.035623401752673e-05 / Item reconstruction: 0.000906379078514874 / Text reconstruction: 0.00011282200284767896\n",
            "loss in epoch 10/11 iteration 46/111: 1.3866047859191895 / BPR loss: 1.3861033916473389 / Matching loss: 2.1445457605295815e-05 / Item reconstruction: 0.0009205255191773176 / Text reconstruction: 9.829459304455668e-05\n",
            "loss in epoch 10/11 iteration 47/111: 1.3864235877990723 / BPR loss: 1.385922908782959 / Matching loss: 2.2051885025575757e-05 / Item reconstruction: 0.0009065965423360467 / Text reconstruction: 0.00012661598157137632\n",
            "loss in epoch 10/11 iteration 48/111: 1.386568307876587 / BPR loss: 1.3860446214675903 / Matching loss: 2.2777743652113713e-05 / Item reconstruction: 0.0009590013069100678 / Text reconstruction: 0.00010702406871132553\n",
            "loss in epoch 10/11 iteration 49/111: 1.3864542245864868 / BPR loss: 1.385944128036499 / Matching loss: 2.174075234506745e-05 / Item reconstruction: 0.0009382445132359862 / Text reconstruction: 9.666338155511767e-05\n",
            "loss in epoch 10/11 iteration 50/111: 1.3865911960601807 / BPR loss: 1.3860584497451782 / Matching loss: 2.234220301033929e-05 / Item reconstruction: 0.0009647032711654902 / Text reconstruction: 0.00014067435404285789\n",
            "loss in epoch 10/11 iteration 51/111: 1.3867285251617432 / BPR loss: 1.3862242698669434 / Matching loss: 2.271741868753452e-05 / Item reconstruction: 0.0009229981806129217 / Text reconstruction: 9.99014446279034e-05\n",
            "loss in epoch 10/11 iteration 52/111: 1.3864353895187378 / BPR loss: 1.3859329223632812 / Matching loss: 2.0434861653484404e-05 / Item reconstruction: 0.0009166522650048137 / Text reconstruction: 0.0001188261085189879\n",
            "loss in epoch 10/11 iteration 53/111: 1.386307954788208 / BPR loss: 1.385831594467163 / Matching loss: 2.1124731574673206e-05 / Item reconstruction: 0.0008550903876312077 / Text reconstruction: 0.0001380420580971986\n",
            "loss in epoch 10/11 iteration 54/111: 1.3866004943847656 / BPR loss: 1.3860905170440674 / Matching loss: 2.2346575860865414e-05 / Item reconstruction: 0.0009338747477158904 / Text reconstruction: 0.00010357300925534219\n",
            "loss in epoch 10/11 iteration 55/111: 1.3863747119903564 / BPR loss: 1.3858616352081299 / Matching loss: 2.139599200745579e-05 / Item reconstruction: 0.0009303586557507515 / Text reconstruction: 0.0001326482160948217\n",
            "loss in epoch 10/11 iteration 56/111: 1.3866512775421143 / BPR loss: 1.3861112594604492 / Matching loss: 2.430700624245219e-05 / Item reconstruction: 0.000988053623586893 / Text reconstruction: 0.00010876434680540115\n",
            "loss in epoch 10/11 iteration 57/111: 1.3865269422531128 / BPR loss: 1.3860125541687012 / Matching loss: 2.2981479560257867e-05 / Item reconstruction: 0.0009365351870656013 / Text reconstruction: 0.00011574817472137511\n",
            "loss in epoch 10/11 iteration 58/111: 1.386596918106079 / BPR loss: 1.386082410812378 / Matching loss: 2.1614438082906418e-05 / Item reconstruction: 0.0009344159043394029 / Text reconstruction: 0.00012896816770080477\n",
            "loss in epoch 10/11 iteration 59/111: 1.3866114616394043 / BPR loss: 1.386112928390503 / Matching loss: 2.02734736376442e-05 / Item reconstruction: 0.0009097419679164886 / Text reconstruction: 0.00011701628682203591\n",
            "loss in epoch 10/11 iteration 60/111: 1.3863582611083984 / BPR loss: 1.3858623504638672 / Matching loss: 2.2317888578982092e-05 / Item reconstruction: 0.000905526801943779 / Text reconstruction: 0.00010459666373208165\n",
            "loss in epoch 10/11 iteration 61/111: 1.3864446878433228 / BPR loss: 1.3859065771102905 / Matching loss: 2.2207444999367e-05 / Item reconstruction: 0.0009707912104204297 / Text reconstruction: 0.00015235136379487813\n",
            "loss in epoch 10/11 iteration 62/111: 1.3865715265274048 / BPR loss: 1.38607656955719 / Matching loss: 2.1353811462176964e-05 / Item reconstruction: 0.0009050527587532997 / Text reconstruction: 0.00010555465269135311\n",
            "loss in epoch 10/11 iteration 63/111: 1.3864905834197998 / BPR loss: 1.3859965801239014 / Matching loss: 2.1953404939267784e-05 / Item reconstruction: 0.0009004350868053734 / Text reconstruction: 0.00010896580351982266\n",
            "loss in epoch 10/11 iteration 64/111: 1.386517882347107 / BPR loss: 1.3860365152359009 / Matching loss: 1.9344277461641468e-05 / Item reconstruction: 0.0008655783021822572 / Text reconstruction: 0.00014667067443951964\n",
            "loss in epoch 10/11 iteration 65/111: 1.3863133192062378 / BPR loss: 1.3858098983764648 / Matching loss: 2.228435914730653e-05 / Item reconstruction: 0.0009156226878985763 / Text reconstruction: 0.0001167400914710015\n",
            "loss in epoch 10/11 iteration 66/111: 1.3865998983383179 / BPR loss: 1.3861186504364014 / Matching loss: 1.963275281013921e-05 / Item reconstruction: 0.0008767468389123678 / Text reconstruction: 0.00011613210517680272\n",
            "loss in epoch 10/11 iteration 67/111: 1.386538028717041 / BPR loss: 1.3860092163085938 / Matching loss: 2.162241071346216e-05 / Item reconstruction: 0.0009580075275152922 / Text reconstruction: 0.0001412570127286017\n",
            "loss in epoch 10/11 iteration 68/111: 1.3864935636520386 / BPR loss: 1.3859820365905762 / Matching loss: 2.281140405102633e-05 / Item reconstruction: 0.000936947064474225 / Text reconstruction: 0.00010114253382198513\n",
            "loss in epoch 10/11 iteration 69/111: 1.3865652084350586 / BPR loss: 1.3860583305358887 / Matching loss: 2.230386235169135e-05 / Item reconstruction: 0.0009116617729887366 / Text reconstruction: 0.00014363994705490768\n",
            "loss in epoch 10/11 iteration 70/111: 1.386409044265747 / BPR loss: 1.3859202861785889 / Matching loss: 2.080514059343841e-05 / Item reconstruction: 0.0008774022571742535 / Text reconstruction: 0.0001462319487472996\n",
            "loss in epoch 10/11 iteration 71/111: 1.3866748809814453 / BPR loss: 1.3861675262451172 / Matching loss: 2.025252251769416e-05 / Item reconstruction: 0.0008990697679109871 / Text reconstruction: 0.00018781839753501117\n",
            "loss in epoch 10/11 iteration 72/111: 1.3864015340805054 / BPR loss: 1.3859007358551025 / Matching loss: 2.2092983272159472e-05 / Item reconstruction: 0.0009115362190641463 / Text reconstruction: 0.00011526871821843088\n",
            "loss in epoch 10/11 iteration 73/111: 1.3865238428115845 / BPR loss: 1.3860489130020142 / Matching loss: 2.1781321265734732e-05 / Item reconstruction: 0.0008672494441270828 / Text reconstruction: 9.71214467426762e-05\n",
            "loss in epoch 10/11 iteration 74/111: 1.3864094018936157 / BPR loss: 1.3858739137649536 / Matching loss: 2.1832413040101528e-05 / Item reconstruction: 0.0009600118501111865 / Text reconstruction: 0.0001678679109318182\n",
            "loss in epoch 10/11 iteration 75/111: 1.386584758758545 / BPR loss: 1.3860855102539062 / Matching loss: 2.118491283908952e-05 / Item reconstruction: 0.000909527181647718 / Text reconstruction: 0.00011627167259575799\n",
            "loss in epoch 10/11 iteration 76/111: 1.3865214586257935 / BPR loss: 1.3860074281692505 / Matching loss: 2.2194213670445606e-05 / Item reconstruction: 0.0009322370169684291 / Text reconstruction: 0.0001289714127779007\n",
            "loss in epoch 10/11 iteration 77/111: 1.3864853382110596 / BPR loss: 1.3859962224960327 / Matching loss: 2.081437924061902e-05 / Item reconstruction: 0.0008927790331654251 / Text reconstruction: 0.00010878976172534749\n",
            "loss in epoch 10/11 iteration 78/111: 1.3866833448410034 / BPR loss: 1.3861758708953857 / Matching loss: 2.2373853425960988e-05 / Item reconstruction: 0.0009224905516020954 / Text reconstruction: 0.00011918396194232628\n",
            "loss in epoch 10/11 iteration 79/111: 1.386561393737793 / BPR loss: 1.3860599994659424 / Matching loss: 2.146394399460405e-05 / Item reconstruction: 0.0009033262031152844 / Text reconstruction: 0.00014140903658699244\n",
            "loss in epoch 10/11 iteration 80/111: 1.386599063873291 / BPR loss: 1.3860944509506226 / Matching loss: 2.227819641120732e-05 / Item reconstruction: 0.0009193909936584532 / Text reconstruction: 0.00011353039008099586\n",
            "loss in epoch 10/11 iteration 81/111: 1.386391043663025 / BPR loss: 1.3858838081359863 / Matching loss: 2.098398545058444e-05 / Item reconstruction: 0.0009108338272199035 / Text reconstruction: 0.0001544331171317026\n",
            "loss in epoch 10/11 iteration 82/111: 1.386248230934143 / BPR loss: 1.3857457637786865 / Matching loss: 2.1932042727712542e-05 / Item reconstruction: 0.0009000913705676794 / Text reconstruction: 0.00015235482715070248\n",
            "loss in epoch 10/11 iteration 83/111: 1.386674404144287 / BPR loss: 1.3861658573150635 / Matching loss: 2.3033666366245598e-05 / Item reconstruction: 0.0009312478359788656 / Text reconstruction: 9.962447074940428e-05\n",
            "loss in epoch 10/11 iteration 84/111: 1.3863918781280518 / BPR loss: 1.3858956098556519 / Matching loss: 1.9580882508307695e-05 / Item reconstruction: 0.0008741173660382628 / Text reconstruction: 0.00019827080541290343\n",
            "loss in epoch 10/11 iteration 85/111: 1.3864085674285889 / BPR loss: 1.3858909606933594 / Matching loss: 2.264790964545682e-05 / Item reconstruction: 0.0009357386734336615 / Text reconstruction: 0.0001352296385448426\n",
            "loss in epoch 10/11 iteration 86/111: 1.3863592147827148 / BPR loss: 1.3858368396759033 / Matching loss: 2.2596139388042502e-05 / Item reconstruction: 0.0009380643023177981 / Text reconstruction: 0.00015331912436522543\n",
            "loss in epoch 10/11 iteration 87/111: 1.3865236043930054 / BPR loss: 1.3860201835632324 / Matching loss: 2.1918156562605873e-05 / Item reconstruction: 0.0009063238394446671 / Text reconstruction: 0.0001416726445313543\n",
            "loss in epoch 10/11 iteration 88/111: 1.3865339756011963 / BPR loss: 1.386005163192749 / Matching loss: 2.3361593775916845e-05 / Item reconstruction: 0.000905198510736227 / Text reconstruction: 0.0002639617305248976\n",
            "loss in epoch 10/11 iteration 89/111: 1.3865658044815063 / BPR loss: 1.3860665559768677 / Matching loss: 2.0915817003697157e-05 / Item reconstruction: 0.0009171689162030816 / Text reconstruction: 9.905197657644749e-05\n",
            "loss in epoch 10/11 iteration 90/111: 1.386458158493042 / BPR loss: 1.3859528303146362 / Matching loss: 2.1722606106777675e-05 / Item reconstruction: 0.0008968162583187222 / Text reconstruction: 0.00017606737674213946\n",
            "loss in epoch 10/11 iteration 91/111: 1.3865212202072144 / BPR loss: 1.3860223293304443 / Matching loss: 2.2885928046889603e-05 / Item reconstruction: 0.0008952554780989885 / Text reconstruction: 0.00014191883383318782\n",
            "loss in epoch 10/11 iteration 92/111: 1.3865097761154175 / BPR loss: 1.3860149383544922 / Matching loss: 2.120171848218888e-05 / Item reconstruction: 0.0008913620840758085 / Text reconstruction: 0.0001394821156281978\n",
            "loss in epoch 10/11 iteration 93/111: 1.386427402496338 / BPR loss: 1.3859068155288696 / Matching loss: 2.3614062229171395e-05 / Item reconstruction: 0.000938410172238946 / Text reconstruction: 0.0001388800737913698\n",
            "loss in epoch 10/11 iteration 94/111: 1.386658787727356 / BPR loss: 1.386155366897583 / Matching loss: 2.2356789486366324e-05 / Item reconstruction: 0.0009148758254013956 / Text reconstruction: 0.00011817050108220428\n",
            "loss in epoch 10/11 iteration 95/111: 1.3864175081253052 / BPR loss: 1.3859100341796875 / Matching loss: 2.0846224288106896e-05 / Item reconstruction: 0.0009140935144387186 / Text reconstruction: 0.0001480931678088382\n",
            "loss in epoch 10/11 iteration 96/111: 1.3866459131240845 / BPR loss: 1.3861446380615234 / Matching loss: 2.2172584067448042e-05 / Item reconstruction: 0.0009079322917386889 / Text reconstruction: 0.00012550111568998545\n",
            "loss in epoch 10/11 iteration 97/111: 1.3864948749542236 / BPR loss: 1.3859972953796387 / Matching loss: 2.1206506062299013e-05 / Item reconstruction: 0.0008853981271386147 / Text reconstruction: 0.00016824148769956082\n",
            "loss in epoch 10/11 iteration 98/111: 1.3867666721343994 / BPR loss: 1.386268138885498 / Matching loss: 2.1570293029071763e-05 / Item reconstruction: 0.0009042648016475141 / Text reconstruction: 0.00012390293704811484\n",
            "loss in epoch 10/11 iteration 99/111: 1.386476993560791 / BPR loss: 1.3859694004058838 / Matching loss: 2.1644984371960163e-05 / Item reconstruction: 0.0009250778239220381 / Text reconstruction: 0.00011673237895593047\n",
            "loss in epoch 10/11 iteration 100/111: 1.3865371942520142 / BPR loss: 1.3860411643981934 / Matching loss: 2.127735388057772e-05 / Item reconstruction: 0.0009148984681814909 / Text reconstruction: 8.675277058500797e-05\n",
            "loss in epoch 10/11 iteration 101/111: 1.3866052627563477 / BPR loss: 1.3861075639724731 / Matching loss: 2.0080186004634015e-05 / Item reconstruction: 0.0008835120825096965 / Text reconstruction: 0.00017928701709024608\n",
            "loss in epoch 10/11 iteration 102/111: 1.3863778114318848 / BPR loss: 1.3858391046524048 / Matching loss: 2.3131899069994688e-05 / Item reconstruction: 0.000945619773119688 / Text reconstruction: 0.00021401734557002783\n",
            "loss in epoch 10/11 iteration 103/111: 1.3867814540863037 / BPR loss: 1.386269211769104 / Matching loss: 2.1369411115301773e-05 / Item reconstruction: 0.0009074168046936393 / Text reconstruction: 0.00018612832354847342\n",
            "loss in epoch 10/11 iteration 104/111: 1.386563777923584 / BPR loss: 1.3860852718353271 / Matching loss: 1.994821650441736e-05 / Item reconstruction: 0.0008631169330328703 / Text reconstruction: 0.00013554576435126364\n",
            "loss in epoch 10/11 iteration 105/111: 1.386436939239502 / BPR loss: 1.3859331607818604 / Matching loss: 2.2357155103236437e-05 / Item reconstruction: 0.0009160063927993178 / Text reconstruction: 0.00011670410458464175\n",
            "loss in epoch 10/11 iteration 106/111: 1.3866664171218872 / BPR loss: 1.386159896850586 / Matching loss: 2.184682307415642e-05 / Item reconstruction: 0.0009102259646169841 / Text reconstruction: 0.00014779335469938815\n",
            "loss in epoch 10/11 iteration 107/111: 1.3867067098617554 / BPR loss: 1.3862271308898926 / Matching loss: 2.0447165297809988e-05 / Item reconstruction: 0.0008684271015226841 / Text reconstruction: 0.00012456136755645275\n",
            "loss in epoch 10/11 iteration 108/111: 1.386579155921936 / BPR loss: 1.3860838413238525 / Matching loss: 2.1709312932216562e-05 / Item reconstruction: 0.0008991496870294213 / Text reconstruction: 0.00012060908193234354\n",
            "loss in epoch 10/11 iteration 109/111: 1.3867671489715576 / BPR loss: 1.3862626552581787 / Matching loss: 2.186907295254059e-05 / Item reconstruction: 0.0009154697181656957 / Text reconstruction: 0.00012465610052458942\n",
            "loss in epoch 10/11 iteration 110/111: 1.386772871017456 / BPR loss: 1.3863086700439453 / Matching loss: 1.8682963855098933e-05 / Item reconstruction: 0.0007733075181022286 / Text reconstruction: 0.00029461918165907264\n",
            "loss in epoch 10/11 iteration 111/111: 1.386582851409912 / BPR loss: 1.3861587047576904 / Matching loss: 1.4215035662346054e-05 / Item reconstruction: 0.0006712026079185307 / Text reconstruction: 0.0003718226798810065\n",
            "100% 10/10 [53:33<00:00, 321.35s/it]\n",
            "train time : 3213.4709465503693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.move('/content/models', '/content/drive/MyDrive/models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mFUvUrzJZ41w",
        "outputId": "854ea634-6b37-4b79-fd29-e5853363f5a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train stage 2\n",
        "!python /content/drive/MyDrive/Rec_Proj_DL/main.py --pretrain_stage2 --rec_pre_trained_data Software"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RzL1XeJfNBVf",
        "outputId": "7ddc4d62-a369-4156-a36b-5f7084273944"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A-LLMRec strat train phase-2\n",
            "\n",
            "user num: 3577 item num: 4557\n",
            "average sequence length: 4.91\n",
            "Initializing with num_user: 3577\n",
            "  0% 0/1 [00:00<?, ?it/s]A-LLMRec model loss in epoch 1/2 iteration 0/1788: 2.8859140872955322\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1/1788: 4.256014823913574\n",
            "A-LLMRec model loss in epoch 1/2 iteration 2/1788: 2.6492764949798584\n",
            "A-LLMRec model loss in epoch 1/2 iteration 3/1788: 1.6560101509094238\n",
            "A-LLMRec model loss in epoch 1/2 iteration 4/1788: 1.869297742843628\n",
            "A-LLMRec model loss in epoch 1/2 iteration 5/1788: 1.0813997983932495\n",
            "A-LLMRec model loss in epoch 1/2 iteration 6/1788: 1.6390305757522583\n",
            "A-LLMRec model loss in epoch 1/2 iteration 7/1788: 1.13101065158844\n",
            "A-LLMRec model loss in epoch 1/2 iteration 8/1788: 1.3371503353118896\n",
            "A-LLMRec model loss in epoch 1/2 iteration 9/1788: 1.2414122819900513\n",
            "A-LLMRec model loss in epoch 1/2 iteration 10/1788: 1.5098010301589966\n",
            "A-LLMRec model loss in epoch 1/2 iteration 11/1788: 1.323154330253601\n",
            "A-LLMRec model loss in epoch 1/2 iteration 12/1788: 1.3008874654769897\n",
            "A-LLMRec model loss in epoch 1/2 iteration 13/1788: 1.8690390586853027\n",
            "A-LLMRec model loss in epoch 1/2 iteration 14/1788: 2.3181138038635254\n",
            "A-LLMRec model loss in epoch 1/2 iteration 15/1788: 1.6582353115081787\n",
            "A-LLMRec model loss in epoch 1/2 iteration 16/1788: 1.2151519060134888\n",
            "A-LLMRec model loss in epoch 1/2 iteration 17/1788: 1.413319706916809\n",
            "A-LLMRec model loss in epoch 1/2 iteration 18/1788: 1.200143575668335\n",
            "A-LLMRec model loss in epoch 1/2 iteration 19/1788: 0.9473335146903992\n",
            "A-LLMRec model loss in epoch 1/2 iteration 20/1788: 1.0022938251495361\n",
            "A-LLMRec model loss in epoch 1/2 iteration 21/1788: 1.1286098957061768\n",
            "A-LLMRec model loss in epoch 1/2 iteration 22/1788: 1.5163613557815552\n",
            "A-LLMRec model loss in epoch 1/2 iteration 23/1788: 1.3837825059890747\n",
            "A-LLMRec model loss in epoch 1/2 iteration 24/1788: 0.9615793228149414\n",
            "A-LLMRec model loss in epoch 1/2 iteration 25/1788: 0.8717433214187622\n",
            "A-LLMRec model loss in epoch 1/2 iteration 26/1788: 1.049640417098999\n",
            "A-LLMRec model loss in epoch 1/2 iteration 27/1788: 1.402222990989685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 28/1788: 1.6783843040466309\n",
            "A-LLMRec model loss in epoch 1/2 iteration 29/1788: 0.9005770087242126\n",
            "A-LLMRec model loss in epoch 1/2 iteration 30/1788: 1.2409822940826416\n",
            "A-LLMRec model loss in epoch 1/2 iteration 31/1788: 0.6352189779281616\n",
            "A-LLMRec model loss in epoch 1/2 iteration 32/1788: 1.0026133060455322\n",
            "A-LLMRec model loss in epoch 1/2 iteration 33/1788: 0.925094723701477\n",
            "A-LLMRec model loss in epoch 1/2 iteration 34/1788: 0.7765357494354248\n",
            "A-LLMRec model loss in epoch 1/2 iteration 35/1788: 0.710614025592804\n",
            "A-LLMRec model loss in epoch 1/2 iteration 36/1788: 0.6974657773971558\n",
            "A-LLMRec model loss in epoch 1/2 iteration 37/1788: 0.6471224427223206\n",
            "A-LLMRec model loss in epoch 1/2 iteration 38/1788: 0.40716418623924255\n",
            "A-LLMRec model loss in epoch 1/2 iteration 39/1788: 0.5735248327255249\n",
            "A-LLMRec model loss in epoch 1/2 iteration 40/1788: 0.29716232419013977\n",
            "A-LLMRec model loss in epoch 1/2 iteration 41/1788: 0.9318890571594238\n",
            "A-LLMRec model loss in epoch 1/2 iteration 42/1788: 0.4284641146659851\n",
            "A-LLMRec model loss in epoch 1/2 iteration 43/1788: 0.2915199100971222\n",
            "A-LLMRec model loss in epoch 1/2 iteration 44/1788: 0.9755843877792358\n",
            "A-LLMRec model loss in epoch 1/2 iteration 45/1788: 0.45891666412353516\n",
            "A-LLMRec model loss in epoch 1/2 iteration 46/1788: 0.39710286259651184\n",
            "A-LLMRec model loss in epoch 1/2 iteration 47/1788: 0.6170340180397034\n",
            "A-LLMRec model loss in epoch 1/2 iteration 48/1788: 0.4414972960948944\n",
            "A-LLMRec model loss in epoch 1/2 iteration 49/1788: 0.4927371144294739\n",
            "A-LLMRec model loss in epoch 1/2 iteration 50/1788: 0.5005525350570679\n",
            "A-LLMRec model loss in epoch 1/2 iteration 51/1788: 0.24507606029510498\n",
            "A-LLMRec model loss in epoch 1/2 iteration 52/1788: 0.26434603333473206\n",
            "A-LLMRec model loss in epoch 1/2 iteration 53/1788: 0.22231480479240417\n",
            "A-LLMRec model loss in epoch 1/2 iteration 54/1788: 0.33346831798553467\n",
            "A-LLMRec model loss in epoch 1/2 iteration 55/1788: 0.7345726490020752\n",
            "A-LLMRec model loss in epoch 1/2 iteration 56/1788: 0.5409069061279297\n",
            "A-LLMRec model loss in epoch 1/2 iteration 57/1788: 0.22509180009365082\n",
            "A-LLMRec model loss in epoch 1/2 iteration 58/1788: 0.4501540958881378\n",
            "A-LLMRec model loss in epoch 1/2 iteration 59/1788: 0.3177298605442047\n",
            "A-LLMRec model loss in epoch 1/2 iteration 60/1788: 0.405723512172699\n",
            "A-LLMRec model loss in epoch 1/2 iteration 61/1788: 0.7373252511024475\n",
            "A-LLMRec model loss in epoch 1/2 iteration 62/1788: 0.5076196789741516\n",
            "A-LLMRec model loss in epoch 1/2 iteration 63/1788: 0.5977105498313904\n",
            "A-LLMRec model loss in epoch 1/2 iteration 64/1788: 0.2593870162963867\n",
            "A-LLMRec model loss in epoch 1/2 iteration 65/1788: 0.39320990443229675\n",
            "A-LLMRec model loss in epoch 1/2 iteration 66/1788: 0.7413710355758667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 67/1788: 0.18982894718647003\n",
            "A-LLMRec model loss in epoch 1/2 iteration 68/1788: 0.278377890586853\n",
            "A-LLMRec model loss in epoch 1/2 iteration 69/1788: 0.9838946461677551\n",
            "A-LLMRec model loss in epoch 1/2 iteration 70/1788: 0.41201168298721313\n",
            "A-LLMRec model loss in epoch 1/2 iteration 71/1788: 0.6103666424751282\n",
            "A-LLMRec model loss in epoch 1/2 iteration 72/1788: 0.46898144483566284\n",
            "A-LLMRec model loss in epoch 1/2 iteration 73/1788: 0.3553747236728668\n",
            "A-LLMRec model loss in epoch 1/2 iteration 74/1788: 0.23549002408981323\n",
            "A-LLMRec model loss in epoch 1/2 iteration 75/1788: 0.29339799284935\n",
            "A-LLMRec model loss in epoch 1/2 iteration 76/1788: 0.2413562685251236\n",
            "A-LLMRec model loss in epoch 1/2 iteration 77/1788: 0.20351654291152954\n",
            "A-LLMRec model loss in epoch 1/2 iteration 78/1788: 0.289518803358078\n",
            "A-LLMRec model loss in epoch 1/2 iteration 79/1788: 0.2681141793727875\n",
            "A-LLMRec model loss in epoch 1/2 iteration 80/1788: 0.432081013917923\n",
            "A-LLMRec model loss in epoch 1/2 iteration 81/1788: 0.5184798836708069\n",
            "A-LLMRec model loss in epoch 1/2 iteration 82/1788: 0.4264594614505768\n",
            "A-LLMRec model loss in epoch 1/2 iteration 83/1788: 0.6120970249176025\n",
            "A-LLMRec model loss in epoch 1/2 iteration 84/1788: 0.3392685055732727\n",
            "A-LLMRec model loss in epoch 1/2 iteration 85/1788: 0.17885296046733856\n",
            "A-LLMRec model loss in epoch 1/2 iteration 86/1788: 0.20423874258995056\n",
            "A-LLMRec model loss in epoch 1/2 iteration 87/1788: 0.17023640871047974\n",
            "A-LLMRec model loss in epoch 1/2 iteration 88/1788: 0.3602738678455353\n",
            "A-LLMRec model loss in epoch 1/2 iteration 89/1788: 0.2531875967979431\n",
            "A-LLMRec model loss in epoch 1/2 iteration 90/1788: 0.3584006130695343\n",
            "A-LLMRec model loss in epoch 1/2 iteration 91/1788: 0.44612154364585876\n",
            "A-LLMRec model loss in epoch 1/2 iteration 92/1788: 0.23107309639453888\n",
            "A-LLMRec model loss in epoch 1/2 iteration 93/1788: 0.25487422943115234\n",
            "A-LLMRec model loss in epoch 1/2 iteration 94/1788: 0.3530319035053253\n",
            "A-LLMRec model loss in epoch 1/2 iteration 95/1788: 0.29390662908554077\n",
            "A-LLMRec model loss in epoch 1/2 iteration 96/1788: 0.07592231780290604\n",
            "A-LLMRec model loss in epoch 1/2 iteration 97/1788: 0.13523025810718536\n",
            "A-LLMRec model loss in epoch 1/2 iteration 98/1788: 0.14885832369327545\n",
            "A-LLMRec model loss in epoch 1/2 iteration 99/1788: 0.10653328895568848\n",
            "A-LLMRec model loss in epoch 1/2 iteration 100/1788: 0.4371386766433716\n",
            "A-LLMRec model loss in epoch 1/2 iteration 101/1788: 0.37697675824165344\n",
            "A-LLMRec model loss in epoch 1/2 iteration 102/1788: 0.4775083065032959\n",
            "A-LLMRec model loss in epoch 1/2 iteration 103/1788: 0.348352313041687\n",
            "A-LLMRec model loss in epoch 1/2 iteration 104/1788: 0.42937570810317993\n",
            "A-LLMRec model loss in epoch 1/2 iteration 105/1788: 0.4235721826553345\n",
            "A-LLMRec model loss in epoch 1/2 iteration 106/1788: 0.2737167775630951\n",
            "A-LLMRec model loss in epoch 1/2 iteration 107/1788: 0.16500428318977356\n",
            "A-LLMRec model loss in epoch 1/2 iteration 108/1788: 0.5732725262641907\n",
            "A-LLMRec model loss in epoch 1/2 iteration 109/1788: 0.41705459356307983\n",
            "A-LLMRec model loss in epoch 1/2 iteration 110/1788: 0.37453508377075195\n",
            "A-LLMRec model loss in epoch 1/2 iteration 111/1788: 0.21049393713474274\n",
            "A-LLMRec model loss in epoch 1/2 iteration 112/1788: 0.2643647789955139\n",
            "A-LLMRec model loss in epoch 1/2 iteration 113/1788: 0.26600730419158936\n",
            "A-LLMRec model loss in epoch 1/2 iteration 114/1788: 0.4980754256248474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 115/1788: 0.3981490135192871\n",
            "A-LLMRec model loss in epoch 1/2 iteration 116/1788: 0.44584065675735474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 117/1788: 0.22144532203674316\n",
            "A-LLMRec model loss in epoch 1/2 iteration 118/1788: 0.13813313841819763\n",
            "A-LLMRec model loss in epoch 1/2 iteration 119/1788: 0.20250746607780457\n",
            "A-LLMRec model loss in epoch 1/2 iteration 120/1788: 0.43153175711631775\n",
            "A-LLMRec model loss in epoch 1/2 iteration 121/1788: 0.24451923370361328\n",
            "A-LLMRec model loss in epoch 1/2 iteration 122/1788: 0.2846718430519104\n",
            "A-LLMRec model loss in epoch 1/2 iteration 123/1788: 0.1944185197353363\n",
            "A-LLMRec model loss in epoch 1/2 iteration 124/1788: 0.4274277985095978\n",
            "A-LLMRec model loss in epoch 1/2 iteration 125/1788: 0.34696289896965027\n",
            "A-LLMRec model loss in epoch 1/2 iteration 126/1788: 0.27888375520706177\n",
            "A-LLMRec model loss in epoch 1/2 iteration 127/1788: 0.28868368268013\n",
            "A-LLMRec model loss in epoch 1/2 iteration 128/1788: 0.6294630765914917\n",
            "A-LLMRec model loss in epoch 1/2 iteration 129/1788: 0.21945449709892273\n",
            "A-LLMRec model loss in epoch 1/2 iteration 130/1788: 0.5520716905593872\n",
            "A-LLMRec model loss in epoch 1/2 iteration 131/1788: 0.35311514139175415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 132/1788: 0.3922698497772217\n",
            "A-LLMRec model loss in epoch 1/2 iteration 133/1788: 0.1693374365568161\n",
            "A-LLMRec model loss in epoch 1/2 iteration 134/1788: 0.23491717875003815\n",
            "A-LLMRec model loss in epoch 1/2 iteration 135/1788: 0.2855772078037262\n",
            "A-LLMRec model loss in epoch 1/2 iteration 136/1788: 0.34747809171676636\n",
            "A-LLMRec model loss in epoch 1/2 iteration 137/1788: 0.5146816968917847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 138/1788: 0.13165044784545898\n",
            "A-LLMRec model loss in epoch 1/2 iteration 139/1788: 0.20977887511253357\n",
            "A-LLMRec model loss in epoch 1/2 iteration 140/1788: 0.3156924843788147\n",
            "A-LLMRec model loss in epoch 1/2 iteration 141/1788: 0.5567405223846436\n",
            "A-LLMRec model loss in epoch 1/2 iteration 142/1788: 0.27756986021995544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 143/1788: 0.10870322585105896\n",
            "A-LLMRec model loss in epoch 1/2 iteration 144/1788: 0.5686814188957214\n",
            "A-LLMRec model loss in epoch 1/2 iteration 145/1788: 0.24215272068977356\n",
            "A-LLMRec model loss in epoch 1/2 iteration 146/1788: 0.4804135262966156\n",
            "A-LLMRec model loss in epoch 1/2 iteration 147/1788: 0.269608736038208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 148/1788: 0.23915426433086395\n",
            "A-LLMRec model loss in epoch 1/2 iteration 149/1788: 0.2937268614768982\n",
            "A-LLMRec model loss in epoch 1/2 iteration 150/1788: 0.23511052131652832\n",
            "A-LLMRec model loss in epoch 1/2 iteration 151/1788: 0.39199066162109375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 152/1788: 0.4314590096473694\n",
            "A-LLMRec model loss in epoch 1/2 iteration 153/1788: 0.2378433495759964\n",
            "A-LLMRec model loss in epoch 1/2 iteration 154/1788: 0.2883570194244385\n",
            "A-LLMRec model loss in epoch 1/2 iteration 155/1788: 0.2021109014749527\n",
            "A-LLMRec model loss in epoch 1/2 iteration 156/1788: 0.3796103894710541\n",
            "A-LLMRec model loss in epoch 1/2 iteration 157/1788: 0.22868689894676208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 158/1788: 0.2448672503232956\n",
            "A-LLMRec model loss in epoch 1/2 iteration 159/1788: 0.6670359373092651\n",
            "A-LLMRec model loss in epoch 1/2 iteration 160/1788: 0.28263381123542786\n",
            "A-LLMRec model loss in epoch 1/2 iteration 161/1788: 0.22800706326961517\n",
            "A-LLMRec model loss in epoch 1/2 iteration 162/1788: 0.5262488722801208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 163/1788: 0.31462782621383667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 164/1788: 0.29554620385169983\n",
            "A-LLMRec model loss in epoch 1/2 iteration 165/1788: 0.16979564726352692\n",
            "A-LLMRec model loss in epoch 1/2 iteration 166/1788: 0.2585303783416748\n",
            "A-LLMRec model loss in epoch 1/2 iteration 167/1788: 0.34259915351867676\n",
            "A-LLMRec model loss in epoch 1/2 iteration 168/1788: 0.24801288545131683\n",
            "A-LLMRec model loss in epoch 1/2 iteration 169/1788: 0.19798143208026886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 170/1788: 0.46792593598365784\n",
            "A-LLMRec model loss in epoch 1/2 iteration 171/1788: 0.22077320516109467\n",
            "A-LLMRec model loss in epoch 1/2 iteration 172/1788: 0.20613181591033936\n",
            "A-LLMRec model loss in epoch 1/2 iteration 173/1788: 0.24808962643146515\n",
            "A-LLMRec model loss in epoch 1/2 iteration 174/1788: 0.3847169578075409\n",
            "A-LLMRec model loss in epoch 1/2 iteration 175/1788: 0.8205544948577881\n",
            "A-LLMRec model loss in epoch 1/2 iteration 176/1788: 0.24319976568222046\n",
            "A-LLMRec model loss in epoch 1/2 iteration 177/1788: 0.2800282835960388\n",
            "A-LLMRec model loss in epoch 1/2 iteration 178/1788: 0.1692514568567276\n",
            "A-LLMRec model loss in epoch 1/2 iteration 179/1788: 0.18726134300231934\n",
            "A-LLMRec model loss in epoch 1/2 iteration 180/1788: 0.2106827348470688\n",
            "A-LLMRec model loss in epoch 1/2 iteration 181/1788: 0.22448153793811798\n",
            "A-LLMRec model loss in epoch 1/2 iteration 182/1788: 0.22002838551998138\n",
            "A-LLMRec model loss in epoch 1/2 iteration 183/1788: 0.29345300793647766\n",
            "A-LLMRec model loss in epoch 1/2 iteration 184/1788: 0.28337210416793823\n",
            "A-LLMRec model loss in epoch 1/2 iteration 185/1788: 1.602818250656128\n",
            "A-LLMRec model loss in epoch 1/2 iteration 186/1788: 0.43090522289276123\n",
            "A-LLMRec model loss in epoch 1/2 iteration 187/1788: 0.3189319968223572\n",
            "A-LLMRec model loss in epoch 1/2 iteration 188/1788: 0.4313935935497284\n",
            "A-LLMRec model loss in epoch 1/2 iteration 189/1788: 0.300548791885376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 190/1788: 0.31393611431121826\n",
            "A-LLMRec model loss in epoch 1/2 iteration 191/1788: 0.1830815225839615\n",
            "A-LLMRec model loss in epoch 1/2 iteration 192/1788: 0.4634256064891815\n",
            "A-LLMRec model loss in epoch 1/2 iteration 193/1788: 0.10120579600334167\n",
            "A-LLMRec model loss in epoch 1/2 iteration 194/1788: 0.45609813928604126\n",
            "A-LLMRec model loss in epoch 1/2 iteration 195/1788: 0.5186928510665894\n",
            "A-LLMRec model loss in epoch 1/2 iteration 196/1788: 0.746900200843811\n",
            "A-LLMRec model loss in epoch 1/2 iteration 197/1788: 0.37603724002838135\n",
            "A-LLMRec model loss in epoch 1/2 iteration 198/1788: 0.147776260972023\n",
            "A-LLMRec model loss in epoch 1/2 iteration 199/1788: 0.6191264986991882\n",
            "A-LLMRec model loss in epoch 1/2 iteration 200/1788: 0.40822139382362366\n",
            "A-LLMRec model loss in epoch 1/2 iteration 201/1788: 0.3154754638671875\n",
            "A-LLMRec model loss in epoch 1/2 iteration 202/1788: 0.29554295539855957\n",
            "A-LLMRec model loss in epoch 1/2 iteration 203/1788: 0.1994391828775406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 204/1788: 0.1424676924943924\n",
            "A-LLMRec model loss in epoch 1/2 iteration 205/1788: 0.4312874972820282\n",
            "A-LLMRec model loss in epoch 1/2 iteration 206/1788: 0.1581900417804718\n",
            "A-LLMRec model loss in epoch 1/2 iteration 207/1788: 0.20584942400455475\n",
            "A-LLMRec model loss in epoch 1/2 iteration 208/1788: 0.15364474058151245\n",
            "A-LLMRec model loss in epoch 1/2 iteration 209/1788: 0.237541064620018\n",
            "A-LLMRec model loss in epoch 1/2 iteration 210/1788: 0.41542455554008484\n",
            "A-LLMRec model loss in epoch 1/2 iteration 211/1788: 0.3094477653503418\n",
            "A-LLMRec model loss in epoch 1/2 iteration 212/1788: 0.34643295407295227\n",
            "A-LLMRec model loss in epoch 1/2 iteration 213/1788: 0.36866286396980286\n",
            "A-LLMRec model loss in epoch 1/2 iteration 214/1788: 0.27742841839790344\n",
            "A-LLMRec model loss in epoch 1/2 iteration 215/1788: 0.39934468269348145\n",
            "A-LLMRec model loss in epoch 1/2 iteration 216/1788: 0.22265908122062683\n",
            "A-LLMRec model loss in epoch 1/2 iteration 217/1788: 0.14542923867702484\n",
            "A-LLMRec model loss in epoch 1/2 iteration 218/1788: 0.12417353689670563\n",
            "A-LLMRec model loss in epoch 1/2 iteration 219/1788: 0.6485670208930969\n",
            "A-LLMRec model loss in epoch 1/2 iteration 220/1788: 0.18909691274166107\n",
            "A-LLMRec model loss in epoch 1/2 iteration 221/1788: 0.30300527811050415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 222/1788: 0.8117347359657288\n",
            "A-LLMRec model loss in epoch 1/2 iteration 223/1788: 0.3823002576828003\n",
            "A-LLMRec model loss in epoch 1/2 iteration 224/1788: 0.18273547291755676\n",
            "A-LLMRec model loss in epoch 1/2 iteration 225/1788: 0.2427450716495514\n",
            "A-LLMRec model loss in epoch 1/2 iteration 226/1788: 0.18361884355545044\n",
            "A-LLMRec model loss in epoch 1/2 iteration 227/1788: 0.36988744139671326\n",
            "A-LLMRec model loss in epoch 1/2 iteration 228/1788: 0.23670074343681335\n",
            "A-LLMRec model loss in epoch 1/2 iteration 229/1788: 0.5236814618110657\n",
            "A-LLMRec model loss in epoch 1/2 iteration 230/1788: 0.8091997504234314\n",
            "A-LLMRec model loss in epoch 1/2 iteration 231/1788: 0.4482092559337616\n",
            "A-LLMRec model loss in epoch 1/2 iteration 232/1788: 0.1396639198064804\n",
            "A-LLMRec model loss in epoch 1/2 iteration 233/1788: 0.4730416536331177\n",
            "A-LLMRec model loss in epoch 1/2 iteration 234/1788: 0.3251812756061554\n",
            "A-LLMRec model loss in epoch 1/2 iteration 235/1788: 0.4106065332889557\n",
            "A-LLMRec model loss in epoch 1/2 iteration 236/1788: 0.1474386304616928\n",
            "A-LLMRec model loss in epoch 1/2 iteration 237/1788: 0.16156890988349915\n",
            "A-LLMRec model loss in epoch 1/2 iteration 238/1788: 0.6750736832618713\n",
            "A-LLMRec model loss in epoch 1/2 iteration 239/1788: 0.1825263649225235\n",
            "A-LLMRec model loss in epoch 1/2 iteration 240/1788: 0.28852134943008423\n",
            "A-LLMRec model loss in epoch 1/2 iteration 241/1788: 0.5078686475753784\n",
            "A-LLMRec model loss in epoch 1/2 iteration 242/1788: 0.1787036955356598\n",
            "A-LLMRec model loss in epoch 1/2 iteration 243/1788: 0.2834279239177704\n",
            "A-LLMRec model loss in epoch 1/2 iteration 244/1788: 0.14295223355293274\n",
            "A-LLMRec model loss in epoch 1/2 iteration 245/1788: 0.20135937631130219\n",
            "A-LLMRec model loss in epoch 1/2 iteration 246/1788: 0.2121468484401703\n",
            "A-LLMRec model loss in epoch 1/2 iteration 247/1788: 0.3643932044506073\n",
            "A-LLMRec model loss in epoch 1/2 iteration 248/1788: 0.4501878023147583\n",
            "A-LLMRec model loss in epoch 1/2 iteration 249/1788: 0.18546193838119507\n",
            "A-LLMRec model loss in epoch 1/2 iteration 250/1788: 0.17117759585380554\n",
            "A-LLMRec model loss in epoch 1/2 iteration 251/1788: 0.5424389839172363\n",
            "A-LLMRec model loss in epoch 1/2 iteration 252/1788: 0.26865407824516296\n",
            "A-LLMRec model loss in epoch 1/2 iteration 253/1788: 0.5300775766372681\n",
            "A-LLMRec model loss in epoch 1/2 iteration 254/1788: 0.34085285663604736\n",
            "A-LLMRec model loss in epoch 1/2 iteration 255/1788: 0.2737589478492737\n",
            "A-LLMRec model loss in epoch 1/2 iteration 256/1788: 0.15807193517684937\n",
            "A-LLMRec model loss in epoch 1/2 iteration 257/1788: 0.4774804711341858\n",
            "A-LLMRec model loss in epoch 1/2 iteration 258/1788: 0.21430659294128418\n",
            "A-LLMRec model loss in epoch 1/2 iteration 259/1788: 0.3439153730869293\n",
            "A-LLMRec model loss in epoch 1/2 iteration 260/1788: 0.19105978310108185\n",
            "A-LLMRec model loss in epoch 1/2 iteration 261/1788: 0.3771159052848816\n",
            "A-LLMRec model loss in epoch 1/2 iteration 262/1788: 0.22983847558498383\n",
            "A-LLMRec model loss in epoch 1/2 iteration 263/1788: 0.3182031512260437\n",
            "A-LLMRec model loss in epoch 1/2 iteration 264/1788: 0.3967764973640442\n",
            "A-LLMRec model loss in epoch 1/2 iteration 265/1788: 0.13205106556415558\n",
            "A-LLMRec model loss in epoch 1/2 iteration 266/1788: 0.1666719764471054\n",
            "A-LLMRec model loss in epoch 1/2 iteration 267/1788: 0.07909101992845535\n",
            "A-LLMRec model loss in epoch 1/2 iteration 268/1788: 0.20289142429828644\n",
            "A-LLMRec model loss in epoch 1/2 iteration 269/1788: 0.2105480432510376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 270/1788: 0.37868228554725647\n",
            "A-LLMRec model loss in epoch 1/2 iteration 271/1788: 0.22285950183868408\n",
            "A-LLMRec model loss in epoch 1/2 iteration 272/1788: 0.1628476083278656\n",
            "A-LLMRec model loss in epoch 1/2 iteration 273/1788: 0.15492625534534454\n",
            "A-LLMRec model loss in epoch 1/2 iteration 274/1788: 0.4304109513759613\n",
            "A-LLMRec model loss in epoch 1/2 iteration 275/1788: 0.43048447370529175\n",
            "A-LLMRec model loss in epoch 1/2 iteration 276/1788: 0.29498788714408875\n",
            "A-LLMRec model loss in epoch 1/2 iteration 277/1788: 0.15444719791412354\n",
            "A-LLMRec model loss in epoch 1/2 iteration 278/1788: 0.13710542023181915\n",
            "A-LLMRec model loss in epoch 1/2 iteration 279/1788: 0.5319837927818298\n",
            "A-LLMRec model loss in epoch 1/2 iteration 280/1788: 0.28802165389060974\n",
            "A-LLMRec model loss in epoch 1/2 iteration 281/1788: 0.406571626663208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 282/1788: 0.2709910273551941\n",
            "A-LLMRec model loss in epoch 1/2 iteration 283/1788: 0.4904513955116272\n",
            "A-LLMRec model loss in epoch 1/2 iteration 284/1788: 0.1769491583108902\n",
            "A-LLMRec model loss in epoch 1/2 iteration 285/1788: 0.40931278467178345\n",
            "A-LLMRec model loss in epoch 1/2 iteration 286/1788: 0.18585847318172455\n",
            "A-LLMRec model loss in epoch 1/2 iteration 287/1788: 0.43954989314079285\n",
            "A-LLMRec model loss in epoch 1/2 iteration 288/1788: 0.08824307471513748\n",
            "A-LLMRec model loss in epoch 1/2 iteration 289/1788: 1.195969820022583\n",
            "A-LLMRec model loss in epoch 1/2 iteration 290/1788: 0.4315105378627777\n",
            "A-LLMRec model loss in epoch 1/2 iteration 291/1788: 0.30047541856765747\n",
            "A-LLMRec model loss in epoch 1/2 iteration 292/1788: 0.9206658601760864\n",
            "A-LLMRec model loss in epoch 1/2 iteration 293/1788: 0.13610851764678955\n",
            "A-LLMRec model loss in epoch 1/2 iteration 294/1788: 1.2927697896957397\n",
            "A-LLMRec model loss in epoch 1/2 iteration 295/1788: 0.21852636337280273\n",
            "A-LLMRec model loss in epoch 1/2 iteration 296/1788: 0.3543631136417389\n",
            "A-LLMRec model loss in epoch 1/2 iteration 297/1788: 0.16801202297210693\n",
            "A-LLMRec model loss in epoch 1/2 iteration 298/1788: 0.3369136154651642\n",
            "A-LLMRec model loss in epoch 1/2 iteration 299/1788: 0.23833723366260529\n",
            "A-LLMRec model loss in epoch 1/2 iteration 300/1788: 0.12845461070537567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 301/1788: 0.2103462666273117\n",
            "A-LLMRec model loss in epoch 1/2 iteration 302/1788: 0.18388274312019348\n",
            "A-LLMRec model loss in epoch 1/2 iteration 303/1788: 0.16645625233650208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 304/1788: 0.1891328990459442\n",
            "A-LLMRec model loss in epoch 1/2 iteration 305/1788: 0.4391935169696808\n",
            "A-LLMRec model loss in epoch 1/2 iteration 306/1788: 0.2675485610961914\n",
            "A-LLMRec model loss in epoch 1/2 iteration 307/1788: 0.6717730760574341\n",
            "A-LLMRec model loss in epoch 1/2 iteration 308/1788: 0.1599879264831543\n",
            "A-LLMRec model loss in epoch 1/2 iteration 309/1788: 0.15938732028007507\n",
            "A-LLMRec model loss in epoch 1/2 iteration 310/1788: 0.2656496465206146\n",
            "A-LLMRec model loss in epoch 1/2 iteration 311/1788: 0.32823577523231506\n",
            "A-LLMRec model loss in epoch 1/2 iteration 312/1788: 0.227408766746521\n",
            "A-LLMRec model loss in epoch 1/2 iteration 313/1788: 0.5254026055335999\n",
            "A-LLMRec model loss in epoch 1/2 iteration 314/1788: 0.20656320452690125\n",
            "A-LLMRec model loss in epoch 1/2 iteration 315/1788: 0.14876288175582886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 316/1788: 0.09559739381074905\n",
            "A-LLMRec model loss in epoch 1/2 iteration 317/1788: 0.18000753223896027\n",
            "A-LLMRec model loss in epoch 1/2 iteration 318/1788: 0.3356205224990845\n",
            "A-LLMRec model loss in epoch 1/2 iteration 319/1788: 0.07318658381700516\n",
            "A-LLMRec model loss in epoch 1/2 iteration 320/1788: 0.29741793870925903\n",
            "A-LLMRec model loss in epoch 1/2 iteration 321/1788: 0.23106975853443146\n",
            "A-LLMRec model loss in epoch 1/2 iteration 322/1788: 0.2772550582885742\n",
            "A-LLMRec model loss in epoch 1/2 iteration 323/1788: 0.3017635941505432\n",
            "A-LLMRec model loss in epoch 1/2 iteration 324/1788: 0.3521682322025299\n",
            "A-LLMRec model loss in epoch 1/2 iteration 325/1788: 0.1493135392665863\n",
            "A-LLMRec model loss in epoch 1/2 iteration 326/1788: 0.24413765966892242\n",
            "A-LLMRec model loss in epoch 1/2 iteration 327/1788: 0.25409889221191406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 328/1788: 0.2096717357635498\n",
            "A-LLMRec model loss in epoch 1/2 iteration 329/1788: 0.07365113496780396\n",
            "A-LLMRec model loss in epoch 1/2 iteration 330/1788: 0.36772871017456055\n",
            "A-LLMRec model loss in epoch 1/2 iteration 331/1788: 0.40297284722328186\n",
            "A-LLMRec model loss in epoch 1/2 iteration 332/1788: 0.33886995911598206\n",
            "A-LLMRec model loss in epoch 1/2 iteration 333/1788: 0.34797045588493347\n",
            "A-LLMRec model loss in epoch 1/2 iteration 334/1788: 0.601172149181366\n",
            "A-LLMRec model loss in epoch 1/2 iteration 335/1788: 0.17109137773513794\n",
            "A-LLMRec model loss in epoch 1/2 iteration 336/1788: 0.1869841068983078\n",
            "A-LLMRec model loss in epoch 1/2 iteration 337/1788: 1.1368058919906616\n",
            "A-LLMRec model loss in epoch 1/2 iteration 338/1788: 0.1412038654088974\n",
            "A-LLMRec model loss in epoch 1/2 iteration 339/1788: 0.14410865306854248\n",
            "A-LLMRec model loss in epoch 1/2 iteration 340/1788: 0.18926391005516052\n",
            "A-LLMRec model loss in epoch 1/2 iteration 341/1788: 0.22154489159584045\n",
            "A-LLMRec model loss in epoch 1/2 iteration 342/1788: 0.35114729404449463\n",
            "A-LLMRec model loss in epoch 1/2 iteration 343/1788: 0.2825087606906891\n",
            "A-LLMRec model loss in epoch 1/2 iteration 344/1788: 0.0494723878800869\n",
            "A-LLMRec model loss in epoch 1/2 iteration 345/1788: 0.3858771324157715\n",
            "A-LLMRec model loss in epoch 1/2 iteration 346/1788: 0.37531474232673645\n",
            "A-LLMRec model loss in epoch 1/2 iteration 347/1788: 0.2988363802433014\n",
            "A-LLMRec model loss in epoch 1/2 iteration 348/1788: 0.624282956123352\n",
            "A-LLMRec model loss in epoch 1/2 iteration 349/1788: 0.45639410614967346\n",
            "A-LLMRec model loss in epoch 1/2 iteration 350/1788: 0.22956587374210358\n",
            "A-LLMRec model loss in epoch 1/2 iteration 351/1788: 0.34248554706573486\n",
            "A-LLMRec model loss in epoch 1/2 iteration 352/1788: 0.09408405423164368\n",
            "A-LLMRec model loss in epoch 1/2 iteration 353/1788: 0.19925491511821747\n",
            "A-LLMRec model loss in epoch 1/2 iteration 354/1788: 0.18227000534534454\n",
            "A-LLMRec model loss in epoch 1/2 iteration 355/1788: 0.33062511682510376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 356/1788: 0.2588842511177063\n",
            "A-LLMRec model loss in epoch 1/2 iteration 357/1788: 0.2529967427253723\n",
            "A-LLMRec model loss in epoch 1/2 iteration 358/1788: 0.5521138310432434\n",
            "A-LLMRec model loss in epoch 1/2 iteration 359/1788: 0.8505063056945801\n",
            "A-LLMRec model loss in epoch 1/2 iteration 360/1788: 0.2488461285829544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 361/1788: 0.45422670245170593\n",
            "A-LLMRec model loss in epoch 1/2 iteration 362/1788: 0.37200409173965454\n",
            "A-LLMRec model loss in epoch 1/2 iteration 363/1788: 0.39277416467666626\n",
            "A-LLMRec model loss in epoch 1/2 iteration 364/1788: 0.3011336028575897\n",
            "A-LLMRec model loss in epoch 1/2 iteration 365/1788: 0.3665117025375366\n",
            "A-LLMRec model loss in epoch 1/2 iteration 366/1788: 0.19542434811592102\n",
            "A-LLMRec model loss in epoch 1/2 iteration 367/1788: 0.3334996998310089\n",
            "A-LLMRec model loss in epoch 1/2 iteration 368/1788: 0.49795714020729065\n",
            "A-LLMRec model loss in epoch 1/2 iteration 369/1788: 0.14507493376731873\n",
            "A-LLMRec model loss in epoch 1/2 iteration 370/1788: 0.30033424496650696\n",
            "A-LLMRec model loss in epoch 1/2 iteration 371/1788: 0.31055596470832825\n",
            "A-LLMRec model loss in epoch 1/2 iteration 372/1788: 0.31136050820350647\n",
            "A-LLMRec model loss in epoch 1/2 iteration 373/1788: 0.5481239557266235\n",
            "A-LLMRec model loss in epoch 1/2 iteration 374/1788: 0.5107433795928955\n",
            "A-LLMRec model loss in epoch 1/2 iteration 375/1788: 0.502267599105835\n",
            "A-LLMRec model loss in epoch 1/2 iteration 376/1788: 0.3684213161468506\n",
            "A-LLMRec model loss in epoch 1/2 iteration 377/1788: 0.2831547260284424\n",
            "A-LLMRec model loss in epoch 1/2 iteration 378/1788: 0.5730841159820557\n",
            "A-LLMRec model loss in epoch 1/2 iteration 379/1788: 0.1656801700592041\n",
            "A-LLMRec model loss in epoch 1/2 iteration 380/1788: 0.631672739982605\n",
            "A-LLMRec model loss in epoch 1/2 iteration 381/1788: 0.21272815763950348\n",
            "A-LLMRec model loss in epoch 1/2 iteration 382/1788: 0.2737005949020386\n",
            "A-LLMRec model loss in epoch 1/2 iteration 383/1788: 0.16778728365898132\n",
            "A-LLMRec model loss in epoch 1/2 iteration 384/1788: 0.19538752734661102\n",
            "A-LLMRec model loss in epoch 1/2 iteration 385/1788: 0.16735738515853882\n",
            "A-LLMRec model loss in epoch 1/2 iteration 386/1788: 0.17120125889778137\n",
            "A-LLMRec model loss in epoch 1/2 iteration 387/1788: 0.4071613848209381\n",
            "A-LLMRec model loss in epoch 1/2 iteration 388/1788: 0.10554303228855133\n",
            "A-LLMRec model loss in epoch 1/2 iteration 389/1788: 0.4478507339954376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 390/1788: 0.28555792570114136\n",
            "A-LLMRec model loss in epoch 1/2 iteration 391/1788: 0.20476330816745758\n",
            "A-LLMRec model loss in epoch 1/2 iteration 392/1788: 0.21226057410240173\n",
            "A-LLMRec model loss in epoch 1/2 iteration 393/1788: 0.29310157895088196\n",
            "A-LLMRec model loss in epoch 1/2 iteration 394/1788: 0.5251390337944031\n",
            "A-LLMRec model loss in epoch 1/2 iteration 395/1788: 0.28840458393096924\n",
            "A-LLMRec model loss in epoch 1/2 iteration 396/1788: 0.35728955268859863\n",
            "A-LLMRec model loss in epoch 1/2 iteration 397/1788: 0.2934572696685791\n",
            "A-LLMRec model loss in epoch 1/2 iteration 398/1788: 0.17532934248447418\n",
            "A-LLMRec model loss in epoch 1/2 iteration 399/1788: 0.2381545454263687\n",
            "A-LLMRec model loss in epoch 1/2 iteration 400/1788: 0.1833798587322235\n",
            "A-LLMRec model loss in epoch 1/2 iteration 401/1788: 0.24886605143547058\n",
            "A-LLMRec model loss in epoch 1/2 iteration 402/1788: 0.31542059779167175\n",
            "A-LLMRec model loss in epoch 1/2 iteration 403/1788: 0.24520549178123474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 404/1788: 0.2206786721944809\n",
            "A-LLMRec model loss in epoch 1/2 iteration 405/1788: 0.2212090939283371\n",
            "A-LLMRec model loss in epoch 1/2 iteration 406/1788: 0.25533413887023926\n",
            "A-LLMRec model loss in epoch 1/2 iteration 407/1788: 0.26430708169937134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 408/1788: 0.505692720413208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 409/1788: 0.08572462946176529\n",
            "A-LLMRec model loss in epoch 1/2 iteration 410/1788: 0.2623133361339569\n",
            "A-LLMRec model loss in epoch 1/2 iteration 411/1788: 0.2238430380821228\n",
            "A-LLMRec model loss in epoch 1/2 iteration 412/1788: 0.07632910460233688\n",
            "A-LLMRec model loss in epoch 1/2 iteration 413/1788: 0.16369542479515076\n",
            "A-LLMRec model loss in epoch 1/2 iteration 414/1788: 0.21699701249599457\n",
            "A-LLMRec model loss in epoch 1/2 iteration 415/1788: 0.3493768274784088\n",
            "A-LLMRec model loss in epoch 1/2 iteration 416/1788: 0.20405732095241547\n",
            "A-LLMRec model loss in epoch 1/2 iteration 417/1788: 0.16480298340320587\n",
            "A-LLMRec model loss in epoch 1/2 iteration 418/1788: 0.47786152362823486\n",
            "A-LLMRec model loss in epoch 1/2 iteration 419/1788: 0.10639681667089462\n",
            "A-LLMRec model loss in epoch 1/2 iteration 420/1788: 0.23151493072509766\n",
            "A-LLMRec model loss in epoch 1/2 iteration 421/1788: 0.09357265383005142\n",
            "A-LLMRec model loss in epoch 1/2 iteration 422/1788: 0.34308892488479614\n",
            "A-LLMRec model loss in epoch 1/2 iteration 423/1788: 0.23630216717720032\n",
            "A-LLMRec model loss in epoch 1/2 iteration 424/1788: 0.34429189562797546\n",
            "A-LLMRec model loss in epoch 1/2 iteration 425/1788: 0.2487257868051529\n",
            "A-LLMRec model loss in epoch 1/2 iteration 426/1788: 0.4027037024497986\n",
            "A-LLMRec model loss in epoch 1/2 iteration 427/1788: 0.2638537287712097\n",
            "A-LLMRec model loss in epoch 1/2 iteration 428/1788: 0.5953166484832764\n",
            "A-LLMRec model loss in epoch 1/2 iteration 429/1788: 0.3109983801841736\n",
            "A-LLMRec model loss in epoch 1/2 iteration 430/1788: 0.3573731482028961\n",
            "A-LLMRec model loss in epoch 1/2 iteration 431/1788: 0.23404380679130554\n",
            "A-LLMRec model loss in epoch 1/2 iteration 432/1788: 0.3628016412258148\n",
            "A-LLMRec model loss in epoch 1/2 iteration 433/1788: 0.4771745502948761\n",
            "A-LLMRec model loss in epoch 1/2 iteration 434/1788: 0.2030186951160431\n",
            "A-LLMRec model loss in epoch 1/2 iteration 435/1788: 0.2763844430446625\n",
            "A-LLMRec model loss in epoch 1/2 iteration 436/1788: 0.22866497933864594\n",
            "A-LLMRec model loss in epoch 1/2 iteration 437/1788: 0.5939922332763672\n",
            "A-LLMRec model loss in epoch 1/2 iteration 438/1788: 0.2250027060508728\n",
            "A-LLMRec model loss in epoch 1/2 iteration 439/1788: 0.20151053369045258\n",
            "A-LLMRec model loss in epoch 1/2 iteration 440/1788: 0.17143674194812775\n",
            "A-LLMRec model loss in epoch 1/2 iteration 441/1788: 0.4221676290035248\n",
            "A-LLMRec model loss in epoch 1/2 iteration 442/1788: 0.3151199221611023\n",
            "A-LLMRec model loss in epoch 1/2 iteration 443/1788: 0.4404916763305664\n",
            "A-LLMRec model loss in epoch 1/2 iteration 444/1788: 0.22845415771007538\n",
            "A-LLMRec model loss in epoch 1/2 iteration 445/1788: 0.2619200348854065\n",
            "A-LLMRec model loss in epoch 1/2 iteration 446/1788: 0.41905856132507324\n",
            "A-LLMRec model loss in epoch 1/2 iteration 447/1788: 0.10897215455770493\n",
            "A-LLMRec model loss in epoch 1/2 iteration 448/1788: 0.2863050401210785\n",
            "A-LLMRec model loss in epoch 1/2 iteration 449/1788: 0.23246629536151886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 450/1788: 0.15063180029392242\n",
            "A-LLMRec model loss in epoch 1/2 iteration 451/1788: 0.23388046026229858\n",
            "A-LLMRec model loss in epoch 1/2 iteration 452/1788: 0.6446607708930969\n",
            "A-LLMRec model loss in epoch 1/2 iteration 453/1788: 0.1989218294620514\n",
            "A-LLMRec model loss in epoch 1/2 iteration 454/1788: 0.13852733373641968\n",
            "A-LLMRec model loss in epoch 1/2 iteration 455/1788: 0.2583547830581665\n",
            "A-LLMRec model loss in epoch 1/2 iteration 456/1788: 0.4643945097923279\n",
            "A-LLMRec model loss in epoch 1/2 iteration 457/1788: 0.08976905047893524\n",
            "A-LLMRec model loss in epoch 1/2 iteration 458/1788: 0.456961065530777\n",
            "A-LLMRec model loss in epoch 1/2 iteration 459/1788: 0.26294663548469543\n",
            "A-LLMRec model loss in epoch 1/2 iteration 460/1788: 0.31043440103530884\n",
            "A-LLMRec model loss in epoch 1/2 iteration 461/1788: 0.3701367974281311\n",
            "A-LLMRec model loss in epoch 1/2 iteration 462/1788: 0.32424986362457275\n",
            "A-LLMRec model loss in epoch 1/2 iteration 463/1788: 0.20132620632648468\n",
            "A-LLMRec model loss in epoch 1/2 iteration 464/1788: 0.15318183600902557\n",
            "A-LLMRec model loss in epoch 1/2 iteration 465/1788: 0.3908708989620209\n",
            "A-LLMRec model loss in epoch 1/2 iteration 466/1788: 0.44034987688064575\n",
            "A-LLMRec model loss in epoch 1/2 iteration 467/1788: 0.19612596929073334\n",
            "A-LLMRec model loss in epoch 1/2 iteration 468/1788: 0.187464639544487\n",
            "A-LLMRec model loss in epoch 1/2 iteration 469/1788: 0.49910709261894226\n",
            "A-LLMRec model loss in epoch 1/2 iteration 470/1788: 0.3024560511112213\n",
            "A-LLMRec model loss in epoch 1/2 iteration 471/1788: 0.12467179447412491\n",
            "A-LLMRec model loss in epoch 1/2 iteration 472/1788: 0.1713234931230545\n",
            "A-LLMRec model loss in epoch 1/2 iteration 473/1788: 0.17103280127048492\n",
            "A-LLMRec model loss in epoch 1/2 iteration 474/1788: 0.045526497066020966\n",
            "A-LLMRec model loss in epoch 1/2 iteration 475/1788: 0.1292053908109665\n",
            "A-LLMRec model loss in epoch 1/2 iteration 476/1788: 0.16411566734313965\n",
            "A-LLMRec model loss in epoch 1/2 iteration 477/1788: 0.12037596106529236\n",
            "A-LLMRec model loss in epoch 1/2 iteration 478/1788: 0.22663982212543488\n",
            "A-LLMRec model loss in epoch 1/2 iteration 479/1788: 0.20098350942134857\n",
            "A-LLMRec model loss in epoch 1/2 iteration 480/1788: 0.18394935131072998\n",
            "A-LLMRec model loss in epoch 1/2 iteration 481/1788: 0.11274230480194092\n",
            "A-LLMRec model loss in epoch 1/2 iteration 482/1788: 0.22596308588981628\n",
            "A-LLMRec model loss in epoch 1/2 iteration 483/1788: 0.09241334348917007\n",
            "A-LLMRec model loss in epoch 1/2 iteration 484/1788: 0.31066784262657166\n",
            "A-LLMRec model loss in epoch 1/2 iteration 485/1788: 0.18808645009994507\n",
            "A-LLMRec model loss in epoch 1/2 iteration 486/1788: 0.1278807669878006\n",
            "A-LLMRec model loss in epoch 1/2 iteration 487/1788: 0.19487129151821136\n",
            "A-LLMRec model loss in epoch 1/2 iteration 488/1788: 0.2681512236595154\n",
            "A-LLMRec model loss in epoch 1/2 iteration 489/1788: 0.02192109264433384\n",
            "A-LLMRec model loss in epoch 1/2 iteration 490/1788: 0.07437838613986969\n",
            "A-LLMRec model loss in epoch 1/2 iteration 491/1788: 0.3043563663959503\n",
            "A-LLMRec model loss in epoch 1/2 iteration 492/1788: 0.06956828385591507\n",
            "A-LLMRec model loss in epoch 1/2 iteration 493/1788: 0.1983998864889145\n",
            "A-LLMRec model loss in epoch 1/2 iteration 494/1788: 0.40669044852256775\n",
            "A-LLMRec model loss in epoch 1/2 iteration 495/1788: 0.40113365650177\n",
            "A-LLMRec model loss in epoch 1/2 iteration 496/1788: 0.1523512452840805\n",
            "A-LLMRec model loss in epoch 1/2 iteration 497/1788: 0.4072082042694092\n",
            "A-LLMRec model loss in epoch 1/2 iteration 498/1788: 0.38969436287879944\n",
            "A-LLMRec model loss in epoch 1/2 iteration 499/1788: 0.09544485807418823\n",
            "A-LLMRec model loss in epoch 1/2 iteration 500/1788: 0.1443232297897339\n",
            "A-LLMRec model loss in epoch 1/2 iteration 501/1788: 0.35068318247795105\n",
            "A-LLMRec model loss in epoch 1/2 iteration 502/1788: 0.2982991933822632\n",
            "A-LLMRec model loss in epoch 1/2 iteration 503/1788: 0.17336958646774292\n",
            "A-LLMRec model loss in epoch 1/2 iteration 504/1788: 0.28986838459968567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 505/1788: 0.12993794679641724\n",
            "A-LLMRec model loss in epoch 1/2 iteration 506/1788: 0.41800838708877563\n",
            "A-LLMRec model loss in epoch 1/2 iteration 507/1788: 0.2489955574274063\n",
            "A-LLMRec model loss in epoch 1/2 iteration 508/1788: 0.2848770320415497\n",
            "A-LLMRec model loss in epoch 1/2 iteration 509/1788: 0.2018079310655594\n",
            "A-LLMRec model loss in epoch 1/2 iteration 510/1788: 0.2455349713563919\n",
            "A-LLMRec model loss in epoch 1/2 iteration 511/1788: 0.17177172005176544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 512/1788: 0.2282470166683197\n",
            "A-LLMRec model loss in epoch 1/2 iteration 513/1788: 0.07220473885536194\n",
            "A-LLMRec model loss in epoch 1/2 iteration 514/1788: 0.2669263184070587\n",
            "A-LLMRec model loss in epoch 1/2 iteration 515/1788: 0.22757504880428314\n",
            "A-LLMRec model loss in epoch 1/2 iteration 516/1788: 0.3620910942554474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 517/1788: 0.30919384956359863\n",
            "A-LLMRec model loss in epoch 1/2 iteration 518/1788: 0.21929006278514862\n",
            "A-LLMRec model loss in epoch 1/2 iteration 519/1788: 0.5138550400733948\n",
            "A-LLMRec model loss in epoch 1/2 iteration 520/1788: 0.12472829222679138\n",
            "A-LLMRec model loss in epoch 1/2 iteration 521/1788: 0.09038858115673065\n",
            "A-LLMRec model loss in epoch 1/2 iteration 522/1788: 0.17869813740253448\n",
            "A-LLMRec model loss in epoch 1/2 iteration 523/1788: 0.4439990520477295\n",
            "A-LLMRec model loss in epoch 1/2 iteration 524/1788: 0.18498946726322174\n",
            "A-LLMRec model loss in epoch 1/2 iteration 525/1788: 0.08118265122175217\n",
            "A-LLMRec model loss in epoch 1/2 iteration 526/1788: 0.237112894654274\n",
            "A-LLMRec model loss in epoch 1/2 iteration 527/1788: 0.13710248470306396\n",
            "A-LLMRec model loss in epoch 1/2 iteration 528/1788: 0.35770362615585327\n",
            "A-LLMRec model loss in epoch 1/2 iteration 529/1788: 0.22004373371601105\n",
            "A-LLMRec model loss in epoch 1/2 iteration 530/1788: 0.055436864495277405\n",
            "A-LLMRec model loss in epoch 1/2 iteration 531/1788: 0.18180572986602783\n",
            "A-LLMRec model loss in epoch 1/2 iteration 532/1788: 0.14521227777004242\n",
            "A-LLMRec model loss in epoch 1/2 iteration 533/1788: 0.1462419331073761\n",
            "A-LLMRec model loss in epoch 1/2 iteration 534/1788: 0.23418080806732178\n",
            "A-LLMRec model loss in epoch 1/2 iteration 535/1788: 0.21311336755752563\n",
            "A-LLMRec model loss in epoch 1/2 iteration 536/1788: 0.28125032782554626\n",
            "A-LLMRec model loss in epoch 1/2 iteration 537/1788: 0.2229478657245636\n",
            "A-LLMRec model loss in epoch 1/2 iteration 538/1788: 0.2550068795681\n",
            "A-LLMRec model loss in epoch 1/2 iteration 539/1788: 0.08575878292322159\n",
            "A-LLMRec model loss in epoch 1/2 iteration 540/1788: 0.5460627675056458\n",
            "A-LLMRec model loss in epoch 1/2 iteration 541/1788: 0.1132805123925209\n",
            "A-LLMRec model loss in epoch 1/2 iteration 542/1788: 0.18472544848918915\n",
            "A-LLMRec model loss in epoch 1/2 iteration 543/1788: 0.269912987947464\n",
            "A-LLMRec model loss in epoch 1/2 iteration 544/1788: 0.22024396061897278\n",
            "A-LLMRec model loss in epoch 1/2 iteration 545/1788: 0.2852102816104889\n",
            "A-LLMRec model loss in epoch 1/2 iteration 546/1788: 0.2229927033185959\n",
            "A-LLMRec model loss in epoch 1/2 iteration 547/1788: 0.184923455119133\n",
            "A-LLMRec model loss in epoch 1/2 iteration 548/1788: 0.2467728555202484\n",
            "A-LLMRec model loss in epoch 1/2 iteration 549/1788: 0.20897236466407776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 550/1788: 0.5431663990020752\n",
            "A-LLMRec model loss in epoch 1/2 iteration 551/1788: 0.18762724101543427\n",
            "A-LLMRec model loss in epoch 1/2 iteration 552/1788: 0.4256182610988617\n",
            "A-LLMRec model loss in epoch 1/2 iteration 553/1788: 0.12659044563770294\n",
            "A-LLMRec model loss in epoch 1/2 iteration 554/1788: 0.29001563787460327\n",
            "A-LLMRec model loss in epoch 1/2 iteration 555/1788: 0.17672616243362427\n",
            "A-LLMRec model loss in epoch 1/2 iteration 556/1788: 0.2422962635755539\n",
            "A-LLMRec model loss in epoch 1/2 iteration 557/1788: 0.057702064514160156\n",
            "A-LLMRec model loss in epoch 1/2 iteration 558/1788: 0.39244070649147034\n",
            "A-LLMRec model loss in epoch 1/2 iteration 559/1788: 0.08748216927051544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 560/1788: 0.2993922829627991\n",
            "A-LLMRec model loss in epoch 1/2 iteration 561/1788: 0.4321478307247162\n",
            "A-LLMRec model loss in epoch 1/2 iteration 562/1788: 0.2682214081287384\n",
            "A-LLMRec model loss in epoch 1/2 iteration 563/1788: 0.15834873914718628\n",
            "A-LLMRec model loss in epoch 1/2 iteration 564/1788: 0.8352639675140381\n",
            "A-LLMRec model loss in epoch 1/2 iteration 565/1788: 0.19056624174118042\n",
            "A-LLMRec model loss in epoch 1/2 iteration 566/1788: 0.4195135831832886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 567/1788: 0.21765194833278656\n",
            "A-LLMRec model loss in epoch 1/2 iteration 568/1788: 0.1408562809228897\n",
            "A-LLMRec model loss in epoch 1/2 iteration 569/1788: 0.4080619812011719\n",
            "A-LLMRec model loss in epoch 1/2 iteration 570/1788: 0.20499978959560394\n",
            "A-LLMRec model loss in epoch 1/2 iteration 571/1788: 0.3099968433380127\n",
            "A-LLMRec model loss in epoch 1/2 iteration 572/1788: 0.16926386952400208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 573/1788: 0.7505271434783936\n",
            "A-LLMRec model loss in epoch 1/2 iteration 574/1788: 0.18266208469867706\n",
            "A-LLMRec model loss in epoch 1/2 iteration 575/1788: 0.3501579463481903\n",
            "A-LLMRec model loss in epoch 1/2 iteration 576/1788: 0.4647391140460968\n",
            "A-LLMRec model loss in epoch 1/2 iteration 577/1788: 0.24456943571567535\n",
            "A-LLMRec model loss in epoch 1/2 iteration 578/1788: 0.3308960199356079\n",
            "A-LLMRec model loss in epoch 1/2 iteration 579/1788: 0.3097871243953705\n",
            "A-LLMRec model loss in epoch 1/2 iteration 580/1788: 0.27746307849884033\n",
            "A-LLMRec model loss in epoch 1/2 iteration 581/1788: 0.10508088767528534\n",
            "A-LLMRec model loss in epoch 1/2 iteration 582/1788: 0.09585887938737869\n",
            "A-LLMRec model loss in epoch 1/2 iteration 583/1788: 0.15359827876091003\n",
            "A-LLMRec model loss in epoch 1/2 iteration 584/1788: 0.2708548605442047\n",
            "A-LLMRec model loss in epoch 1/2 iteration 585/1788: 0.2741396427154541\n",
            "A-LLMRec model loss in epoch 1/2 iteration 586/1788: 0.21075990796089172\n",
            "A-LLMRec model loss in epoch 1/2 iteration 587/1788: 0.6364932060241699\n",
            "A-LLMRec model loss in epoch 1/2 iteration 588/1788: 0.128539577126503\n",
            "A-LLMRec model loss in epoch 1/2 iteration 589/1788: 0.16293513774871826\n",
            "A-LLMRec model loss in epoch 1/2 iteration 590/1788: 0.3845474421977997\n",
            "A-LLMRec model loss in epoch 1/2 iteration 591/1788: 0.21869462728500366\n",
            "A-LLMRec model loss in epoch 1/2 iteration 592/1788: 0.10738471150398254\n",
            "A-LLMRec model loss in epoch 1/2 iteration 593/1788: 0.23233403265476227\n",
            "A-LLMRec model loss in epoch 1/2 iteration 594/1788: 0.5274328589439392\n",
            "A-LLMRec model loss in epoch 1/2 iteration 595/1788: 0.13193276524543762\n",
            "A-LLMRec model loss in epoch 1/2 iteration 596/1788: 0.12102723121643066\n",
            "A-LLMRec model loss in epoch 1/2 iteration 597/1788: 0.24346768856048584\n",
            "A-LLMRec model loss in epoch 1/2 iteration 598/1788: 0.2802172899246216\n",
            "A-LLMRec model loss in epoch 1/2 iteration 599/1788: 0.572435736656189\n",
            "A-LLMRec model loss in epoch 1/2 iteration 600/1788: 0.15976722538471222\n",
            "A-LLMRec model loss in epoch 1/2 iteration 601/1788: 0.16852538287639618\n",
            "A-LLMRec model loss in epoch 1/2 iteration 602/1788: 0.15859264135360718\n",
            "A-LLMRec model loss in epoch 1/2 iteration 603/1788: 0.396771103143692\n",
            "A-LLMRec model loss in epoch 1/2 iteration 604/1788: 0.3402634859085083\n",
            "A-LLMRec model loss in epoch 1/2 iteration 605/1788: 0.13079743087291718\n",
            "A-LLMRec model loss in epoch 1/2 iteration 606/1788: 0.2619113624095917\n",
            "A-LLMRec model loss in epoch 1/2 iteration 607/1788: 0.2873297929763794\n",
            "A-LLMRec model loss in epoch 1/2 iteration 608/1788: 0.26194632053375244\n",
            "A-LLMRec model loss in epoch 1/2 iteration 609/1788: 0.25774291157722473\n",
            "A-LLMRec model loss in epoch 1/2 iteration 610/1788: 0.33572208881378174\n",
            "A-LLMRec model loss in epoch 1/2 iteration 611/1788: 0.4590306580066681\n",
            "A-LLMRec model loss in epoch 1/2 iteration 612/1788: 0.10268005728721619\n",
            "A-LLMRec model loss in epoch 1/2 iteration 613/1788: 0.41735127568244934\n",
            "A-LLMRec model loss in epoch 1/2 iteration 614/1788: 0.1623700112104416\n",
            "A-LLMRec model loss in epoch 1/2 iteration 615/1788: 0.2669122815132141\n",
            "A-LLMRec model loss in epoch 1/2 iteration 616/1788: 0.2693612277507782\n",
            "A-LLMRec model loss in epoch 1/2 iteration 617/1788: 0.19854603707790375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 618/1788: 0.2269047200679779\n",
            "A-LLMRec model loss in epoch 1/2 iteration 619/1788: 0.45629987120628357\n",
            "A-LLMRec model loss in epoch 1/2 iteration 620/1788: 0.6919057965278625\n",
            "A-LLMRec model loss in epoch 1/2 iteration 621/1788: 0.38678690791130066\n",
            "A-LLMRec model loss in epoch 1/2 iteration 622/1788: 0.1645200252532959\n",
            "A-LLMRec model loss in epoch 1/2 iteration 623/1788: 0.06110406666994095\n",
            "A-LLMRec model loss in epoch 1/2 iteration 624/1788: 0.3844301104545593\n",
            "A-LLMRec model loss in epoch 1/2 iteration 625/1788: 0.133053258061409\n",
            "A-LLMRec model loss in epoch 1/2 iteration 626/1788: 0.3021571636199951\n",
            "A-LLMRec model loss in epoch 1/2 iteration 627/1788: 0.3327210247516632\n",
            "A-LLMRec model loss in epoch 1/2 iteration 628/1788: 0.21528500318527222\n",
            "A-LLMRec model loss in epoch 1/2 iteration 629/1788: 0.3791052997112274\n",
            "A-LLMRec model loss in epoch 1/2 iteration 630/1788: 0.8309600353240967\n",
            "A-LLMRec model loss in epoch 1/2 iteration 631/1788: 0.18863552808761597\n",
            "A-LLMRec model loss in epoch 1/2 iteration 632/1788: 0.1944914311170578\n",
            "A-LLMRec model loss in epoch 1/2 iteration 633/1788: 0.902983546257019\n",
            "A-LLMRec model loss in epoch 1/2 iteration 634/1788: 0.22215186059474945\n",
            "A-LLMRec model loss in epoch 1/2 iteration 635/1788: 0.2926837205886841\n",
            "A-LLMRec model loss in epoch 1/2 iteration 636/1788: 0.10216165333986282\n",
            "A-LLMRec model loss in epoch 1/2 iteration 637/1788: 0.13845781981945038\n",
            "A-LLMRec model loss in epoch 1/2 iteration 638/1788: 0.3576063811779022\n",
            "A-LLMRec model loss in epoch 1/2 iteration 639/1788: 0.31333792209625244\n",
            "A-LLMRec model loss in epoch 1/2 iteration 640/1788: 0.1966995745897293\n",
            "A-LLMRec model loss in epoch 1/2 iteration 641/1788: 0.1482371836900711\n",
            "A-LLMRec model loss in epoch 1/2 iteration 642/1788: 0.258556604385376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 643/1788: 0.19389720261096954\n",
            "A-LLMRec model loss in epoch 1/2 iteration 644/1788: 0.32305195927619934\n",
            "A-LLMRec model loss in epoch 1/2 iteration 645/1788: 0.1691449135541916\n",
            "A-LLMRec model loss in epoch 1/2 iteration 646/1788: 0.11440850794315338\n",
            "A-LLMRec model loss in epoch 1/2 iteration 647/1788: 0.28930428624153137\n",
            "A-LLMRec model loss in epoch 1/2 iteration 648/1788: 0.7147840857505798\n",
            "A-LLMRec model loss in epoch 1/2 iteration 649/1788: 0.27571460604667664\n",
            "A-LLMRec model loss in epoch 1/2 iteration 650/1788: 0.33265161514282227\n",
            "A-LLMRec model loss in epoch 1/2 iteration 651/1788: 0.31903231143951416\n",
            "A-LLMRec model loss in epoch 1/2 iteration 652/1788: 0.07904684543609619\n",
            "A-LLMRec model loss in epoch 1/2 iteration 653/1788: 0.1279330998659134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 654/1788: 0.11417270451784134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 655/1788: 0.42233702540397644\n",
            "A-LLMRec model loss in epoch 1/2 iteration 656/1788: 0.25305211544036865\n",
            "A-LLMRec model loss in epoch 1/2 iteration 657/1788: 0.12977659702301025\n",
            "A-LLMRec model loss in epoch 1/2 iteration 658/1788: 0.32641351222991943\n",
            "A-LLMRec model loss in epoch 1/2 iteration 659/1788: 0.1886429786682129\n",
            "A-LLMRec model loss in epoch 1/2 iteration 660/1788: 0.32444918155670166\n",
            "A-LLMRec model loss in epoch 1/2 iteration 661/1788: 0.2307886928319931\n",
            "A-LLMRec model loss in epoch 1/2 iteration 662/1788: 0.19417577981948853\n",
            "A-LLMRec model loss in epoch 1/2 iteration 663/1788: 0.5854246020317078\n",
            "A-LLMRec model loss in epoch 1/2 iteration 664/1788: 0.28264564275741577\n",
            "A-LLMRec model loss in epoch 1/2 iteration 665/1788: 0.587360143661499\n",
            "A-LLMRec model loss in epoch 1/2 iteration 666/1788: 0.1970079243183136\n",
            "A-LLMRec model loss in epoch 1/2 iteration 667/1788: 0.1431991457939148\n",
            "A-LLMRec model loss in epoch 1/2 iteration 668/1788: 0.08346927165985107\n",
            "A-LLMRec model loss in epoch 1/2 iteration 669/1788: 0.648371696472168\n",
            "A-LLMRec model loss in epoch 1/2 iteration 670/1788: 0.13555362820625305\n",
            "A-LLMRec model loss in epoch 1/2 iteration 671/1788: 0.16383567452430725\n",
            "A-LLMRec model loss in epoch 1/2 iteration 672/1788: 0.2762320637702942\n",
            "A-LLMRec model loss in epoch 1/2 iteration 673/1788: 0.8231358528137207\n",
            "A-LLMRec model loss in epoch 1/2 iteration 674/1788: 0.197461798787117\n",
            "A-LLMRec model loss in epoch 1/2 iteration 675/1788: 0.1800220012664795\n",
            "A-LLMRec model loss in epoch 1/2 iteration 676/1788: 0.11967618018388748\n",
            "A-LLMRec model loss in epoch 1/2 iteration 677/1788: 0.16504140198230743\n",
            "A-LLMRec model loss in epoch 1/2 iteration 678/1788: 0.37751588225364685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 679/1788: 0.3836885690689087\n",
            "A-LLMRec model loss in epoch 1/2 iteration 680/1788: 0.264460951089859\n",
            "A-LLMRec model loss in epoch 1/2 iteration 681/1788: 0.18943221867084503\n",
            "A-LLMRec model loss in epoch 1/2 iteration 682/1788: 0.31559449434280396\n",
            "A-LLMRec model loss in epoch 1/2 iteration 683/1788: 0.10179758071899414\n",
            "A-LLMRec model loss in epoch 1/2 iteration 684/1788: 0.1661359965801239\n",
            "A-LLMRec model loss in epoch 1/2 iteration 685/1788: 0.2632858455181122\n",
            "A-LLMRec model loss in epoch 1/2 iteration 686/1788: 0.23108507692813873\n",
            "A-LLMRec model loss in epoch 1/2 iteration 687/1788: 0.29680317640304565\n",
            "A-LLMRec model loss in epoch 1/2 iteration 688/1788: 0.12171465158462524\n",
            "A-LLMRec model loss in epoch 1/2 iteration 689/1788: 0.3906262516975403\n",
            "A-LLMRec model loss in epoch 1/2 iteration 690/1788: 0.17324550449848175\n",
            "A-LLMRec model loss in epoch 1/2 iteration 691/1788: 0.20767055451869965\n",
            "A-LLMRec model loss in epoch 1/2 iteration 692/1788: 0.11732149124145508\n",
            "A-LLMRec model loss in epoch 1/2 iteration 693/1788: 0.2796168625354767\n",
            "A-LLMRec model loss in epoch 1/2 iteration 694/1788: 0.38781195878982544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 695/1788: 0.25959575176239014\n",
            "A-LLMRec model loss in epoch 1/2 iteration 696/1788: 0.14931686222553253\n",
            "A-LLMRec model loss in epoch 1/2 iteration 697/1788: 0.48263588547706604\n",
            "A-LLMRec model loss in epoch 1/2 iteration 698/1788: 0.6357523798942566\n",
            "A-LLMRec model loss in epoch 1/2 iteration 699/1788: 0.09673593938350677\n",
            "A-LLMRec model loss in epoch 1/2 iteration 700/1788: 0.20083117485046387\n",
            "A-LLMRec model loss in epoch 1/2 iteration 701/1788: 0.18992651998996735\n",
            "A-LLMRec model loss in epoch 1/2 iteration 702/1788: 0.08688882738351822\n",
            "A-LLMRec model loss in epoch 1/2 iteration 703/1788: 0.3262941241264343\n",
            "A-LLMRec model loss in epoch 1/2 iteration 704/1788: 0.14361242949962616\n",
            "A-LLMRec model loss in epoch 1/2 iteration 705/1788: 0.19737936556339264\n",
            "A-LLMRec model loss in epoch 1/2 iteration 706/1788: 0.5135446190834045\n",
            "A-LLMRec model loss in epoch 1/2 iteration 707/1788: 0.23512305319309235\n",
            "A-LLMRec model loss in epoch 1/2 iteration 708/1788: 0.3617700934410095\n",
            "A-LLMRec model loss in epoch 1/2 iteration 709/1788: 0.2813810408115387\n",
            "A-LLMRec model loss in epoch 1/2 iteration 710/1788: 0.5693943500518799\n",
            "A-LLMRec model loss in epoch 1/2 iteration 711/1788: 0.32717669010162354\n",
            "A-LLMRec model loss in epoch 1/2 iteration 712/1788: 0.9279568791389465\n",
            "A-LLMRec model loss in epoch 1/2 iteration 713/1788: 0.14270158112049103\n",
            "A-LLMRec model loss in epoch 1/2 iteration 714/1788: 0.23662050068378448\n",
            "A-LLMRec model loss in epoch 1/2 iteration 715/1788: 0.15237517654895782\n",
            "A-LLMRec model loss in epoch 1/2 iteration 716/1788: 0.3640233278274536\n",
            "A-LLMRec model loss in epoch 1/2 iteration 717/1788: 0.31773436069488525\n",
            "A-LLMRec model loss in epoch 1/2 iteration 718/1788: 0.27877241373062134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 719/1788: 0.2799816131591797\n",
            "A-LLMRec model loss in epoch 1/2 iteration 720/1788: 0.4213360548019409\n",
            "A-LLMRec model loss in epoch 1/2 iteration 721/1788: 0.22419455647468567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 722/1788: 0.14004327356815338\n",
            "A-LLMRec model loss in epoch 1/2 iteration 723/1788: 0.6468904614448547\n",
            "A-LLMRec model loss in epoch 1/2 iteration 724/1788: 0.15660420060157776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 725/1788: 0.41220688819885254\n",
            "A-LLMRec model loss in epoch 1/2 iteration 726/1788: 0.21515624225139618\n",
            "A-LLMRec model loss in epoch 1/2 iteration 727/1788: 0.2661738395690918\n",
            "A-LLMRec model loss in epoch 1/2 iteration 728/1788: 0.06361666321754456\n",
            "A-LLMRec model loss in epoch 1/2 iteration 729/1788: 0.10781688988208771\n",
            "A-LLMRec model loss in epoch 1/2 iteration 730/1788: 0.1806260198354721\n",
            "A-LLMRec model loss in epoch 1/2 iteration 731/1788: 0.11095794290304184\n",
            "A-LLMRec model loss in epoch 1/2 iteration 732/1788: 0.07064276933670044\n",
            "A-LLMRec model loss in epoch 1/2 iteration 733/1788: 0.3284156620502472\n",
            "A-LLMRec model loss in epoch 1/2 iteration 734/1788: 0.1521017849445343\n",
            "A-LLMRec model loss in epoch 1/2 iteration 735/1788: 0.17166852951049805\n",
            "A-LLMRec model loss in epoch 1/2 iteration 736/1788: 0.5087605118751526\n",
            "A-LLMRec model loss in epoch 1/2 iteration 737/1788: 0.1482250988483429\n",
            "A-LLMRec model loss in epoch 1/2 iteration 738/1788: 0.18081356585025787\n",
            "A-LLMRec model loss in epoch 1/2 iteration 739/1788: 0.08457302302122116\n",
            "A-LLMRec model loss in epoch 1/2 iteration 740/1788: 0.13314859569072723\n",
            "A-LLMRec model loss in epoch 1/2 iteration 741/1788: 0.10797169804573059\n",
            "A-LLMRec model loss in epoch 1/2 iteration 742/1788: 0.3164835572242737\n",
            "A-LLMRec model loss in epoch 1/2 iteration 743/1788: 0.051022935658693314\n",
            "A-LLMRec model loss in epoch 1/2 iteration 744/1788: 0.5064988136291504\n",
            "A-LLMRec model loss in epoch 1/2 iteration 745/1788: 0.13187658786773682\n",
            "A-LLMRec model loss in epoch 1/2 iteration 746/1788: 0.23605722188949585\n",
            "A-LLMRec model loss in epoch 1/2 iteration 747/1788: 0.35277336835861206\n",
            "A-LLMRec model loss in epoch 1/2 iteration 748/1788: 0.2967084050178528\n",
            "A-LLMRec model loss in epoch 1/2 iteration 749/1788: 0.1601153463125229\n",
            "A-LLMRec model loss in epoch 1/2 iteration 750/1788: 0.557453989982605\n",
            "A-LLMRec model loss in epoch 1/2 iteration 751/1788: 0.15042737126350403\n",
            "A-LLMRec model loss in epoch 1/2 iteration 752/1788: 0.26711082458496094\n",
            "A-LLMRec model loss in epoch 1/2 iteration 753/1788: 0.19742299616336823\n",
            "A-LLMRec model loss in epoch 1/2 iteration 754/1788: 0.18987025320529938\n",
            "A-LLMRec model loss in epoch 1/2 iteration 755/1788: 0.3334074318408966\n",
            "A-LLMRec model loss in epoch 1/2 iteration 756/1788: 0.227455273270607\n",
            "A-LLMRec model loss in epoch 1/2 iteration 757/1788: 0.3226211965084076\n",
            "A-LLMRec model loss in epoch 1/2 iteration 758/1788: 0.14193810522556305\n",
            "A-LLMRec model loss in epoch 1/2 iteration 759/1788: 0.2004304826259613\n",
            "A-LLMRec model loss in epoch 1/2 iteration 760/1788: 0.27130281925201416\n",
            "A-LLMRec model loss in epoch 1/2 iteration 761/1788: 0.14377188682556152\n",
            "A-LLMRec model loss in epoch 1/2 iteration 762/1788: 0.042848531156778336\n",
            "A-LLMRec model loss in epoch 1/2 iteration 763/1788: 0.1721017062664032\n",
            "A-LLMRec model loss in epoch 1/2 iteration 764/1788: 0.24785661697387695\n",
            "A-LLMRec model loss in epoch 1/2 iteration 765/1788: 0.488090842962265\n",
            "A-LLMRec model loss in epoch 1/2 iteration 766/1788: 0.13080468773841858\n",
            "A-LLMRec model loss in epoch 1/2 iteration 767/1788: 0.1809055507183075\n",
            "A-LLMRec model loss in epoch 1/2 iteration 768/1788: 0.08551695942878723\n",
            "A-LLMRec model loss in epoch 1/2 iteration 769/1788: 0.20849274098873138\n",
            "A-LLMRec model loss in epoch 1/2 iteration 770/1788: 0.2316226214170456\n",
            "A-LLMRec model loss in epoch 1/2 iteration 771/1788: 0.14035002887248993\n",
            "A-LLMRec model loss in epoch 1/2 iteration 772/1788: 0.11867525428533554\n",
            "A-LLMRec model loss in epoch 1/2 iteration 773/1788: 0.3287915885448456\n",
            "A-LLMRec model loss in epoch 1/2 iteration 774/1788: 0.09257403016090393\n",
            "A-LLMRec model loss in epoch 1/2 iteration 775/1788: 0.14816822111606598\n",
            "A-LLMRec model loss in epoch 1/2 iteration 776/1788: 0.0853458121418953\n",
            "A-LLMRec model loss in epoch 1/2 iteration 777/1788: 0.1662561148405075\n",
            "A-LLMRec model loss in epoch 1/2 iteration 778/1788: 0.4337981045246124\n",
            "A-LLMRec model loss in epoch 1/2 iteration 779/1788: 0.25852030515670776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 780/1788: 0.4527498781681061\n",
            "A-LLMRec model loss in epoch 1/2 iteration 781/1788: 0.4195275902748108\n",
            "A-LLMRec model loss in epoch 1/2 iteration 782/1788: 0.19520336389541626\n",
            "A-LLMRec model loss in epoch 1/2 iteration 783/1788: 0.12280449271202087\n",
            "A-LLMRec model loss in epoch 1/2 iteration 784/1788: 0.08779532462358475\n",
            "A-LLMRec model loss in epoch 1/2 iteration 785/1788: 0.21037928760051727\n",
            "A-LLMRec model loss in epoch 1/2 iteration 786/1788: 0.22052662074565887\n",
            "A-LLMRec model loss in epoch 1/2 iteration 787/1788: 0.2300185263156891\n",
            "A-LLMRec model loss in epoch 1/2 iteration 788/1788: 0.35697001218795776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 789/1788: 0.16211500763893127\n",
            "A-LLMRec model loss in epoch 1/2 iteration 790/1788: 0.16973312199115753\n",
            "A-LLMRec model loss in epoch 1/2 iteration 791/1788: 0.1472381055355072\n",
            "A-LLMRec model loss in epoch 1/2 iteration 792/1788: 0.43349772691726685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 793/1788: 0.30360761284828186\n",
            "A-LLMRec model loss in epoch 1/2 iteration 794/1788: 0.2674759030342102\n",
            "A-LLMRec model loss in epoch 1/2 iteration 795/1788: 0.17805933952331543\n",
            "A-LLMRec model loss in epoch 1/2 iteration 796/1788: 0.09358454495668411\n",
            "A-LLMRec model loss in epoch 1/2 iteration 797/1788: 0.28970879316329956\n",
            "A-LLMRec model loss in epoch 1/2 iteration 798/1788: 0.2550339698791504\n",
            "A-LLMRec model loss in epoch 1/2 iteration 799/1788: 0.13689470291137695\n",
            "A-LLMRec model loss in epoch 1/2 iteration 800/1788: 0.170189768075943\n",
            "A-LLMRec model loss in epoch 1/2 iteration 801/1788: 0.5215728282928467\n",
            "A-LLMRec model loss in epoch 1/2 iteration 802/1788: 0.10988660156726837\n",
            "A-LLMRec model loss in epoch 1/2 iteration 803/1788: 0.2739289104938507\n",
            "A-LLMRec model loss in epoch 1/2 iteration 804/1788: 0.2421039342880249\n",
            "A-LLMRec model loss in epoch 1/2 iteration 805/1788: 0.21027503907680511\n",
            "A-LLMRec model loss in epoch 1/2 iteration 806/1788: 0.20667897164821625\n",
            "A-LLMRec model loss in epoch 1/2 iteration 807/1788: 0.23528604209423065\n",
            "A-LLMRec model loss in epoch 1/2 iteration 808/1788: 0.5590003132820129\n",
            "A-LLMRec model loss in epoch 1/2 iteration 809/1788: 0.3026147186756134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 810/1788: 0.17935264110565186\n",
            "A-LLMRec model loss in epoch 1/2 iteration 811/1788: 0.16796354949474335\n",
            "A-LLMRec model loss in epoch 1/2 iteration 812/1788: 0.3416232764720917\n",
            "A-LLMRec model loss in epoch 1/2 iteration 813/1788: 0.3215762674808502\n",
            "A-LLMRec model loss in epoch 1/2 iteration 814/1788: 0.35128486156463623\n",
            "A-LLMRec model loss in epoch 1/2 iteration 815/1788: 0.47001439332962036\n",
            "A-LLMRec model loss in epoch 1/2 iteration 816/1788: 0.14259135723114014\n",
            "A-LLMRec model loss in epoch 1/2 iteration 817/1788: 0.5320397615432739\n",
            "A-LLMRec model loss in epoch 1/2 iteration 818/1788: 0.3414608836174011\n",
            "A-LLMRec model loss in epoch 1/2 iteration 819/1788: 0.30149438977241516\n",
            "A-LLMRec model loss in epoch 1/2 iteration 820/1788: 0.19634778797626495\n",
            "A-LLMRec model loss in epoch 1/2 iteration 821/1788: 0.14762595295906067\n",
            "A-LLMRec model loss in epoch 1/2 iteration 822/1788: 0.15682555735111237\n",
            "A-LLMRec model loss in epoch 1/2 iteration 823/1788: 0.1415885090827942\n",
            "A-LLMRec model loss in epoch 1/2 iteration 824/1788: 0.20593620836734772\n",
            "A-LLMRec model loss in epoch 1/2 iteration 825/1788: 0.7181754112243652\n",
            "A-LLMRec model loss in epoch 1/2 iteration 826/1788: 0.20405595004558563\n",
            "A-LLMRec model loss in epoch 1/2 iteration 827/1788: 0.0952950045466423\n",
            "A-LLMRec model loss in epoch 1/2 iteration 828/1788: 0.146199569106102\n",
            "A-LLMRec model loss in epoch 1/2 iteration 829/1788: 0.2620241045951843\n",
            "A-LLMRec model loss in epoch 1/2 iteration 830/1788: 0.35901349782943726\n",
            "A-LLMRec model loss in epoch 1/2 iteration 831/1788: 0.10193072259426117\n",
            "A-LLMRec model loss in epoch 1/2 iteration 832/1788: 0.12942197918891907\n",
            "A-LLMRec model loss in epoch 1/2 iteration 833/1788: 0.2949933111667633\n",
            "A-LLMRec model loss in epoch 1/2 iteration 834/1788: 0.11713992804288864\n",
            "A-LLMRec model loss in epoch 1/2 iteration 835/1788: 0.6226367354393005\n",
            "A-LLMRec model loss in epoch 1/2 iteration 836/1788: 0.29200682044029236\n",
            "A-LLMRec model loss in epoch 1/2 iteration 837/1788: 0.11502674967050552\n",
            "A-LLMRec model loss in epoch 1/2 iteration 838/1788: 0.3578677475452423\n",
            "A-LLMRec model loss in epoch 1/2 iteration 839/1788: 0.1740799844264984\n",
            "A-LLMRec model loss in epoch 1/2 iteration 840/1788: 0.3259525001049042\n",
            "A-LLMRec model loss in epoch 1/2 iteration 841/1788: 0.3312723934650421\n",
            "A-LLMRec model loss in epoch 1/2 iteration 842/1788: 0.12935253977775574\n",
            "A-LLMRec model loss in epoch 1/2 iteration 843/1788: 0.36709704995155334\n",
            "A-LLMRec model loss in epoch 1/2 iteration 844/1788: 0.1428162157535553\n",
            "A-LLMRec model loss in epoch 1/2 iteration 845/1788: 0.1566189080476761\n",
            "A-LLMRec model loss in epoch 1/2 iteration 846/1788: 0.49927079677581787\n",
            "A-LLMRec model loss in epoch 1/2 iteration 847/1788: 0.6028498411178589\n",
            "A-LLMRec model loss in epoch 1/2 iteration 848/1788: 0.19061653316020966\n",
            "A-LLMRec model loss in epoch 1/2 iteration 849/1788: 0.3354681730270386\n",
            "A-LLMRec model loss in epoch 1/2 iteration 850/1788: 0.617950439453125\n",
            "A-LLMRec model loss in epoch 1/2 iteration 851/1788: 0.2110407054424286\n",
            "A-LLMRec model loss in epoch 1/2 iteration 852/1788: 0.1934460699558258\n",
            "A-LLMRec model loss in epoch 1/2 iteration 853/1788: 0.34727898240089417\n",
            "A-LLMRec model loss in epoch 1/2 iteration 854/1788: 0.22692523896694183\n",
            "A-LLMRec model loss in epoch 1/2 iteration 855/1788: 0.24664588272571564\n",
            "A-LLMRec model loss in epoch 1/2 iteration 856/1788: 0.51068115234375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 857/1788: 0.2936665117740631\n",
            "A-LLMRec model loss in epoch 1/2 iteration 858/1788: 0.2830410897731781\n",
            "A-LLMRec model loss in epoch 1/2 iteration 859/1788: 0.41005057096481323\n",
            "A-LLMRec model loss in epoch 1/2 iteration 860/1788: 0.1570855975151062\n",
            "A-LLMRec model loss in epoch 1/2 iteration 861/1788: 0.08962751179933548\n",
            "A-LLMRec model loss in epoch 1/2 iteration 862/1788: 0.10750745981931686\n",
            "A-LLMRec model loss in epoch 1/2 iteration 863/1788: 0.25267842411994934\n",
            "A-LLMRec model loss in epoch 1/2 iteration 864/1788: 0.24340273439884186\n",
            "A-LLMRec model loss in epoch 1/2 iteration 865/1788: 0.09461434930562973\n",
            "A-LLMRec model loss in epoch 1/2 iteration 866/1788: 0.06469853967428207\n",
            "A-LLMRec model loss in epoch 1/2 iteration 867/1788: 0.18099017441272736\n",
            "A-LLMRec model loss in epoch 1/2 iteration 868/1788: 0.19637531042099\n",
            "A-LLMRec model loss in epoch 1/2 iteration 869/1788: 0.021072527393698692\n",
            "A-LLMRec model loss in epoch 1/2 iteration 870/1788: 0.2555427551269531\n",
            "A-LLMRec model loss in epoch 1/2 iteration 871/1788: 0.5569595694541931\n",
            "A-LLMRec model loss in epoch 1/2 iteration 872/1788: 0.13232335448265076\n",
            "A-LLMRec model loss in epoch 1/2 iteration 873/1788: 0.7332916855812073\n",
            "A-LLMRec model loss in epoch 1/2 iteration 874/1788: 0.2935695946216583\n",
            "A-LLMRec model loss in epoch 1/2 iteration 875/1788: 0.1743922382593155\n",
            "A-LLMRec model loss in epoch 1/2 iteration 876/1788: 0.21576222777366638\n",
            "A-LLMRec model loss in epoch 1/2 iteration 877/1788: 0.052335869520902634\n",
            "A-LLMRec model loss in epoch 1/2 iteration 878/1788: 0.20852237939834595\n",
            "A-LLMRec model loss in epoch 1/2 iteration 879/1788: 0.4120630621910095\n",
            "A-LLMRec model loss in epoch 1/2 iteration 880/1788: 0.2172444462776184\n",
            "A-LLMRec model loss in epoch 1/2 iteration 881/1788: 0.21028567850589752\n",
            "A-LLMRec model loss in epoch 1/2 iteration 882/1788: 0.23795391619205475\n",
            "A-LLMRec model loss in epoch 1/2 iteration 883/1788: 0.11743617057800293\n",
            "A-LLMRec model loss in epoch 1/2 iteration 884/1788: 0.09426391869783401\n",
            "A-LLMRec model loss in epoch 1/2 iteration 885/1788: 0.21243801712989807\n",
            "A-LLMRec model loss in epoch 1/2 iteration 886/1788: 0.3477920591831207\n",
            "A-LLMRec model loss in epoch 1/2 iteration 887/1788: 0.045480988919734955\n",
            "A-LLMRec model loss in epoch 1/2 iteration 888/1788: 0.1461477428674698\n",
            "A-LLMRec model loss in epoch 1/2 iteration 889/1788: 0.2725532352924347\n",
            "A-LLMRec model loss in epoch 1/2 iteration 890/1788: 0.3438991904258728\n",
            "A-LLMRec model loss in epoch 1/2 iteration 891/1788: 0.09734425693750381\n",
            "A-LLMRec model loss in epoch 1/2 iteration 892/1788: 0.48275190591812134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 893/1788: 0.4999191164970398\n",
            "A-LLMRec model loss in epoch 1/2 iteration 894/1788: 0.11048732697963715\n",
            "A-LLMRec model loss in epoch 1/2 iteration 895/1788: 0.31271860003471375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 896/1788: 0.20253659784793854\n",
            "A-LLMRec model loss in epoch 1/2 iteration 897/1788: 0.13628268241882324\n",
            "A-LLMRec model loss in epoch 1/2 iteration 898/1788: 0.2933071553707123\n",
            "A-LLMRec model loss in epoch 1/2 iteration 899/1788: 0.30404314398765564\n",
            "A-LLMRec model loss in epoch 1/2 iteration 900/1788: 0.18535585701465607\n",
            "A-LLMRec model loss in epoch 1/2 iteration 901/1788: 0.31164291501045227\n",
            "A-LLMRec model loss in epoch 1/2 iteration 902/1788: 0.08404390513896942\n",
            "A-LLMRec model loss in epoch 1/2 iteration 903/1788: 0.6165274977684021\n",
            "A-LLMRec model loss in epoch 1/2 iteration 904/1788: 0.2072652131319046\n",
            "A-LLMRec model loss in epoch 1/2 iteration 905/1788: 0.9148933291435242\n",
            "A-LLMRec model loss in epoch 1/2 iteration 906/1788: 0.25918933749198914\n",
            "A-LLMRec model loss in epoch 1/2 iteration 907/1788: 0.41036659479141235\n",
            "A-LLMRec model loss in epoch 1/2 iteration 908/1788: 0.13818348944187164\n",
            "A-LLMRec model loss in epoch 1/2 iteration 909/1788: 0.23623961210250854\n",
            "A-LLMRec model loss in epoch 1/2 iteration 910/1788: 0.24466313421726227\n",
            "A-LLMRec model loss in epoch 1/2 iteration 911/1788: 0.34317347407341003\n",
            "A-LLMRec model loss in epoch 1/2 iteration 912/1788: 0.1312892884016037\n",
            "A-LLMRec model loss in epoch 1/2 iteration 913/1788: 0.2136543095111847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 914/1788: 0.1930191069841385\n",
            "A-LLMRec model loss in epoch 1/2 iteration 915/1788: 0.3020833432674408\n",
            "A-LLMRec model loss in epoch 1/2 iteration 916/1788: 0.5744249224662781\n",
            "A-LLMRec model loss in epoch 1/2 iteration 917/1788: 0.4701155126094818\n",
            "A-LLMRec model loss in epoch 1/2 iteration 918/1788: 0.1471078246831894\n",
            "A-LLMRec model loss in epoch 1/2 iteration 919/1788: 0.22532248497009277\n",
            "A-LLMRec model loss in epoch 1/2 iteration 920/1788: 0.4652154743671417\n",
            "A-LLMRec model loss in epoch 1/2 iteration 921/1788: 0.5563467144966125\n",
            "A-LLMRec model loss in epoch 1/2 iteration 922/1788: 0.20681287348270416\n",
            "A-LLMRec model loss in epoch 1/2 iteration 923/1788: 0.42764967679977417\n",
            "A-LLMRec model loss in epoch 1/2 iteration 924/1788: 0.1607799381017685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 925/1788: 0.10567682236433029\n",
            "A-LLMRec model loss in epoch 1/2 iteration 926/1788: 0.3457280099391937\n",
            "A-LLMRec model loss in epoch 1/2 iteration 927/1788: 0.140486478805542\n",
            "A-LLMRec model loss in epoch 1/2 iteration 928/1788: 0.24066585302352905\n",
            "A-LLMRec model loss in epoch 1/2 iteration 929/1788: 0.23502963781356812\n",
            "A-LLMRec model loss in epoch 1/2 iteration 930/1788: 0.24128225445747375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 931/1788: 0.5440012812614441\n",
            "A-LLMRec model loss in epoch 1/2 iteration 932/1788: 0.2474249303340912\n",
            "A-LLMRec model loss in epoch 1/2 iteration 933/1788: 0.3913753628730774\n",
            "A-LLMRec model loss in epoch 1/2 iteration 934/1788: 0.4327588975429535\n",
            "A-LLMRec model loss in epoch 1/2 iteration 935/1788: 0.09480337053537369\n",
            "A-LLMRec model loss in epoch 1/2 iteration 936/1788: 0.3504343330860138\n",
            "A-LLMRec model loss in epoch 1/2 iteration 937/1788: 0.3053208589553833\n",
            "A-LLMRec model loss in epoch 1/2 iteration 938/1788: 0.20747649669647217\n",
            "A-LLMRec model loss in epoch 1/2 iteration 939/1788: 0.17132702469825745\n",
            "A-LLMRec model loss in epoch 1/2 iteration 940/1788: 0.1815713495016098\n",
            "A-LLMRec model loss in epoch 1/2 iteration 941/1788: 0.17227134108543396\n",
            "A-LLMRec model loss in epoch 1/2 iteration 942/1788: 0.42741820216178894\n",
            "A-LLMRec model loss in epoch 1/2 iteration 943/1788: 0.2119206041097641\n",
            "A-LLMRec model loss in epoch 1/2 iteration 944/1788: 0.09506015479564667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 945/1788: 0.1978934109210968\n",
            "A-LLMRec model loss in epoch 1/2 iteration 946/1788: 0.2198508381843567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 947/1788: 0.22336331009864807\n",
            "A-LLMRec model loss in epoch 1/2 iteration 948/1788: 0.10484040528535843\n",
            "A-LLMRec model loss in epoch 1/2 iteration 949/1788: 0.1168929785490036\n",
            "A-LLMRec model loss in epoch 1/2 iteration 950/1788: 0.143959641456604\n",
            "A-LLMRec model loss in epoch 1/2 iteration 951/1788: 0.168387308716774\n",
            "A-LLMRec model loss in epoch 1/2 iteration 952/1788: 0.46425291895866394\n",
            "A-LLMRec model loss in epoch 1/2 iteration 953/1788: 0.22480788826942444\n",
            "A-LLMRec model loss in epoch 1/2 iteration 954/1788: 0.22240965068340302\n",
            "A-LLMRec model loss in epoch 1/2 iteration 955/1788: 0.15281200408935547\n",
            "A-LLMRec model loss in epoch 1/2 iteration 956/1788: 0.15185031294822693\n",
            "A-LLMRec model loss in epoch 1/2 iteration 957/1788: 0.16255399584770203\n",
            "A-LLMRec model loss in epoch 1/2 iteration 958/1788: 0.49043625593185425\n",
            "A-LLMRec model loss in epoch 1/2 iteration 959/1788: 0.22758668661117554\n",
            "A-LLMRec model loss in epoch 1/2 iteration 960/1788: 0.21247249841690063\n",
            "A-LLMRec model loss in epoch 1/2 iteration 961/1788: 0.1889575719833374\n",
            "A-LLMRec model loss in epoch 1/2 iteration 962/1788: 0.3235242962837219\n",
            "A-LLMRec model loss in epoch 1/2 iteration 963/1788: 0.18629790842533112\n",
            "A-LLMRec model loss in epoch 1/2 iteration 964/1788: 0.1486656665802002\n",
            "A-LLMRec model loss in epoch 1/2 iteration 965/1788: 0.3337203860282898\n",
            "A-LLMRec model loss in epoch 1/2 iteration 966/1788: 0.1846579760313034\n",
            "A-LLMRec model loss in epoch 1/2 iteration 967/1788: 0.25920018553733826\n",
            "A-LLMRec model loss in epoch 1/2 iteration 968/1788: 0.25712850689888\n",
            "A-LLMRec model loss in epoch 1/2 iteration 969/1788: 0.22248214483261108\n",
            "A-LLMRec model loss in epoch 1/2 iteration 970/1788: 0.244115948677063\n",
            "A-LLMRec model loss in epoch 1/2 iteration 971/1788: 0.4094782769680023\n",
            "A-LLMRec model loss in epoch 1/2 iteration 972/1788: 0.38355007767677307\n",
            "A-LLMRec model loss in epoch 1/2 iteration 973/1788: 0.22262942790985107\n",
            "A-LLMRec model loss in epoch 1/2 iteration 974/1788: 0.2972978353500366\n",
            "A-LLMRec model loss in epoch 1/2 iteration 975/1788: 0.21423475444316864\n",
            "A-LLMRec model loss in epoch 1/2 iteration 976/1788: 0.11689914762973785\n",
            "A-LLMRec model loss in epoch 1/2 iteration 977/1788: 0.27833741903305054\n",
            "A-LLMRec model loss in epoch 1/2 iteration 978/1788: 0.2234557420015335\n",
            "A-LLMRec model loss in epoch 1/2 iteration 979/1788: 0.08288829028606415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 980/1788: 0.12549178302288055\n",
            "A-LLMRec model loss in epoch 1/2 iteration 981/1788: 0.1806657612323761\n",
            "A-LLMRec model loss in epoch 1/2 iteration 982/1788: 0.23422247171401978\n",
            "A-LLMRec model loss in epoch 1/2 iteration 983/1788: 0.5570019483566284\n",
            "A-LLMRec model loss in epoch 1/2 iteration 984/1788: 0.24518944323062897\n",
            "A-LLMRec model loss in epoch 1/2 iteration 985/1788: 0.4062047302722931\n",
            "A-LLMRec model loss in epoch 1/2 iteration 986/1788: 0.3155093789100647\n",
            "A-LLMRec model loss in epoch 1/2 iteration 987/1788: 0.3505440950393677\n",
            "A-LLMRec model loss in epoch 1/2 iteration 988/1788: 0.29891642928123474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 989/1788: 0.35186275839805603\n",
            "A-LLMRec model loss in epoch 1/2 iteration 990/1788: 0.07065244019031525\n",
            "A-LLMRec model loss in epoch 1/2 iteration 991/1788: 0.5001611709594727\n",
            "A-LLMRec model loss in epoch 1/2 iteration 992/1788: 0.23558878898620605\n",
            "A-LLMRec model loss in epoch 1/2 iteration 993/1788: 0.17859067022800446\n",
            "A-LLMRec model loss in epoch 1/2 iteration 994/1788: 0.45211687684059143\n",
            "A-LLMRec model loss in epoch 1/2 iteration 995/1788: 0.2427045851945877\n",
            "A-LLMRec model loss in epoch 1/2 iteration 996/1788: 0.29289865493774414\n",
            "A-LLMRec model loss in epoch 1/2 iteration 997/1788: 0.2657512128353119\n",
            "A-LLMRec model loss in epoch 1/2 iteration 998/1788: 0.1333453208208084\n",
            "A-LLMRec model loss in epoch 1/2 iteration 999/1788: 0.12649700045585632\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1000/1788: 0.30250754952430725\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1001/1788: 0.12506331503391266\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1002/1788: 0.20126616954803467\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1003/1788: 0.3234579265117645\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1004/1788: 0.13511739671230316\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1005/1788: 0.1726643294095993\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1006/1788: 0.18771223723888397\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1007/1788: 0.14698228240013123\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1008/1788: 0.054612912237644196\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1009/1788: 0.895465075969696\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1010/1788: 0.08009073138237\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1011/1788: 0.2615223228931427\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1012/1788: 0.16639699041843414\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1013/1788: 0.11021549999713898\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1014/1788: 0.14900965988636017\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1015/1788: 0.06439840793609619\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1016/1788: 0.44080978631973267\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1017/1788: 0.40008190274238586\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1018/1788: 0.1766470968723297\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1019/1788: 0.1720775067806244\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1020/1788: 0.2951015830039978\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1021/1788: 0.13848818838596344\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1022/1788: 0.33984705805778503\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1023/1788: 0.2890176475048065\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1024/1788: 0.18480326235294342\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1025/1788: 0.24858611822128296\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1026/1788: 0.43413984775543213\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1027/1788: 0.1907571703195572\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1028/1788: 0.18758037686347961\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1029/1788: 0.30863067507743835\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1030/1788: 0.12310700863599777\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1031/1788: 0.15132707357406616\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1032/1788: 0.2813858389854431\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1033/1788: 0.5024695992469788\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1034/1788: 0.20288391411304474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1035/1788: 0.21928904950618744\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1036/1788: 0.3471758961677551\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1037/1788: 0.20391827821731567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1038/1788: 0.6793994903564453\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1039/1788: 0.301589697599411\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1040/1788: 0.1994885802268982\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1041/1788: 0.22056514024734497\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1042/1788: 0.2172270268201828\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1043/1788: 0.43965068459510803\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1044/1788: 0.08727674186229706\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1045/1788: 0.44846802949905396\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1046/1788: 0.41514602303504944\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1047/1788: 0.22550904750823975\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1048/1788: 0.5224858522415161\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1049/1788: 0.1302880495786667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1050/1788: 0.25897955894470215\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1051/1788: 0.37376976013183594\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1052/1788: 0.2041693925857544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1053/1788: 0.31175702810287476\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1054/1788: 0.21686510741710663\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1055/1788: 0.20154725015163422\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1056/1788: 0.2762909531593323\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1057/1788: 0.191547229886055\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1058/1788: 0.14711551368236542\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1059/1788: 0.16637106239795685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1060/1788: 0.2548133134841919\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1061/1788: 0.6005116701126099\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1062/1788: 0.15812042355537415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1063/1788: 0.2921036183834076\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1064/1788: 0.07686002552509308\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1065/1788: 0.08674043416976929\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1066/1788: 0.36487963795661926\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1067/1788: 0.19882811605930328\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1068/1788: 0.11861518025398254\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1069/1788: 0.12463279068470001\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1070/1788: 0.2715591788291931\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1071/1788: 0.12400824576616287\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1072/1788: 0.3341665267944336\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1073/1788: 0.8278018832206726\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1074/1788: 0.3245084285736084\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1075/1788: 0.1855340152978897\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1076/1788: 0.20032437145709991\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1077/1788: 0.11703773587942123\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1078/1788: 0.2322729378938675\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1079/1788: 0.2674739360809326\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1080/1788: 0.5440889596939087\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1081/1788: 0.14441519975662231\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1082/1788: 0.14925231039524078\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1083/1788: 0.21952232718467712\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1084/1788: 0.18611136078834534\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1085/1788: 0.3054267168045044\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1086/1788: 0.14532679319381714\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1087/1788: 0.3277161121368408\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1088/1788: 0.0866997018456459\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1089/1788: 0.2963949143886566\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1090/1788: 0.1885301172733307\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1091/1788: 0.25735554099082947\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1092/1788: 0.23388491570949554\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1093/1788: 0.3028263747692108\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1094/1788: 0.31822335720062256\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1095/1788: 0.42227599024772644\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1096/1788: 0.09871172159910202\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1097/1788: 0.21609677374362946\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1098/1788: 0.12211107462644577\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1099/1788: 0.5991447567939758\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1100/1788: 0.2336021512746811\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1101/1788: 0.18812136352062225\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1102/1788: 0.28836265206336975\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1103/1788: 0.22944720089435577\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1104/1788: 0.0993536040186882\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1105/1788: 0.3253154158592224\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1106/1788: 0.28593695163726807\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1107/1788: 0.3352932631969452\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1108/1788: 0.08813855051994324\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1109/1788: 0.1534706950187683\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1110/1788: 0.25892218947410583\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1111/1788: 0.2729857265949249\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1112/1788: 0.17026031017303467\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1113/1788: 0.4322988986968994\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1114/1788: 0.48654094338417053\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1115/1788: 0.43782100081443787\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1116/1788: 0.3263106644153595\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1117/1788: 0.20842473208904266\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1118/1788: 0.26152846217155457\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1119/1788: 0.23450231552124023\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1120/1788: 0.1286364048719406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1121/1788: 0.06667420268058777\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1122/1788: 0.18200047314167023\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1123/1788: 0.35065412521362305\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1124/1788: 0.5652636289596558\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1125/1788: 0.4118063151836395\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1126/1788: 0.19423560798168182\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1127/1788: 0.0686427429318428\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1128/1788: 0.08295118063688278\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1129/1788: 0.1869860142469406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1130/1788: 0.20736224949359894\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1131/1788: 0.19178172945976257\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1132/1788: 0.2780769467353821\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1133/1788: 0.2800230085849762\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1134/1788: 0.2753036618232727\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1135/1788: 0.1805417686700821\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1136/1788: 0.302070289850235\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1137/1788: 0.37542927265167236\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1138/1788: 0.36492371559143066\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1139/1788: 0.22243332862854004\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1140/1788: 0.16098733246326447\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1141/1788: 0.3133610486984253\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1142/1788: 0.4262191653251648\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1143/1788: 0.17263637483119965\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1144/1788: 0.16788291931152344\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1145/1788: 0.20864258706569672\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1146/1788: 0.08817939460277557\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1147/1788: 0.21779537200927734\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1148/1788: 0.36103734374046326\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1149/1788: 0.22638869285583496\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1150/1788: 0.20023392140865326\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1151/1788: 0.21702441573143005\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1152/1788: 0.2126312553882599\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1153/1788: 0.31592074036598206\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1154/1788: 0.19577504694461823\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1155/1788: 0.42600953578948975\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1156/1788: 0.34186625480651855\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1157/1788: 0.382619708776474\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1158/1788: 0.27949410676956177\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1159/1788: 0.19182786345481873\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1160/1788: 0.15387368202209473\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1161/1788: 0.14500656723976135\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1162/1788: 0.2498197853565216\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1163/1788: 0.17920777201652527\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1164/1788: 0.30529606342315674\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1165/1788: 0.0930771678686142\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1166/1788: 0.24133944511413574\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1167/1788: 0.14474977552890778\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1168/1788: 0.28002041578292847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1169/1788: 0.23716609179973602\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1170/1788: 0.11898630857467651\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1171/1788: 0.0727212056517601\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1172/1788: 0.075261689722538\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1173/1788: 0.20953227579593658\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1174/1788: 0.08055969327688217\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1175/1788: 0.3201550543308258\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1176/1788: 0.30986613035202026\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1177/1788: 0.3310508728027344\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1178/1788: 0.23243838548660278\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1179/1788: 0.26489606499671936\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1180/1788: 0.17217811942100525\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1181/1788: 0.28746291995048523\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1182/1788: 0.2193896472454071\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1183/1788: 0.19108431041240692\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1184/1788: 0.1343861222267151\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1185/1788: 0.12054813653230667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1186/1788: 0.32853519916534424\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1187/1788: 0.06307735294103622\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1188/1788: 0.15210561454296112\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1189/1788: 0.12999217212200165\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1190/1788: 0.30642667412757874\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1191/1788: 0.02168858051300049\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1192/1788: 0.4507019817829132\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1193/1788: 0.09150975942611694\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1194/1788: 0.298395037651062\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1195/1788: 0.3324265480041504\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1196/1788: 0.08499659597873688\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1197/1788: 0.2161414474248886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1198/1788: 0.3385152220726013\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1199/1788: 0.3426752984523773\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1200/1788: 0.03211737424135208\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1201/1788: 0.4494864046573639\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1202/1788: 0.18634574115276337\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1203/1788: 0.6092601418495178\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1204/1788: 0.18870994448661804\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1205/1788: 0.20688281953334808\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1206/1788: 0.18185678124427795\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1207/1788: 0.38701117038726807\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1208/1788: 0.04100697115063667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1209/1788: 0.37076789140701294\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1210/1788: 0.39332324266433716\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1211/1788: 0.2484971135854721\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1212/1788: 0.1318245530128479\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1213/1788: 0.4438127875328064\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1214/1788: 0.20726308226585388\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1215/1788: 0.8493053913116455\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1216/1788: 0.6504271626472473\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1217/1788: 0.24081194400787354\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1218/1788: 0.1588849127292633\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1219/1788: 0.1076623871922493\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1220/1788: 0.23780909180641174\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1221/1788: 0.28794798254966736\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1222/1788: 0.37608423829078674\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1223/1788: 0.3328152894973755\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1224/1788: 0.28856921195983887\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1225/1788: 0.16978217661380768\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1226/1788: 0.18071547150611877\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1227/1788: 0.28298282623291016\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1228/1788: 0.13271786272525787\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1229/1788: 0.3222038447856903\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1230/1788: 0.28499388694763184\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1231/1788: 0.35151538252830505\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1232/1788: 0.23773981630802155\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1233/1788: 0.18106640875339508\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1234/1788: 0.21819882094860077\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1235/1788: 0.1501258909702301\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1236/1788: 0.30882391333580017\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1237/1788: 0.46211567521095276\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1238/1788: 0.3499894142150879\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1239/1788: 0.22149361670017242\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1240/1788: 0.24692204594612122\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1241/1788: 0.18684141337871552\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1242/1788: 0.41889914870262146\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1243/1788: 0.2637794017791748\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1244/1788: 0.21858583390712738\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1245/1788: 0.1330682635307312\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1246/1788: 0.17027729749679565\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1247/1788: 0.16279354691505432\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1248/1788: 0.07909556478261948\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1249/1788: 0.2577857971191406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1250/1788: 0.21362808346748352\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1251/1788: 0.27284660935401917\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1252/1788: 0.07614424079656601\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1253/1788: 0.25237351655960083\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1254/1788: 0.5655975937843323\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1255/1788: 0.17107994854450226\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1256/1788: 0.153139129281044\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1257/1788: 0.24532140791416168\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1258/1788: 0.1486639529466629\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1259/1788: 0.191580668091774\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1260/1788: 0.2885163426399231\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1261/1788: 0.37105846405029297\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1262/1788: 0.1969769299030304\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1263/1788: 0.3726968467235565\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1264/1788: 0.06217604875564575\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1265/1788: 0.381826251745224\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1266/1788: 0.08603066205978394\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1267/1788: 0.384973406791687\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1268/1788: 0.09218147397041321\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1269/1788: 0.3171829283237457\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1270/1788: 0.18809567391872406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1271/1788: 0.1703735888004303\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1272/1788: 0.03183605894446373\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1273/1788: 0.9359034299850464\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1274/1788: 0.21019625663757324\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1275/1788: 0.22723031044006348\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1276/1788: 0.21684923768043518\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1277/1788: 0.08778436481952667\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1278/1788: 0.23986409604549408\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1279/1788: 0.21717409789562225\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1280/1788: 0.2269972562789917\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1281/1788: 0.1554417461156845\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1282/1788: 0.24311991035938263\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1283/1788: 0.19410133361816406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1284/1788: 0.2423112541437149\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1285/1788: 0.23049262166023254\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1286/1788: 0.7860148549079895\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1287/1788: 0.26726335287094116\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1288/1788: 0.1705324798822403\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1289/1788: 0.15108542144298553\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1290/1788: 0.13229578733444214\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1291/1788: 0.3759411573410034\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1292/1788: 0.19804929196834564\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1293/1788: 0.16422566771507263\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1294/1788: 0.17274080216884613\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1295/1788: 0.17037834227085114\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1296/1788: 0.06806488335132599\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1297/1788: 0.3232293725013733\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1298/1788: 0.1708836704492569\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1299/1788: 0.25710010528564453\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1300/1788: 0.09089037030935287\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1301/1788: 0.2872525155544281\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1302/1788: 0.20065100491046906\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1303/1788: 0.22310489416122437\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1304/1788: 0.13768863677978516\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1305/1788: 0.3368742763996124\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1306/1788: 0.40267080068588257\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1307/1788: 0.459367573261261\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1308/1788: 0.1763342022895813\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1309/1788: 0.36839812994003296\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1310/1788: 0.15177950263023376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1311/1788: 0.35744625329971313\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1312/1788: 0.23661692440509796\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1313/1788: 0.3587912917137146\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1314/1788: 0.0857410728931427\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1315/1788: 0.24407559633255005\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1316/1788: 0.2631016671657562\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1317/1788: 0.16725872457027435\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1318/1788: 0.47204461693763733\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1319/1788: 0.0664423480629921\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1320/1788: 0.14794085919857025\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1321/1788: 0.11621204763650894\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1322/1788: 0.0749051496386528\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1323/1788: 0.2566458582878113\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1324/1788: 0.2710973918437958\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1325/1788: 0.2487928718328476\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1326/1788: 0.2175130993127823\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1327/1788: 0.15940603613853455\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1328/1788: 0.1193455383181572\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1329/1788: 0.15736651420593262\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1330/1788: 0.3358144760131836\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1331/1788: 0.149775892496109\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1332/1788: 0.1774248629808426\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1333/1788: 0.15390807390213013\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1334/1788: 0.08344502002000809\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1335/1788: 0.5089324116706848\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1336/1788: 0.16310431063175201\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1337/1788: 0.26625099778175354\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1338/1788: 0.24703054130077362\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1339/1788: 0.24703244864940643\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1340/1788: 0.28511080145835876\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1341/1788: 0.13434924185276031\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1342/1788: 0.062298648059368134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1343/1788: 0.20893001556396484\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1344/1788: 0.18549844622612\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1345/1788: 0.1946406066417694\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1346/1788: 0.35819074511528015\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1347/1788: 0.22319990396499634\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1348/1788: 0.17139628529548645\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1349/1788: 0.06103421747684479\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1350/1788: 0.15720048546791077\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1351/1788: 0.3044043779373169\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1352/1788: 0.4590074121952057\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1353/1788: 0.31412291526794434\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1354/1788: 0.18435844779014587\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1355/1788: 0.189003586769104\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1356/1788: 0.3007809519767761\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1357/1788: 0.07191361486911774\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1358/1788: 0.42990797758102417\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1359/1788: 0.33089208602905273\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1360/1788: 0.21770384907722473\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1361/1788: 0.3015429377555847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1362/1788: 0.07399913668632507\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1363/1788: 0.27305468916893005\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1364/1788: 0.28209030628204346\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1365/1788: 0.2368408888578415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1366/1788: 0.2402704656124115\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1367/1788: 0.6255912184715271\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1368/1788: 0.20359386503696442\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1369/1788: 0.42872530221939087\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1370/1788: 0.19560593366622925\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1371/1788: 0.21356846392154694\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1372/1788: 0.2231442630290985\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1373/1788: 0.053157247602939606\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1374/1788: 0.2981385588645935\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1375/1788: 0.3182946741580963\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1376/1788: 0.1270119845867157\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1377/1788: 0.23642498254776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1378/1788: 0.05204540118575096\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1379/1788: 0.28718531131744385\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1380/1788: 0.19322460889816284\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1381/1788: 0.4731624722480774\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1382/1788: 0.2460286170244217\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1383/1788: 0.33445772528648376\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1384/1788: 0.12645463645458221\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1385/1788: 0.2386450320482254\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1386/1788: 0.4974095821380615\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1387/1788: 0.22363893687725067\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1388/1788: 0.32862386107444763\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1389/1788: 0.1654455065727234\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1390/1788: 0.08415447920560837\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1391/1788: 0.16828562319278717\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1392/1788: 0.19144795835018158\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1393/1788: 0.1871068775653839\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1394/1788: 0.2056828737258911\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1395/1788: 0.3103121817111969\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1396/1788: 0.35657328367233276\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1397/1788: 0.21699009835720062\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1398/1788: 0.10618451982736588\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1399/1788: 0.3169156014919281\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1400/1788: 0.027091287076473236\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1401/1788: 0.4536355137825012\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1402/1788: 0.16334451735019684\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1403/1788: 0.19350090622901917\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1404/1788: 0.15592223405838013\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1405/1788: 0.2997666001319885\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1406/1788: 0.20790651440620422\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1407/1788: 0.050506073981523514\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1408/1788: 0.6106401681900024\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1409/1788: 0.1582910269498825\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1410/1788: 0.19777870178222656\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1411/1788: 0.3925197720527649\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1412/1788: 0.1564866155385971\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1413/1788: 0.16509491205215454\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1414/1788: 0.6895380616188049\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1415/1788: 0.4254172742366791\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1416/1788: 0.07483890652656555\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1417/1788: 0.15429504215717316\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1418/1788: 0.27741295099258423\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1419/1788: 0.18255572021007538\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1420/1788: 0.20698095858097076\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1421/1788: 0.19956625998020172\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1422/1788: 0.2262272983789444\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1423/1788: 0.21969541907310486\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1424/1788: 0.4846484065055847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1425/1788: 0.6052392721176147\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1426/1788: 0.4565613865852356\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1427/1788: 0.46597445011138916\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1428/1788: 0.35315409302711487\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1429/1788: 0.3313632607460022\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1430/1788: 0.08843982219696045\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1431/1788: 0.15436500310897827\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1432/1788: 0.3763366937637329\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1433/1788: 0.11428027600049973\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1434/1788: 0.09916722029447556\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1435/1788: 0.4218861162662506\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1436/1788: 0.2453128695487976\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1437/1788: 0.5919317007064819\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1438/1788: 0.42166030406951904\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1439/1788: 0.08131126314401627\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1440/1788: 0.17441539466381073\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1441/1788: 0.2753250300884247\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1442/1788: 0.28562241792678833\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1443/1788: 0.39472997188568115\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1444/1788: 0.20141419768333435\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1445/1788: 0.477751761674881\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1446/1788: 0.3603772222995758\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1447/1788: 0.40773090720176697\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1448/1788: 0.2301497906446457\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1449/1788: 0.22298242151737213\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1450/1788: 0.1685025542974472\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1451/1788: 0.4623129665851593\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1452/1788: 0.2857692837715149\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1453/1788: 0.22110454738140106\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1454/1788: 0.2849831283092499\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1455/1788: 0.18922553956508636\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1456/1788: 0.1545872986316681\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1457/1788: 0.24134905636310577\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1458/1788: 0.4046560227870941\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1459/1788: 0.04856819659471512\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1460/1788: 0.20176196098327637\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1461/1788: 0.20328505337238312\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1462/1788: 0.26593735814094543\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1463/1788: 0.3004814684391022\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1464/1788: 0.536740243434906\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1465/1788: 0.399889200925827\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1466/1788: 0.052932240068912506\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1467/1788: 0.1705690175294876\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1468/1788: 0.08220579475164413\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1469/1788: 0.09502694010734558\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1470/1788: 0.06982994079589844\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1471/1788: 0.3774358034133911\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1472/1788: 0.5671936273574829\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1473/1788: 0.3726522624492645\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1474/1788: 0.23187726736068726\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1475/1788: 0.17481601238250732\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1476/1788: 0.5257842540740967\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1477/1788: 0.5460643172264099\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1478/1788: 0.07230792194604874\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1479/1788: 0.15974853932857513\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1480/1788: 0.18227997422218323\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1481/1788: 0.3524706959724426\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1482/1788: 0.14736439287662506\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1483/1788: 0.043651677668094635\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1484/1788: 0.326442152261734\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1485/1788: 0.23317761719226837\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1486/1788: 0.15741124749183655\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1487/1788: 0.222675159573555\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1488/1788: 0.12234777957201004\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1489/1788: 0.07211676239967346\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1490/1788: 0.49675557017326355\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1491/1788: 0.4225163757801056\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1492/1788: 0.16662518680095673\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1493/1788: 0.12157710641622543\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1494/1788: 0.1365216076374054\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1495/1788: 0.1278759390115738\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1496/1788: 0.19382016360759735\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1497/1788: 0.3988995850086212\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1498/1788: 0.2635619640350342\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1499/1788: 0.20817148685455322\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1500/1788: 0.3919724225997925\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1501/1788: 0.22532129287719727\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1502/1788: 0.2904004752635956\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1503/1788: 0.1379060298204422\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1504/1788: 0.05930794030427933\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1505/1788: 0.1362294852733612\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1506/1788: 0.1356726735830307\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1507/1788: 0.4446168839931488\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1508/1788: 0.0406125970184803\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1509/1788: 0.25910240411758423\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1510/1788: 0.4144541919231415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1511/1788: 0.07568156719207764\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1512/1788: 0.500689685344696\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1513/1788: 0.17619705200195312\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1514/1788: 0.09915770590305328\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1515/1788: 0.14721736311912537\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1516/1788: 0.1315506249666214\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1517/1788: 0.3010099530220032\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1518/1788: 0.3364042043685913\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1519/1788: 0.09964185953140259\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1520/1788: 0.4123871922492981\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1521/1788: 0.13776427507400513\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1522/1788: 0.3674047291278839\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1523/1788: 0.16677406430244446\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1524/1788: 0.13157086074352264\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1525/1788: 0.40272513031959534\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1526/1788: 0.25498077273368835\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1527/1788: 0.20883703231811523\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1528/1788: 0.42561525106430054\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1529/1788: 0.16956304013729095\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1530/1788: 0.3503904640674591\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1531/1788: 0.08925378322601318\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1532/1788: 0.1385209560394287\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1533/1788: 0.21511094272136688\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1534/1788: 0.42289668321609497\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1535/1788: 0.16112568974494934\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1536/1788: 0.028171416372060776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1537/1788: 0.5812281370162964\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1538/1788: 0.24702507257461548\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1539/1788: 0.23287086188793182\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1540/1788: 0.14921043813228607\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1541/1788: 0.09488021582365036\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1542/1788: 0.19770127534866333\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1543/1788: 0.24459534883499146\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1544/1788: 0.11166389286518097\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1545/1788: 0.17303046584129333\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1546/1788: 0.08374807238578796\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1547/1788: 0.5680930018424988\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1548/1788: 0.16916418075561523\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1549/1788: 0.4387931227684021\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1550/1788: 0.16108377277851105\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1551/1788: 0.4765376150608063\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1552/1788: 0.06532710790634155\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1553/1788: 0.38679739832878113\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1554/1788: 0.30566540360450745\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1555/1788: 0.22821614146232605\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1556/1788: 0.3314794600009918\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1557/1788: 0.1324414312839508\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1558/1788: 0.215143620967865\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1559/1788: 0.20784877240657806\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1560/1788: 0.37563639879226685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1561/1788: 0.14458194375038147\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1562/1788: 0.13192003965377808\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1563/1788: 0.23741257190704346\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1564/1788: 0.30313625931739807\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1565/1788: 0.29636895656585693\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1566/1788: 0.3711012601852417\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1567/1788: 0.2077648788690567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1568/1788: 0.4003417491912842\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1569/1788: 0.17871081829071045\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1570/1788: 0.3224991261959076\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1571/1788: 0.3382158875465393\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1572/1788: 0.26089075207710266\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1573/1788: 0.343295693397522\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1574/1788: 0.16618196666240692\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1575/1788: 0.19104281067848206\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1576/1788: 0.0983676165342331\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1577/1788: 0.8174976110458374\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1578/1788: 0.32061997056007385\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1579/1788: 0.4914570152759552\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1580/1788: 0.5228469967842102\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1581/1788: 0.3005046844482422\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1582/1788: 0.10923004895448685\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1583/1788: 0.14021171629428864\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1584/1788: 0.40090805292129517\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1585/1788: 0.31810781359672546\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1586/1788: 0.3095267117023468\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1587/1788: 0.2957235872745514\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1588/1788: 0.25036242604255676\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1589/1788: 0.3403039276599884\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1590/1788: 0.2469053417444229\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1591/1788: 0.4712924659252167\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1592/1788: 0.3492661416530609\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1593/1788: 0.3461592495441437\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1594/1788: 0.08657106012105942\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1595/1788: 0.11178863793611526\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1596/1788: 0.2797141373157501\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1597/1788: 0.24834167957305908\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1598/1788: 0.26656603813171387\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1599/1788: 0.14336517453193665\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1600/1788: 0.2108129858970642\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1601/1788: 0.12715665996074677\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1602/1788: 0.1410502791404724\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1603/1788: 0.061459798365831375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1604/1788: 0.41186419129371643\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1605/1788: 0.0514051653444767\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1606/1788: 0.1394706517457962\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1607/1788: 0.124361053109169\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1608/1788: 0.1352517008781433\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1609/1788: 0.36800330877304077\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1610/1788: 0.23785749077796936\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1611/1788: 0.13268794119358063\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1612/1788: 0.2328401505947113\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1613/1788: 0.29729530215263367\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1614/1788: 0.24657928943634033\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1615/1788: 0.20240454375743866\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1616/1788: 0.1690560132265091\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1617/1788: 0.1506209820508957\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1618/1788: 0.23558416962623596\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1619/1788: 0.43475112318992615\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1620/1788: 0.1811293214559555\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1621/1788: 0.3530869781970978\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1622/1788: 0.07928789407014847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1623/1788: 0.4546505808830261\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1624/1788: 0.25914236903190613\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1625/1788: 0.16087549924850464\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1626/1788: 0.2494642734527588\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1627/1788: 0.20133058726787567\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1628/1788: 0.24600841104984283\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1629/1788: 0.1265321671962738\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1630/1788: 0.1380888670682907\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1631/1788: 0.1791728138923645\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1632/1788: 0.2860456109046936\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1633/1788: 0.17422255873680115\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1634/1788: 0.4306578040122986\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1635/1788: 0.3026672601699829\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1636/1788: 0.14365774393081665\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1637/1788: 0.24057745933532715\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1638/1788: 0.11381653696298599\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1639/1788: 0.2637243866920471\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1640/1788: 0.3056375980377197\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1641/1788: 0.0886533111333847\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1642/1788: 0.21391861140727997\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1643/1788: 0.13261546194553375\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1644/1788: 0.42038437724113464\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1645/1788: 0.17586421966552734\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1646/1788: 0.28552767634391785\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1647/1788: 0.1780056357383728\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1648/1788: 0.18506570160388947\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1649/1788: 0.1607929766178131\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1650/1788: 0.13335564732551575\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1651/1788: 0.20202772319316864\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1652/1788: 0.032307736575603485\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1653/1788: 0.36187490820884705\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1654/1788: 0.20105041563510895\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1655/1788: 0.2691143751144409\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1656/1788: 0.2555561363697052\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1657/1788: 0.1545586884021759\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1658/1788: 0.21742253005504608\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1659/1788: 0.15328671038150787\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1660/1788: 0.5180999636650085\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1661/1788: 0.13813747465610504\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1662/1788: 0.11289337277412415\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1663/1788: 0.04446851834654808\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1664/1788: 0.3130514621734619\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1665/1788: 0.2733742892742157\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1666/1788: 0.2032814770936966\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1667/1788: 0.6620050668716431\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1668/1788: 0.6164382696151733\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1669/1788: 0.14424756169319153\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1670/1788: 0.32803094387054443\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1671/1788: 0.3619873523712158\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1672/1788: 0.2072436362504959\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1673/1788: 0.4659106433391571\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1674/1788: 0.3727549612522125\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1675/1788: 0.13916969299316406\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1676/1788: 0.1704864501953125\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1677/1788: 0.2384880632162094\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1678/1788: 0.2834535241127014\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1679/1788: 0.2201082706451416\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1680/1788: 0.2698652446269989\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1681/1788: 0.2097834199666977\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1682/1788: 0.5327366590499878\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1683/1788: 0.19412873685359955\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1684/1788: 0.12777480483055115\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1685/1788: 0.22290939092636108\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1686/1788: 0.2386717051267624\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1687/1788: 0.20597906410694122\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1688/1788: 0.29062891006469727\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1689/1788: 0.48498332500457764\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1690/1788: 0.1953488290309906\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1691/1788: 0.2526163160800934\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1692/1788: 0.19849571585655212\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1693/1788: 0.42922791838645935\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1694/1788: 0.27957087755203247\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1695/1788: 0.3573967516422272\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1696/1788: 0.31307855248451233\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1697/1788: 0.36379361152648926\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1698/1788: 0.2751424312591553\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1699/1788: 0.20098581910133362\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1700/1788: 0.23734648525714874\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1701/1788: 0.29100632667541504\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1702/1788: 0.25807490944862366\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1703/1788: 0.12792213261127472\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1704/1788: 0.3383835256099701\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1705/1788: 0.33545899391174316\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1706/1788: 0.23320774734020233\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1707/1788: 0.25419265031814575\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1708/1788: 0.12104599177837372\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1709/1788: 0.06316384673118591\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1710/1788: 0.2207208126783371\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1711/1788: 0.369254469871521\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1712/1788: 0.06214424967765808\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1713/1788: 0.4211287796497345\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1714/1788: 0.17339754104614258\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1715/1788: 0.156711608171463\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1716/1788: 0.35977432131767273\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1717/1788: 0.12027933448553085\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1718/1788: 0.5891315340995789\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1719/1788: 0.13588854670524597\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1720/1788: 0.3277326226234436\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1721/1788: 0.1100420281291008\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1722/1788: 0.24915257096290588\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1723/1788: 0.1594928652048111\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1724/1788: 0.20962752401828766\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1725/1788: 0.29402121901512146\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1726/1788: 0.22765162587165833\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1727/1788: 0.24133874475955963\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1728/1788: 0.1950860470533371\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1729/1788: 0.3411449193954468\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1730/1788: 0.2997209429740906\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1731/1788: 0.1485702395439148\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1732/1788: 0.14289432764053345\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1733/1788: 0.11488667875528336\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1734/1788: 0.17784534394741058\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1735/1788: 0.15951189398765564\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1736/1788: 0.06881437450647354\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1737/1788: 0.2160664200782776\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1738/1788: 0.11855799704790115\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1739/1788: 0.34108465909957886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1740/1788: 0.20657531917095184\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1741/1788: 0.2152813822031021\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1742/1788: 0.08151059597730637\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1743/1788: 0.21651725471019745\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1744/1788: 0.2904468774795532\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1745/1788: 0.24436728656291962\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1746/1788: 0.2632383406162262\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1747/1788: 0.09143251925706863\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1748/1788: 0.27896416187286377\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1749/1788: 0.3541882038116455\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1750/1788: 0.3058691620826721\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1751/1788: 0.6953721642494202\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1752/1788: 0.3068680167198181\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1753/1788: 0.15756332874298096\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1754/1788: 0.1860179305076599\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1755/1788: 0.23029416799545288\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1756/1788: 0.1903318464756012\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1757/1788: 0.06433890759944916\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1758/1788: 0.23337963223457336\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1759/1788: 0.34187600016593933\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1760/1788: 0.3165344297885895\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1761/1788: 0.13162106275558472\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1762/1788: 0.1309759020805359\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1763/1788: 0.43428170680999756\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1764/1788: 0.2804805040359497\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1765/1788: 0.22600936889648438\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1766/1788: 0.20866723358631134\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1767/1788: 0.26996108889579773\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1768/1788: 0.31504133343696594\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1769/1788: 0.303758442401886\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1770/1788: 0.2169024646282196\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1771/1788: 0.2256290167570114\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1772/1788: 0.35204488039016724\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1773/1788: 0.3381882607936859\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1774/1788: 0.35845667123794556\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1775/1788: 0.2556946277618408\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1776/1788: 0.1820393204689026\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1777/1788: 0.18143807351589203\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1778/1788: 0.21442890167236328\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1779/1788: 0.11706015467643738\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1780/1788: 0.40455910563468933\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1781/1788: 0.10539018362760544\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1782/1788: 0.29258251190185547\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1783/1788: 0.34559231996536255\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1784/1788: 0.4435805082321167\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1785/1788: 0.23200690746307373\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1786/1788: 0.35972222685813904\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1787/1788: 0.36739465594291687\n",
            "A-LLMRec model loss in epoch 1/2 iteration 1788/1788: 0.11578227579593658\n",
            "100% 1/1 [09:17<00:00, 557.61s/it]\n",
            "phase2 train time : 557.6135737895966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"/content/models/saved_models\"\n",
        "target_dir = \"/content/drive/MyDrive/models/saved_models\"\n",
        "\n",
        "for item in os.listdir(source_dir):\n",
        "    source_item = os.path.join(source_dir, item)\n",
        "    target_item = os.path.join(target_dir, item)\n",
        "\n",
        "    # Only copy files, skip directories\n",
        "    if os.path.isfile(source_item):\n",
        "        if not os.path.exists(target_item):\n",
        "            shutil.copy2(source_item, target_item)\n",
        "            print(f\"Copied: {item}\")\n",
        "        else:\n",
        "            print(f\"Skipped: {item} (already exists)\")\n",
        "    else:\n",
        "        print(f\"Skipped directory: {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GteRnMPWlGLX",
        "outputId": "571b6eba-cd97-4c30-d7a0-1a382008ec20"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped directory: .ipynb_checkpoints\n",
            "Copied: tmp.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "!python /content/drive/MyDrive/Rec_Proj_DL/main.py --inference --rec_pre_trained_data Software"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQmwg8EQNSJj",
        "outputId": "c810b27c-3482-4498-f56a-e3ed15126635"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A-LLMRec start inference\n",
            "\n",
            "user num: 3577 item num: 4557\n",
            "average sequence length: 4.91\n",
            "Initializing with num_user: 3536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.move('/content/recommendation_output.txt', '/content/drive/MyDrive/Rec_Proj_DL/recommendation_output.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e3RhZ0KXlXHE",
        "outputId": "c90ab396-b558-4e81-fb97-2c0a373a531c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Rec_Proj_DL/recommendation_output.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Rec_Proj_DL/eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t68hjqoQo3YL",
        "outputId": "f1e997ea-85ef-464e-efef-9f62320e29b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3536 3536\n",
            "3536\n",
            "ndcg at 1: 0.2403846153846154\n",
            "hit at 1: 0.2403846153846154\n"
          ]
        }
      ]
    }
  ]
}